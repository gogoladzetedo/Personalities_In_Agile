{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> The Importance of Personality Traits in Agile Software Development: A Case Study </h1>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.0 Developers` personality insight analysis <hr />\n",
    "using the open source datasets of eoght software development teams JIRA logs </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.0.1 load libraries and datasets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
    "issues = pd.read_csv('jiradataset_issues.csv')\n",
    "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
    "users = pd.read_csv('jiradataset_users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.0.2 Check the datasets </h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created</th>\n",
       "      <th>field</th>\n",
       "      <th>fieldtype</th>\n",
       "      <th>from</th>\n",
       "      <th>fromString</th>\n",
       "      <th>key</th>\n",
       "      <th>project</th>\n",
       "      <th>to</th>\n",
       "      <th>toString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jvalkeal</td>\n",
       "      <td>2016-03-03 18:40:53.171</td>\n",
       "      <td>status</td>\n",
       "      <td>jira</td>\n",
       "      <td>10000</td>\n",
       "      <td>To Do</td>\n",
       "      <td>XD-3751</td>\n",
       "      <td>xd</td>\n",
       "      <td>3</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jvalkeal</td>\n",
       "      <td>2016-03-03 18:41:19.429</td>\n",
       "      <td>Pull Request URL</td>\n",
       "      <td>custom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XD-3751</td>\n",
       "      <td>xd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/spring-projects/spring-xd/p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jvalkeal</td>\n",
       "      <td>2016-03-03 18:41:19.429</td>\n",
       "      <td>status</td>\n",
       "      <td>jira</td>\n",
       "      <td>3</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>XD-3751</td>\n",
       "      <td>xd</td>\n",
       "      <td>10006</td>\n",
       "      <td>In PR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                  created             field fieldtype   from  \\\n",
       "0  jvalkeal  2016-03-03 18:40:53.171            status      jira  10000   \n",
       "1  jvalkeal  2016-03-03 18:41:19.429  Pull Request URL    custom    NaN   \n",
       "2  jvalkeal  2016-03-03 18:41:19.429            status      jira      3   \n",
       "\n",
       "    fromString      key project     to  \\\n",
       "0        To Do  XD-3751      xd      3   \n",
       "1          NaN  XD-3751      xd    NaN   \n",
       "2  In Progress  XD-3751      xd  10006   \n",
       "\n",
       "                                            toString  \n",
       "0                                        In Progress  \n",
       "1  https://github.com/spring-projects/spring-xd/p...  \n",
       "2                                              In PR  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changelog.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fields.assignee.name</th>\n",
       "      <th>fields.components</th>\n",
       "      <th>fields.created</th>\n",
       "      <th>fields.creator.name</th>\n",
       "      <th>fields.description</th>\n",
       "      <th>fields.fixVersions</th>\n",
       "      <th>fields.issuetype.name</th>\n",
       "      <th>fields.issuetype.subtask</th>\n",
       "      <th>fields.priority.name</th>\n",
       "      <th>fields.reporter.name</th>\n",
       "      <th>...</th>\n",
       "      <th>fields.status.name</th>\n",
       "      <th>fields.status.statusCategory.name</th>\n",
       "      <th>fields.summary</th>\n",
       "      <th>fields.updated</th>\n",
       "      <th>fields.versions</th>\n",
       "      <th>fields.watches.watchCount</th>\n",
       "      <th>key</th>\n",
       "      <th>storypoints</th>\n",
       "      <th>project</th>\n",
       "      <th>sprint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[{u'id': u'12786', u'self': u'https://jira.spr...</td>\n",
       "      <td>2016-03-31T22:35:55.000+0000</td>\n",
       "      <td>thomas.risberg</td>\n",
       "      <td>The MapReduce samples should have \"yarn.resour...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Improvement</td>\n",
       "      <td>False</td>\n",
       "      <td>Minor</td>\n",
       "      <td>thomas.risberg</td>\n",
       "      <td>...</td>\n",
       "      <td>To Do</td>\n",
       "      <td>To Do</td>\n",
       "      <td>Add \"yarn.resourcemanager.scheduler.address\" t...</td>\n",
       "      <td>2016-03-31T22:35:55.000+0000</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>XD-3753</td>\n",
       "      <td>2.0</td>\n",
       "      <td>xd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[{u'id': u'12784', u'self': u'https://jira.spr...</td>\n",
       "      <td>2016-03-14T18:09:51.000+0000</td>\n",
       "      <td>manju4ever</td>\n",
       "      <td>When i use the admin-ui web portal which runs ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Bug</td>\n",
       "      <td>False</td>\n",
       "      <td>Trivial</td>\n",
       "      <td>manju4ever</td>\n",
       "      <td>...</td>\n",
       "      <td>To Do</td>\n",
       "      <td>To Do</td>\n",
       "      <td>Admin UI login Page failing to load due to req...</td>\n",
       "      <td>2016-03-14T18:09:51.000+0000</td>\n",
       "      <td>[{u'archived': False, u'description': u'1.3.0 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>XD-3752</td>\n",
       "      <td>20.0</td>\n",
       "      <td>xd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jvalkeal</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-03-03T13:22:14.000+0000</td>\n",
       "      <td>jvalkeal</td>\n",
       "      <td>In a case where reactor's ringbuffer is full a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Bug</td>\n",
       "      <td>False</td>\n",
       "      <td>Major</td>\n",
       "      <td>jvalkeal</td>\n",
       "      <td>...</td>\n",
       "      <td>In PR</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>gpfdist may fail to shutdown with backlog</td>\n",
       "      <td>2016-03-03T18:41:19.000+0000</td>\n",
       "      <td>[{u'archived': False, u'name': u'1.3.1', u'sel...</td>\n",
       "      <td>1</td>\n",
       "      <td>XD-3751</td>\n",
       "      <td>5.0</td>\n",
       "      <td>xd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  fields.assignee.name                                  fields.components  \\\n",
       "0                  NaN  [{u'id': u'12786', u'self': u'https://jira.spr...   \n",
       "1                  NaN  [{u'id': u'12784', u'self': u'https://jira.spr...   \n",
       "2             jvalkeal                                                 []   \n",
       "\n",
       "                 fields.created fields.creator.name  \\\n",
       "0  2016-03-31T22:35:55.000+0000      thomas.risberg   \n",
       "1  2016-03-14T18:09:51.000+0000          manju4ever   \n",
       "2  2016-03-03T13:22:14.000+0000            jvalkeal   \n",
       "\n",
       "                                  fields.description fields.fixVersions  \\\n",
       "0  The MapReduce samples should have \"yarn.resour...                 []   \n",
       "1  When i use the admin-ui web portal which runs ...                 []   \n",
       "2  In a case where reactor's ringbuffer is full a...                 []   \n",
       "\n",
       "  fields.issuetype.name  fields.issuetype.subtask fields.priority.name  \\\n",
       "0           Improvement                     False                Minor   \n",
       "1                   Bug                     False              Trivial   \n",
       "2                   Bug                     False                Major   \n",
       "\n",
       "  fields.reporter.name  ... fields.status.name  \\\n",
       "0       thomas.risberg  ...              To Do   \n",
       "1           manju4ever  ...              To Do   \n",
       "2             jvalkeal  ...              In PR   \n",
       "\n",
       "  fields.status.statusCategory.name  \\\n",
       "0                             To Do   \n",
       "1                             To Do   \n",
       "2                       In Progress   \n",
       "\n",
       "                                      fields.summary  \\\n",
       "0  Add \"yarn.resourcemanager.scheduler.address\" t...   \n",
       "1  Admin UI login Page failing to load due to req...   \n",
       "2          gpfdist may fail to shutdown with backlog   \n",
       "\n",
       "                 fields.updated  \\\n",
       "0  2016-03-31T22:35:55.000+0000   \n",
       "1  2016-03-14T18:09:51.000+0000   \n",
       "2  2016-03-03T18:41:19.000+0000   \n",
       "\n",
       "                                     fields.versions  \\\n",
       "0                                                 []   \n",
       "1  [{u'archived': False, u'description': u'1.3.0 ...   \n",
       "2  [{u'archived': False, u'name': u'1.3.1', u'sel...   \n",
       "\n",
       "  fields.watches.watchCount      key storypoints project  sprint  \n",
       "0                         1  XD-3753         2.0      xd     NaN  \n",
       "1                         1  XD-3752        20.0      xd     NaN  \n",
       "2                         1  XD-3751         5.0      xd     NaN  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>project</th>\n",
       "      <th>sprint.completeDate</th>\n",
       "      <th>sprint.endDate</th>\n",
       "      <th>sprint.id</th>\n",
       "      <th>sprint.name</th>\n",
       "      <th>sprint.startDate</th>\n",
       "      <th>sprint.state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XD-3744</td>\n",
       "      <td>xd</td>\n",
       "      <td>2016-02-26T16:32:18.620Z</td>\n",
       "      <td>2016-02-26T08:34:00.000Z</td>\n",
       "      <td>108</td>\n",
       "      <td>Sprint 68</td>\n",
       "      <td>2016-02-16T00:38:45.289Z</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XD-3743</td>\n",
       "      <td>xd</td>\n",
       "      <td>2016-02-26T16:32:18.620Z</td>\n",
       "      <td>2016-02-26T08:34:00.000Z</td>\n",
       "      <td>108</td>\n",
       "      <td>Sprint 68</td>\n",
       "      <td>2016-02-16T00:38:45.289Z</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XD-3742</td>\n",
       "      <td>xd</td>\n",
       "      <td>2016-02-26T16:32:18.620Z</td>\n",
       "      <td>2016-02-26T08:34:00.000Z</td>\n",
       "      <td>108</td>\n",
       "      <td>Sprint 68</td>\n",
       "      <td>2016-02-16T00:38:45.289Z</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       key project       sprint.completeDate            sprint.endDate  \\\n",
       "0  XD-3744      xd  2016-02-26T16:32:18.620Z  2016-02-26T08:34:00.000Z   \n",
       "1  XD-3743      xd  2016-02-26T16:32:18.620Z  2016-02-26T08:34:00.000Z   \n",
       "2  XD-3742      xd  2016-02-26T16:32:18.620Z  2016-02-26T08:34:00.000Z   \n",
       "\n",
       "   sprint.id sprint.name          sprint.startDate sprint.state  \n",
       "0        108   Sprint 68  2016-02-16T00:38:45.289Z       CLOSED  \n",
       "1        108   Sprint 68  2016-02-16T00:38:45.289Z       CLOSED  \n",
       "2        108   Sprint 68  2016-02-16T00:38:45.289Z       CLOSED  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sprints.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>displayName</th>\n",
       "      <th>emailAddress</th>\n",
       "      <th>name</th>\n",
       "      <th>project</th>\n",
       "      <th>role</th>\n",
       "      <th>timeZone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xd</td>\n",
       "      <td>assignee</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Janne Valkealahti</td>\n",
       "      <td>janne dot valkealahti at gmail dot com</td>\n",
       "      <td>jvalkeal</td>\n",
       "      <td>xd</td>\n",
       "      <td>assignee</td>\n",
       "      <td>Europe/London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Gary Russell</td>\n",
       "      <td>grussell at gopivotal dot com</td>\n",
       "      <td>grussell</td>\n",
       "      <td>xd</td>\n",
       "      <td>assignee</td>\n",
       "      <td>America/New_York</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        displayName                            emailAddress  \\\n",
       "0           0                NaN                                     NaN   \n",
       "1           2  Janne Valkealahti  janne dot valkealahti at gmail dot com   \n",
       "2           5       Gary Russell           grussell at gopivotal dot com   \n",
       "\n",
       "       name project      role          timeZone  \n",
       "0       NaN      xd  assignee               NaN  \n",
       "1  jvalkeal      xd  assignee     Europe/London  \n",
       "2  grussell      xd  assignee  America/New_York  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created</th>\n",
       "      <th>field</th>\n",
       "      <th>fieldtype</th>\n",
       "      <th>from</th>\n",
       "      <th>fromString</th>\n",
       "      <th>key</th>\n",
       "      <th>project</th>\n",
       "      <th>to</th>\n",
       "      <th>toString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jvalkeal</td>\n",
       "      <td>2016-03-03 18:40:53.171</td>\n",
       "      <td>status</td>\n",
       "      <td>jira</td>\n",
       "      <td>10000</td>\n",
       "      <td>To Do</td>\n",
       "      <td>XD-3751</td>\n",
       "      <td>xd</td>\n",
       "      <td>3</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jvalkeal</td>\n",
       "      <td>2016-03-03 18:41:19.429</td>\n",
       "      <td>Pull Request URL</td>\n",
       "      <td>custom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XD-3751</td>\n",
       "      <td>xd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/spring-projects/spring-xd/p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jvalkeal</td>\n",
       "      <td>2016-03-03 18:41:19.429</td>\n",
       "      <td>status</td>\n",
       "      <td>jira</td>\n",
       "      <td>3</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>XD-3751</td>\n",
       "      <td>xd</td>\n",
       "      <td>10006</td>\n",
       "      <td>In PR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                  created             field fieldtype   from  \\\n",
       "0  jvalkeal  2016-03-03 18:40:53.171            status      jira  10000   \n",
       "1  jvalkeal  2016-03-03 18:41:19.429  Pull Request URL    custom    NaN   \n",
       "2  jvalkeal  2016-03-03 18:41:19.429            status      jira      3   \n",
       "\n",
       "    fromString      key project     to  \\\n",
       "0        To Do  XD-3751      xd      3   \n",
       "1          NaN  XD-3751      xd    NaN   \n",
       "2  In Progress  XD-3751      xd  10006   \n",
       "\n",
       "                                            toString  \n",
       "0                                        In Progress  \n",
       "1  https://github.com/spring-projects/spring-xd/p...  \n",
       "2                                              In PR  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changelog.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>1.1 Check the datasets for the manually written textual values</h3>\n",
    "<h5>1.1.1 within 'changelog' take two examplary row values from each unique field, store into dataframe, export into csv and check manually</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>field</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>fromString</th>\n",
       "      <th>toString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>status</td>\n",
       "      <td>0          To Do\n",
       "2    In Progress\n",
       "Name: fromSt...</td>\n",
       "      <td>0    In Progress\n",
       "2          In PR\n",
       "Name: toStri...</td>\n",
       "      <td>0    10000\n",
       "2        3\n",
       "Name: from, dtype: object</td>\n",
       "      <td>0        3\n",
       "2    10006\n",
       "Name: to, dtype: object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pull Request URL</td>\n",
       "      <td>1     NaN\n",
       "13    NaN\n",
       "Name: fromString, dtype: o...</td>\n",
       "      <td>1     https://github.com/spring-projects/sprin...</td>\n",
       "      <td>1     NaN\n",
       "13    NaN\n",
       "Name: from, dtype: object</td>\n",
       "      <td>1     NaN\n",
       "13    NaN\n",
       "Name: to, dtype: object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>summary</td>\n",
       "      <td>3    Can completely remove module after puttin...</td>\n",
       "      <td>3    Can completely remove custom module after...</td>\n",
       "      <td>3    NaN\n",
       "5    NaN\n",
       "Name: from, dtype: object</td>\n",
       "      <td>3    NaN\n",
       "5    NaN\n",
       "Name: to, dtype: object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>issuetype</td>\n",
       "      <td>4     Story\n",
       "17    Story\n",
       "Name: fromString, dtyp...</td>\n",
       "      <td>4             Bug\n",
       "17    Improvement\n",
       "Name: toSt...</td>\n",
       "      <td>4     8\n",
       "17    8\n",
       "Name: from, dtype: object</td>\n",
       "      <td>4     1\n",
       "17    4\n",
       "Name: to, dtype: object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>description</td>\n",
       "      <td>7    Custom conversion is broken.\\r\\r\\n\\r\\r\\nI...</td>\n",
       "      <td>7    Custom conversion is broken.\\r\\r\\n\\r\\r\\nI...</td>\n",
       "      <td>7    NaN\n",
       "8    NaN\n",
       "Name: from, dtype: object</td>\n",
       "      <td>7    NaN\n",
       "8    NaN\n",
       "Name: to, dtype: object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id             field                                               from  \\\n",
       "0   1            status  0          To Do\n",
       "2    In Progress\n",
       "Name: fromSt...   \n",
       "1   2  Pull Request URL  1     NaN\n",
       "13    NaN\n",
       "Name: fromString, dtype: o...   \n",
       "2   3           summary  3    Can completely remove module after puttin...   \n",
       "3   4         issuetype  4     Story\n",
       "17    Story\n",
       "Name: fromString, dtyp...   \n",
       "4   5       description  7    Custom conversion is broken.\\r\\r\\n\\r\\r\\nI...   \n",
       "\n",
       "                                                  to  \\\n",
       "0  0    In Progress\n",
       "2          In PR\n",
       "Name: toStri...   \n",
       "1  1     https://github.com/spring-projects/sprin...   \n",
       "2  3    Can completely remove custom module after...   \n",
       "3  4             Bug\n",
       "17    Improvement\n",
       "Name: toSt...   \n",
       "4  7    Custom conversion is broken.\\r\\r\\n\\r\\r\\nI...   \n",
       "\n",
       "                                        fromString  \\\n",
       "0  0    10000\n",
       "2        3\n",
       "Name: from, dtype: object   \n",
       "1    1     NaN\n",
       "13    NaN\n",
       "Name: from, dtype: object   \n",
       "2      3    NaN\n",
       "5    NaN\n",
       "Name: from, dtype: object   \n",
       "3        4     8\n",
       "17    8\n",
       "Name: from, dtype: object   \n",
       "4      7    NaN\n",
       "8    NaN\n",
       "Name: from, dtype: object   \n",
       "\n",
       "                                        toString  \n",
       "0  0        3\n",
       "2    10006\n",
       "Name: to, dtype: object  \n",
       "1    1     NaN\n",
       "13    NaN\n",
       "Name: to, dtype: object  \n",
       "2      3    NaN\n",
       "5    NaN\n",
       "Name: to, dtype: object  \n",
       "3        4     1\n",
       "17    4\n",
       "Name: to, dtype: object  \n",
       "4      7    NaN\n",
       "8    NaN\n",
       "Name: to, dtype: object  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declare the empty series\n",
    "i_row = []\n",
    "field_row = []\n",
    "from_row = []\n",
    "to_row = []\n",
    "from_string_row = []\n",
    "to_string_row = []\n",
    "\n",
    "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
    "# append these two values to the respective series\n",
    "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
    "i = 1\n",
    "for field in changelog['field'].unique():\n",
    "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
    "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
    "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
    "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
    "    i_row.append(i)\n",
    "    field_row.append(field)\n",
    "    from_row.append(from_str)\n",
    "    to_row.append(to_str)\n",
    "    from_string_row.append(from_)\n",
    "    to_string_row.append(to_)\n",
    "    i = i + 1\n",
    "df = pd.DataFrame({'id':i_row,\n",
    "                   'field':field_row, \n",
    "                   'from':from_row, \n",
    "                   'to':to_row, \n",
    "                   'fromString':from_string_row, \n",
    "                   'toString':to_string_row })\n",
    "df.to_csv('fields_check.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After checking the fields, detected the ones withthe written textual values:** <hr/>\n",
    "*'summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
    " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2 Prepare 'changelog' dataset with needed columns  </h3> <hr/>\n",
    "Since the jira tasks dataset only have one reporter for each jira task, we need to find the author of actual changes of the textual fields with change log file, as in changelog every action is stored and the author of the change is tracked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>project</th>\n",
       "      <th>author</th>\n",
       "      <th>field</th>\n",
       "      <th>created</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XD-3750</td>\n",
       "      <td>xd</td>\n",
       "      <td>aliiqbal</td>\n",
       "      <td>summary</td>\n",
       "      <td>2016-02-29 10:00:55.086</td>\n",
       "      <td>Can completely remove custom module after putt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XD-3750</td>\n",
       "      <td>xd</td>\n",
       "      <td>aliiqbal</td>\n",
       "      <td>summary</td>\n",
       "      <td>2016-02-29 10:03:14.662</td>\n",
       "      <td>Can completely remove custom module after putt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XD-3750</td>\n",
       "      <td>xd</td>\n",
       "      <td>aliiqbal</td>\n",
       "      <td>summary</td>\n",
       "      <td>2016-03-13 10:24:15.636</td>\n",
       "      <td>Cant completely remove custom module after put...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XD-3749</td>\n",
       "      <td>xd</td>\n",
       "      <td>grussell</td>\n",
       "      <td>description</td>\n",
       "      <td>2016-02-26 15:59:57.698</td>\n",
       "      <td>Custom conversion is broken.\\r\\r\\n\\r\\r\\nIf the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XD-3749</td>\n",
       "      <td>xd</td>\n",
       "      <td>grussell</td>\n",
       "      <td>description</td>\n",
       "      <td>2016-02-26 16:01:28.028</td>\n",
       "      <td>Custom conversion is broken.\\r\\r\\n\\r\\r\\nIf the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       key project    author        field                  created  \\\n",
       "3  XD-3750      xd  aliiqbal      summary  2016-02-29 10:00:55.086   \n",
       "5  XD-3750      xd  aliiqbal      summary  2016-02-29 10:03:14.662   \n",
       "6  XD-3750      xd  aliiqbal      summary  2016-03-13 10:24:15.636   \n",
       "7  XD-3749      xd  grussell  description  2016-02-26 15:59:57.698   \n",
       "8  XD-3749      xd  grussell  description  2016-02-26 16:01:28.028   \n",
       "\n",
       "                                                text  \n",
       "3  Can completely remove custom module after putt...  \n",
       "5  Can completely remove custom module after putt...  \n",
       "6  Cant completely remove custom module after put...  \n",
       "7  Custom conversion is broken.\\r\\r\\n\\r\\r\\nIf the...  \n",
       "8  Custom conversion is broken.\\r\\r\\n\\r\\r\\nIf the...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter changelog with the textual fields\n",
    "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
    " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
    "\n",
    "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
    "# author (author of change), field (what field has been changed), created (date of the change action),\n",
    "# toString (what textual value was assigned to the field), \n",
    "# and from(what was the value of the field, this only works for comments)\n",
    "\n",
    "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
    "log_cols = log_filtered[cols].copy(deep=True)\n",
    "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
    "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
    "\n",
    "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
    "log_cols = log_cols[newcols]\n",
    "log_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.3. Get the unique textual values </h3> \n",
    "<br>discard all the duplicated texts, keep only the latest edits of the fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12280 9050 9050\n"
     ]
    }
   ],
   "source": [
    "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
    "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
    "log_grouped = log_cols.groupby((['project', 'key', 'field', 'author'])).agg({'created':'max'})\n",
    "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
    "\n",
    "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
    "                                   left_on = ['project', 'key', 'field', 'created'],\n",
    "                                   right_on = ['project', 'key', 'field', 'created'])\n",
    "\n",
    "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
    "log_cols = latest_logs_from_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>project</th>\n",
       "      <th>author</th>\n",
       "      <th>field</th>\n",
       "      <th>created</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XD-3750</td>\n",
       "      <td>xd</td>\n",
       "      <td>aliiqbal</td>\n",
       "      <td>summary</td>\n",
       "      <td>2016-03-13 10:24:15.636</td>\n",
       "      <td>Cant completely remove custom module after put...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XD-3749</td>\n",
       "      <td>xd</td>\n",
       "      <td>grussell</td>\n",
       "      <td>description</td>\n",
       "      <td>2016-02-26 16:01:28.028</td>\n",
       "      <td>Custom conversion is broken.\\r\\r\\n\\r\\r\\nIf the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XD-3748</td>\n",
       "      <td>xd</td>\n",
       "      <td>dgarcia</td>\n",
       "      <td>description</td>\n",
       "      <td>2016-02-24 10:51:45.747</td>\n",
       "      <td>If I try to use &lt;int:message-history/&gt; when de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       key project    author        field                  created  \\\n",
       "0  XD-3750      xd  aliiqbal      summary  2016-03-13 10:24:15.636   \n",
       "1  XD-3749      xd  grussell  description  2016-02-26 16:01:28.028   \n",
       "2  XD-3748      xd   dgarcia  description  2016-02-24 10:51:45.747   \n",
       "\n",
       "                                                text  \n",
       "0  Cant completely remove custom module after put...  \n",
       "1  Custom conversion is broken.\\r\\r\\n\\r\\r\\nIf the...  \n",
       "2  If I try to use <int:message-history/> when de...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's export the dataset before cleaning values, to compare afterwards.\n",
    "log_cols.to_csv('original_text_cols.csv')\n",
    "log_cols.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.4 Clean the texts from the unwanted parts and characters </h3> <br>\n",
    "Develoeprs tend to write the code snippets, error messages, system logs and traces and other kinds of technical information into the Jira system. On the other hand, to get the personality insights from the texts written by the developers, we need to have clean texts, therefore we need to detect and remove all the unneccessary parts from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the whitespaces. \n",
    "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
    "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
    "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
    "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
    "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Clean the texts: \n",
    "def removeCodeSnippet(text):\n",
    "    text = str(text).replace('&nbsp;', ' ')\n",
    "    text = str(text).replace('sp_executesql', ' ')\n",
    "    text = str(text).replace('exec', ' ')\n",
    "    # remove not formatted code and trace part\n",
    "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
    "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
    "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
    "    # remove html tags:\n",
    "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
    "    # remove another type code snippets:\n",
    "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
    "    #remove tags\n",
    "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
    "    #remove java calls \n",
    "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
    "    # remove module calls\n",
    "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
    "    # remove job calls\n",
    "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
    "    # remove SQL Begin-end transactions\n",
    "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
    "    # remove SQL SELECT Statements\n",
    "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
    "    # remove SQL INSERT statements\n",
    "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
    "    # remove SQL DELETE statements\n",
    "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
    "    # remove system version information part\n",
    "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
    "    # remove deployment system descriptions\n",
    "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
    "    #remove system component descriptions\n",
    "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
    "    # remove system generated headers within description\n",
    "    text = text.replace('***Description', '')\n",
    "    text = text.replace('***Steps to recreate the problem', '')\n",
    "    text = text.replace('***Error Message:', '')\n",
    "    # remove square brakets with one word in it (since they are tags)\n",
    "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
    "    #remove web links:\n",
    "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
    "    #remove local path links (with slashes)\n",
    "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
    "    #remove local path links (with backslashes)\n",
    "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
    "    #remove logs within asterisks\n",
    "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
    "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
    "    #remove text with more than 18 character, that usually are the command codes. \n",
    "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
    "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
    "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
    "    #remove  call commands with \"--\"\n",
    "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
    "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
    "    #text = re.sub(\"-\\S*\", '', str(text))\n",
    "    # remove call commands with \"--\"\n",
    "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
    "    # remove sql SELECT statements\n",
    "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
    "    # remove websites and one dotted version numbers\n",
    "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
    "    # remove words containing :\n",
    "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
    "    # remove command words and versions\n",
    "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
    "    # remove multiple 'at' left after the code snippets cleaning\n",
    "    text = text.replace('at at ', ' ') \n",
    "    #remove multiple whitespaces (needed for removing 'at at' texts, regex is the next command below)\n",
    "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
    "    # remove multiple 'at'\n",
    "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
    "    # remove the non-textual characters\n",
    "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\|#)', ' ', str(text))\n",
    "    # remove non-unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
    "    # remove dates:\n",
    "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
    "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
    "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
    "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
    "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
    "    #remove multiple whitespaces\n",
    "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply defined cleaning regex functions to actual text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
    "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field\n",
      "Acceptance Criteria      97\n",
      "Business Value            1\n",
      "Comment                 522\n",
      "Epic Name                57\n",
      "Epic/Theme              414\n",
      "Migration Impact          6\n",
      "Out of Scope             10\n",
      "QA Test Plan             21\n",
      "description            4227\n",
      "summary                3695\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check the number of rows per field type\n",
    "print(log_cols.groupby('field').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the number of values, we can ignore all the fields except ***Description, summary and comments***\n",
    "let's export and manually check each of these separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment',\n",
    "                                            'Acceptance Criteria', 'Migration Impact', 'QA Test Plan', 'Out of Scope'])]\n",
    "# create datasets for each field type\n",
    "descriptions = log_cols[log_cols['field'] == 'description']\n",
    "summaries = log_cols[log_cols['field'] == 'summary']\n",
    "comments = log_cols[log_cols['field'] == 'Comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.5 filter the dataset rows </h3> <hr/>\n",
    "Add the text length column into the combined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
    "log_cols['textLength'] = log_cols['text'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dataset for visible outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>project</th>\n",
       "      <th>author</th>\n",
       "      <th>field</th>\n",
       "      <th>created</th>\n",
       "      <th>text</th>\n",
       "      <th>textLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3923</th>\n",
       "      <td>MESOS-830</td>\n",
       "      <td>mesos</td>\n",
       "      <td>greggomann</td>\n",
       "      <td>description</td>\n",
       "      <td>2015-08-03 21:14:43.067</td>\n",
       "      <td>Identify the cause of the following test failu...</td>\n",
       "      <td>13503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>DNN-7299</td>\n",
       "      <td>dnn</td>\n",
       "      <td>zyhfish</td>\n",
       "      <td>description</td>\n",
       "      <td>2015-07-28 02:39:58.500</td>\n",
       "      <td>Clean build warnings in platform solution. War...</td>\n",
       "      <td>8622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3801</th>\n",
       "      <td>MESOS-2324</td>\n",
       "      <td>mesos</td>\n",
       "      <td>xujyan</td>\n",
       "      <td>description</td>\n",
       "      <td>2015-02-09 21:26:05.755</td>\n",
       "      <td>Using temporary directory I0206 15065 Opened d...</td>\n",
       "      <td>7666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>APSTUD-4677</td>\n",
       "      <td>apstud</td>\n",
       "      <td>ingo</td>\n",
       "      <td>description</td>\n",
       "      <td>2012-04-23 11:32:44</td>\n",
       "      <td>I've already tried to start Aptana as administ...</td>\n",
       "      <td>7001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7794</th>\n",
       "      <td>TISTUD-6258</td>\n",
       "      <td>tistud</td>\n",
       "      <td>cwilliams</td>\n",
       "      <td>Comment</td>\n",
       "      <td>2014-07-10 14:08:40</td>\n",
       "      <td>Unless I'm missing it, there is no way to atta...</td>\n",
       "      <td>6599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>APSTUD-4842</td>\n",
       "      <td>apstud</td>\n",
       "      <td>cwilliams</td>\n",
       "      <td>description</td>\n",
       "      <td>2012-05-29 11:13:24.000</td>\n",
       "      <td>Option JSHint Option a label ' ' is a statemen...</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>DNN-5978</td>\n",
       "      <td>dnn</td>\n",
       "      <td>francesco.rivola</td>\n",
       "      <td>description</td>\n",
       "      <td>2015-12-11 16:27:32.060</td>\n",
       "      <td>- Installed with CE - Log in as Host, in Host ...</td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>DNN-7231</td>\n",
       "      <td>dnn</td>\n",
       "      <td>KenGrierson</td>\n",
       "      <td>Comment</td>\n",
       "      <td>2015-08-21 18:00:19.634</td>\n",
       "      <td>Upload a few dozen .txt files with unique file...</td>\n",
       "      <td>4093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5691</th>\n",
       "      <td>TIMOB-18956</td>\n",
       "      <td>timob</td>\n",
       "      <td>patakijv</td>\n",
       "      <td>description</td>\n",
       "      <td>2015-05-26 21:42:06</td>\n",
       "      <td>The documentation confirms that this is missin...</td>\n",
       "      <td>3635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>DNN-8141</td>\n",
       "      <td>dnn</td>\n",
       "      <td>george.alatrash</td>\n",
       "      <td>description</td>\n",
       "      <td>2015-12-22 00:45:21.859</td>\n",
       "      <td>A moniker is a simple name provided for an obj...</td>\n",
       "      <td>3537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>APSTUD-4113</td>\n",
       "      <td>apstud</td>\n",
       "      <td>klindsey</td>\n",
       "      <td>description</td>\n",
       "      <td>2012-05-17 13:38:37</td>\n",
       "      <td>refactor out utility classes to determine if a...</td>\n",
       "      <td>3284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4510</th>\n",
       "      <td>NEXUS-8883</td>\n",
       "      <td>nexus</td>\n",
       "      <td>jtom</td>\n",
       "      <td>description</td>\n",
       "      <td>2015-06-18 21:08:21.659</td>\n",
       "      <td>Testing NEXUS-8846, I noticed that sorting by ...</td>\n",
       "      <td>3208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>APSTUD-4840</td>\n",
       "      <td>apstud</td>\n",
       "      <td>neill</td>\n",
       "      <td>description</td>\n",
       "      <td>2012-05-21 06:16:55.000</td>\n",
       "      <td>This is a clone of bug APSTUD 4661 as I made t...</td>\n",
       "      <td>3188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>DNN-8530</td>\n",
       "      <td>dnn</td>\n",
       "      <td>KenGrierson</td>\n",
       "      <td>description</td>\n",
       "      <td>2016-04-06 21:01:23.972</td>\n",
       "      <td>Install InBone (Issimo Digital) customer site ...</td>\n",
       "      <td>2869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4428</th>\n",
       "      <td>NEXUS-9221</td>\n",
       "      <td>nexus</td>\n",
       "      <td>jtom</td>\n",
       "      <td>description</td>\n",
       "      <td>2015-09-23 17:54:15.312</td>\n",
       "      <td>Running proxy test against NX3, I found that I...</td>\n",
       "      <td>2776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>DNN-3344</td>\n",
       "      <td>dnn</td>\n",
       "      <td>Alexey.tregub</td>\n",
       "      <td>description</td>\n",
       "      <td>2013-08-14 17:57:59.894</td>\n",
       "      <td>Inconsistent behaviour of Update page URL butt...</td>\n",
       "      <td>2751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>MESOS-1817</td>\n",
       "      <td>mesos</td>\n",
       "      <td>nnielsen</td>\n",
       "      <td>description</td>\n",
       "      <td>2014-09-19 00:18:32.272</td>\n",
       "      <td>We have run into a problem that cause tasks wh...</td>\n",
       "      <td>2689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3587</th>\n",
       "      <td>MESOS-3072</td>\n",
       "      <td>mesos</td>\n",
       "      <td>arojas</td>\n",
       "      <td>description</td>\n",
       "      <td>2015-08-12 14:39:54.428</td>\n",
       "      <td>As it stands right now, default of modularized...</td>\n",
       "      <td>2664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>DNN-3263</td>\n",
       "      <td>dnn</td>\n",
       "      <td>philip.beadle</td>\n",
       "      <td>description</td>\n",
       "      <td>2013-08-05 22:46:53.636</td>\n",
       "      <td>Developers who want to build extensions for th...</td>\n",
       "      <td>2650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>DNN-4246</td>\n",
       "      <td>dnn</td>\n",
       "      <td>Alexey.tregub</td>\n",
       "      <td>description</td>\n",
       "      <td>2013-12-03 00:50:56.244</td>\n",
       "      <td>The page permission UI behavior is confusing (...</td>\n",
       "      <td>2630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              key project            author        field  \\\n",
       "3923    MESOS-830   mesos        greggomann  description   \n",
       "1861     DNN-7299     dnn           zyhfish  description   \n",
       "3801   MESOS-2324   mesos            xujyan  description   \n",
       "2763  APSTUD-4677  apstud              ingo  description   \n",
       "7794  TISTUD-6258  tistud         cwilliams      Comment   \n",
       "2689  APSTUD-4842  apstud         cwilliams  description   \n",
       "2157     DNN-5978     dnn  francesco.rivola  description   \n",
       "1877     DNN-7231     dnn       KenGrierson      Comment   \n",
       "5691  TIMOB-18956   timob          patakijv  description   \n",
       "1640     DNN-8141     dnn   george.alatrash  description   \n",
       "2919  APSTUD-4113  apstud          klindsey  description   \n",
       "4510   NEXUS-8883   nexus              jtom  description   \n",
       "2691  APSTUD-4840  apstud             neill  description   \n",
       "1605     DNN-8530     dnn       KenGrierson  description   \n",
       "4428   NEXUS-9221   nexus              jtom  description   \n",
       "2492     DNN-3344     dnn     Alexey.tregub  description   \n",
       "3874   MESOS-1817   mesos          nnielsen  description   \n",
       "3587   MESOS-3072   mesos            arojas  description   \n",
       "2499     DNN-3263     dnn     philip.beadle  description   \n",
       "2361     DNN-4246     dnn     Alexey.tregub  description   \n",
       "\n",
       "                      created  \\\n",
       "3923  2015-08-03 21:14:43.067   \n",
       "1861  2015-07-28 02:39:58.500   \n",
       "3801  2015-02-09 21:26:05.755   \n",
       "2763      2012-04-23 11:32:44   \n",
       "7794      2014-07-10 14:08:40   \n",
       "2689  2012-05-29 11:13:24.000   \n",
       "2157  2015-12-11 16:27:32.060   \n",
       "1877  2015-08-21 18:00:19.634   \n",
       "5691      2015-05-26 21:42:06   \n",
       "1640  2015-12-22 00:45:21.859   \n",
       "2919      2012-05-17 13:38:37   \n",
       "4510  2015-06-18 21:08:21.659   \n",
       "2691  2012-05-21 06:16:55.000   \n",
       "1605  2016-04-06 21:01:23.972   \n",
       "4428  2015-09-23 17:54:15.312   \n",
       "2492  2013-08-14 17:57:59.894   \n",
       "3874  2014-09-19 00:18:32.272   \n",
       "3587  2015-08-12 14:39:54.428   \n",
       "2499  2013-08-05 22:46:53.636   \n",
       "2361  2013-12-03 00:50:56.244   \n",
       "\n",
       "                                                   text  textLength  \n",
       "3923  Identify the cause of the following test failu...       13503  \n",
       "1861  Clean build warnings in platform solution. War...        8622  \n",
       "3801  Using temporary directory I0206 15065 Opened d...        7666  \n",
       "2763  I've already tried to start Aptana as administ...        7001  \n",
       "7794  Unless I'm missing it, there is no way to atta...        6599  \n",
       "2689  Option JSHint Option a label ' ' is a statemen...        5648  \n",
       "2157  - Installed with CE - Log in as Host, in Host ...        4121  \n",
       "1877  Upload a few dozen .txt files with unique file...        4093  \n",
       "5691  The documentation confirms that this is missin...        3635  \n",
       "1640  A moniker is a simple name provided for an obj...        3537  \n",
       "2919  refactor out utility classes to determine if a...        3284  \n",
       "4510  Testing NEXUS-8846, I noticed that sorting by ...        3208  \n",
       "2691  This is a clone of bug APSTUD 4661 as I made t...        3188  \n",
       "1605  Install InBone (Issimo Digital) customer site ...        2869  \n",
       "4428  Running proxy test against NX3, I found that I...        2776  \n",
       "2492  Inconsistent behaviour of Update page URL butt...        2751  \n",
       "3874  We have run into a problem that cause tasks wh...        2689  \n",
       "3587  As it stands right now, default of modularized...        2664  \n",
       "2499  Developers who want to build extensions for th...        2650  \n",
       "2361  The page permission UI behavior is confusing (...        2630  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cols.sort_values('textLength', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers\n",
    "# I have checked them manually, \n",
    "#these are the ones that the text cleaning functions could not properly clean and contain mostly the code snippet or logs\n",
    "#log_cols = log_cols.drop([3923, 1861, 3801, 2763, 7794, 2157])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export dataset to csv file and explore manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cols.to_csv('log_cols.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of all text lengths.\n",
    "also let's take a look at the distribution of the text lengths which falls in the top 90% of all values, the bottom 10% of the values and the ones that fall in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr of rows from 5% to 95%:  7707\n",
      "nr of rows above 95% and minimum length of them:  429 839\n",
      "nr of rows below 5% and maximum length of them:  442 24\n",
      "Total number of rows in dataset 8578\n"
     ]
    }
   ],
   "source": [
    "cut_val = round((int(log_cols.shape[0]) * 0.05))\n",
    "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
    "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
    "\n",
    "print('nr of rows from 5% to 95%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
    "print('nr of rows above 95% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
    "print('nr of rows below 5% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
    "print('Total number of rows in dataset', log_cols.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the histogram plot function that shows percentage of each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, _color,\n",
    "                  _data2, _bins2, _title2, \n",
    "                  _data3, _bins3, _title3, \n",
    "                  _data4, _bins4, _title4):\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    for i in range(0, 4):\n",
    "        if i ==0:\n",
    "            _data=_data1\n",
    "            _bins=_bins1\n",
    "            _title=_title1\n",
    "        elif i ==1:\n",
    "            _data=_data2\n",
    "            _bins=_bins2\n",
    "            _title=_title2\n",
    "        elif i ==2:\n",
    "            _data=_data3\n",
    "            _bins=_bins3\n",
    "            _title=_title3\n",
    "        elif i ==3:\n",
    "            _data=_data4\n",
    "            _bins=_bins4\n",
    "            _title=_title4\n",
    "        ax = fig.add_subplot(2,2,i+1)\n",
    "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
    "        ax.set_xticks(bins.round(0))\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(_title, fontsize=20)\n",
    "        plt.ylabel(_ylabel, fontsize=15)\n",
    "        plt.xlabel(_xlabel, fontsize=15)\n",
    "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
    "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
    "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
    "        for i in range(len(bins)-1):\n",
    "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
    "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_with_perc(log_cols['textLength'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
    "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
    "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
    "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,15,'the texts between 5%-95% percentile length \\n in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After analysis, need to decide to remove certain percent of the longest and shortest texts.**\n",
    "Currently, plan to remove top 1% and bottom 2% of rows. \n",
    "<hr/>\n",
    "*Remove the rows which have too long and too short texts code below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in dataset: 8578\n",
      "Number of top 1% of the rows:  86\n",
      "Number of bottom 2% of the rows:  172\n",
      "length of the text, above which to filter out the rows:  1504\n",
      "length of the text, below which to filter out the rows:  13\n"
     ]
    }
   ],
   "source": [
    "#detect the number of rows that fall in top % and bottom %\n",
    "top= 0.01\n",
    "bottom = 0.02\n",
    "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
    "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
    "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
    "print('Number of top '+str(round(top*100))+'% of the rows: ', str(cutoff_percent_top))\n",
    "print('Number of bottom '+str(round(bottom*100))+'% of the rows: ', str(cutoff_percent_bottom))\n",
    "\n",
    "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
    "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
    "\n",
    "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
    "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
    "\n",
    "#log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
    "#log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.6 Combine all texts by developers </h3><br>\n",
    "group the texts together that are manually written by the same developer in the same project. The whole texts willbe used as an input for third-party personality insights API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(618, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>count_of_texts</th>\n",
       "      <th>words_in_text</th>\n",
       "      <th>texts_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xd</td>\n",
       "      <td>aliiqbal</td>\n",
       "      <td>Cant completely remove custom module after put...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xd</td>\n",
       "      <td>grussell</td>\n",
       "      <td>Custom conversion is broken. If the custom doe...</td>\n",
       "      <td>59</td>\n",
       "      <td>1891</td>\n",
       "      <td>11088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xd</td>\n",
       "      <td>dgarcia</td>\n",
       "      <td>If I try to use when developing a Spring XD mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  project      user                                               text  \\\n",
       "0      xd  aliiqbal  Cant completely remove custom module after put...   \n",
       "1      xd  grussell  Custom conversion is broken. If the custom doe...   \n",
       "2      xd   dgarcia  If I try to use when developing a Spring XD mo...   \n",
       "\n",
       "   count_of_texts  words_in_text  texts_length  \n",
       "0               1              9            64  \n",
       "1              59           1891         11088  \n",
       "2               1             30           152  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
    "# store the project, author and text data into series and form a data frame.\n",
    "\n",
    "df_proj_name = []\n",
    "df_user_name = []\n",
    "df_user_text = []\n",
    "df_texts_count = []\n",
    "df_texts_length = []\n",
    "df_words_in_text = []\n",
    "for project in log_cut['project'].unique():\n",
    "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
    "        user_txt = ''       \n",
    "        texts_count = 0\n",
    "        texts_length = 0\n",
    "        words_in_text = 0\n",
    "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
    "        for index, row in curr_df.iterrows():\n",
    "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
    "            texts_count = texts_count + 1\n",
    "            texts_length = texts_length + len(row['text'])\n",
    "            words_in_text = words_in_text + len(row['text'].split()) \n",
    "        df_proj_name.append(project)\n",
    "        df_user_name.append(dev_user)\n",
    "        df_user_text.append(user_txt)\n",
    "        df_texts_count.append(texts_count)\n",
    "        df_texts_length.append(texts_length)\n",
    "        df_words_in_text.append(words_in_text)\n",
    "    \n",
    "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
    "                   'user':df_user_name, \n",
    "                   'text':df_user_text,\n",
    "                    'count_of_texts':df_texts_count,\n",
    "                    'words_in_text':df_words_in_text,\n",
    "                    'texts_length':df_texts_length})\n",
    "     \n",
    "user_text_combined.to_csv('user_text_combined.csv')\n",
    "print(user_text_combined.shape)\n",
    "user_text_combined.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">!!! Important notice</font>** IBM Watson says that minimum of 600 words are required for the proper personality report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_text_combined[user_text_combined['words_in_text']>=600].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combine the texts of the users, that are present in different projects.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the unique users per Name(displayName), Email(emailAddress), Username(name), Project(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = users[['displayName','emailAddress','name','project']]\n",
    "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the users dataframe with the user_texts dataframe with username and project code <br/>\n",
    "Clean the email addresses, to match one normalized format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_all_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
    "                                   left_on = ['project', 'user'],\n",
    "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
    "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot org', '.org')\n",
    "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot io', '.io')\n",
    "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot me', '.me')\n",
    "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot com', '.com')\n",
    "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot fr', '.fr')\n",
    "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot ', '.')\n",
    "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' at ', '@')\n",
    "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the users with emails, that are present in more than one project.<br/>\n",
    "Identify the users with emails, that are present in only one project. <br />\n",
    "identify the users with no emails.<br>\n",
    "For the users that are present in more than one project, combine texts from all projects, and update the number of words/textlength columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_proj = user_all_texts_emails[['user', 'emailAddress', 'project']].groupby(['user', 'emailAddress']).count()\n",
    "users_proj.reset_index(level= [0,1], inplace=True)\n",
    "\n",
    "users_with_duplicates = users_proj[users_proj['project']>1][['user', 'emailAddress']]\n",
    "users_with_single = users_proj[users_proj['project']==1][['user', 'emailAddress']]\n",
    "users_with_single_noemail=user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True][['user', 'emailAddress']]\n",
    "\n",
    "for i in range(0, users_with_duplicates.shape[0]):\n",
    "    user_name = users_with_duplicates.iloc[i]['user']\n",
    "    user_email = users_with_duplicates.iloc[i]['emailAddress']   \n",
    "    df = (user_all_texts_emails[(user_all_texts_emails['user']==user_name) & (user_all_texts_emails['emailAddress']==user_email)])\n",
    "    text_=''\n",
    "    count_of_texts_=0\n",
    "    words_in_text_=0\n",
    "    texts_length_=0\n",
    "    for k in range(0, df.shape[0]):\n",
    "        text_ = text_ + df.iloc[k]['text']\n",
    "        count_of_texts_ = count_of_texts_ + df.iloc[k]['count_of_texts']\n",
    "        words_in_text_ = words_in_text_ + df.iloc[k]['words_in_text']\n",
    "        texts_length_ = texts_length_ + df.iloc[k]['texts_length']\n",
    "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
    "                              & (user_all_texts_emails['emailAddress']==user_email), 'text']=text_\n",
    "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
    "                              & (user_all_texts_emails['emailAddress']==user_email), 'count_of_texts']=count_of_texts_\n",
    "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
    "                              & (user_all_texts_emails['emailAddress']==user_email), 'words_in_text']=words_in_text_\n",
    "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
    "                              & (user_all_texts_emails['emailAddress']==user_email), 'texts_length']=texts_length_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check, after these changes, how many users are valid for IBM Watson Personality Insights check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before these changes:  108 \n",
      " after these changes:  136\n"
     ]
    }
   ],
   "source": [
    "print('before these changes: ', user_text_combined[user_text_combined['words_in_text']>=600].shape[0], '\\n',\n",
    "      'after these changes: ', user_all_texts_emails[user_all_texts_emails['words_in_text']>=600].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>count_of_texts</th>\n",
       "      <th>words_in_text</th>\n",
       "      <th>texts_length</th>\n",
       "      <th>emailAddress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xd</td>\n",
       "      <td>grussell</td>\n",
       "      <td>Custom conversion is broken. If the custom doe...</td>\n",
       "      <td>59</td>\n",
       "      <td>1891</td>\n",
       "      <td>11088</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xd</td>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>Update Spring-AMQP to RabbitMQ Client to. Enab...</td>\n",
       "      <td>340</td>\n",
       "      <td>6429</td>\n",
       "      <td>38305</td>\n",
       "      <td>mpollack@gopivotal.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xd</td>\n",
       "      <td>david_geary</td>\n",
       "      <td>All modules that allow groovy (filter, script,...</td>\n",
       "      <td>7</td>\n",
       "      <td>886</td>\n",
       "      <td>5063</td>\n",
       "      <td>d.geary@sophiasearch.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xd</td>\n",
       "      <td>dturanski</td>\n",
       "      <td>User provides a jar file exposing a custom bea...</td>\n",
       "      <td>57</td>\n",
       "      <td>1409</td>\n",
       "      <td>8615</td>\n",
       "      <td>dturanski@gopivotal.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xd</td>\n",
       "      <td>sabby</td>\n",
       "      <td>Spring flo issue with unexpected char. As a de...</td>\n",
       "      <td>390</td>\n",
       "      <td>7131</td>\n",
       "      <td>41490</td>\n",
       "      <td>sanandan@pivotal.io</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  project          user                                               text  \\\n",
       "1      xd      grussell  Custom conversion is broken. If the custom doe...   \n",
       "3      xd  mark.pollack  Update Spring-AMQP to RabbitMQ Client to. Enab...   \n",
       "4      xd   david_geary  All modules that allow groovy (filter, script,...   \n",
       "5      xd     dturanski  User provides a jar file exposing a custom bea...   \n",
       "6      xd         sabby  Spring flo issue with unexpected char. As a de...   \n",
       "\n",
       "   count_of_texts  words_in_text  texts_length              emailAddress  \n",
       "1              59           1891         11088    grussell@gopivotal.com  \n",
       "3             340           6429         38305    mpollack@gopivotal.com  \n",
       "4               7            886          5063  d.geary@sophiasearch.com  \n",
       "5              57           1409          8615   dturanski@gopivotal.com  \n",
       "6             390           7131         41490       sanandan@pivotal.io  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_all_texts_emails[user_all_texts_emails['words_in_text']>=600].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Identify duplicated users accounts and show the unique users that meet words coutn criteria of IBM Watson </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the users with texts that match IBM Watson criteria - minimum 600 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_user_texts = user_all_texts_emails[user_all_texts_emails['words_in_text']>=600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>count_of_texts</th>\n",
       "      <th>words_in_text</th>\n",
       "      <th>texts_length</th>\n",
       "      <th>emailAddress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xd</td>\n",
       "      <td>grussell</td>\n",
       "      <td>Custom conversion is broken. If the custom doe...</td>\n",
       "      <td>59</td>\n",
       "      <td>1891</td>\n",
       "      <td>11088</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xd</td>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>Update Spring-AMQP to RabbitMQ Client to. Enab...</td>\n",
       "      <td>340</td>\n",
       "      <td>6429</td>\n",
       "      <td>38305</td>\n",
       "      <td>mpollack@gopivotal.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xd</td>\n",
       "      <td>david_geary</td>\n",
       "      <td>All modules that allow groovy (filter, script,...</td>\n",
       "      <td>7</td>\n",
       "      <td>886</td>\n",
       "      <td>5063</td>\n",
       "      <td>d.geary@sophiasearch.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  project          user                                               text  \\\n",
       "1      xd      grussell  Custom conversion is broken. If the custom doe...   \n",
       "3      xd  mark.pollack  Update Spring-AMQP to RabbitMQ Client to. Enab...   \n",
       "4      xd   david_geary  All modules that allow groovy (filter, script,...   \n",
       "\n",
       "   count_of_texts  words_in_text  texts_length              emailAddress  \n",
       "1              59           1891         11088    grussell@gopivotal.com  \n",
       "3             340           6429         38305    mpollack@gopivotal.com  \n",
       "4               7            886          5063  d.geary@sophiasearch.com  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(valid_user_texts.shape[0])\n",
    "valid_user_texts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some users have emailAddress value missing ('NaN'), we need to transform NaN value to '-' to make sure duplicates and group_by functions work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique users in valid user texts dataset:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>emailAddress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>mpollack@gopivotal.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>david_geary</td>\n",
       "      <td>d.geary@sophiasearch.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user              emailAddress\n",
       "1      grussell    grussell@gopivotal.com\n",
       "3  mark.pollack    mpollack@gopivotal.com\n",
       "4   david_geary  d.geary@sophiasearch.com"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_user_texts.loc[pd.isnull(valid_user_texts.emailAddress)==True,'emailAddress'] = '-'\n",
    "#valid_user_texts[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
    "print('Total number of unique users in valid user texts dataset: ', valid_user_texts[['user','emailAddress']].drop_duplicates().shape[0])\n",
    "valid_user_texts[['user','emailAddress']].drop_duplicates().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the unique users per each project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tistud</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timob</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dnn</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xd</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mesos</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nexus</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apstud</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mule</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user\n",
       "project      \n",
       "tistud     33\n",
       "timob      30\n",
       "dnn        20\n",
       "xd         15\n",
       "mesos      12\n",
       "nexus      11\n",
       "apstud     10\n",
       "mule        5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#valid_user_texts.groupby(['project']).count().to_csv('project_users.csv')\n",
    "valid_user_texts[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prepare data for watson</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the unique users and their texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>emailAddress</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>Custom conversion is broken. If the custom doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>mpollack@gopivotal.com</td>\n",
       "      <td>Update Spring-AMQP to RabbitMQ Client to. Enab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>david_geary</td>\n",
       "      <td>d.geary@sophiasearch.com</td>\n",
       "      <td>All modules that allow groovy (filter, script,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dturanski</td>\n",
       "      <td>dturanski@gopivotal.com</td>\n",
       "      <td>User provides a jar file exposing a custom bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sabby</td>\n",
       "      <td>sanandan@pivotal.io</td>\n",
       "      <td>Spring flo issue with unexpected char. As a de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user              emailAddress  \\\n",
       "1      grussell    grussell@gopivotal.com   \n",
       "3  mark.pollack    mpollack@gopivotal.com   \n",
       "4   david_geary  d.geary@sophiasearch.com   \n",
       "5     dturanski   dturanski@gopivotal.com   \n",
       "6         sabby       sanandan@pivotal.io   \n",
       "\n",
       "                                                text  \n",
       "1  Custom conversion is broken. If the custom doe...  \n",
       "3  Update Spring-AMQP to RabbitMQ Client to. Enab...  \n",
       "4  All modules that allow groovy (filter, script,...  \n",
       "5  User provides a jar file exposing a custom bea...  \n",
       "6  Spring flo issue with unexpected char. As a de...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_user_texts_unique = valid_user_texts[['user','emailAddress', 'text']].drop_duplicates()\n",
    "print(valid_user_texts_unique.shape[0])\n",
    "valid_user_texts_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_user_texts_unique.to_csv('valid_user_texts_unique.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>emailAddress</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>Custom conversion is broken. If the custom doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>mpollack@gopivotal.com</td>\n",
       "      <td>Update Spring-AMQP to RabbitMQ Client to. Enab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>david_geary</td>\n",
       "      <td>d.geary@sophiasearch.com</td>\n",
       "      <td>All modules that allow groovy (filter, script,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dturanski</td>\n",
       "      <td>dturanski@gopivotal.com</td>\n",
       "      <td>User provides a jar file exposing a custom bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sabby</td>\n",
       "      <td>sanandan@pivotal.io</td>\n",
       "      <td>Spring flo issue with unexpected char. As a de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user              emailAddress  \\\n",
       "1      grussell    grussell@gopivotal.com   \n",
       "3  mark.pollack    mpollack@gopivotal.com   \n",
       "4   david_geary  d.geary@sophiasearch.com   \n",
       "5     dturanski   dturanski@gopivotal.com   \n",
       "6         sabby       sanandan@pivotal.io   \n",
       "\n",
       "                                                text  \n",
       "1  Custom conversion is broken. If the custom doe...  \n",
       "3  Update Spring-AMQP to RabbitMQ Client to. Enab...  \n",
       "4  All modules that allow groovy (filter, script,...  \n",
       "5  User provides a jar file exposing a custom bea...  \n",
       "6  Spring flo issue with unexpected char. As a de...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_user_texts_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Get IBM Watson personality insight results </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nanother account credentials:\\nBFsOsCFLvQgZHLicoAj_ywnJ_92h0ry0gTgKudmuDmjm\\nhttps://gateway-lon.watsonplatform.net/personality-insights/api\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "from os.path import join, dirname\n",
    "from ibm_watson import PersonalityInsightsV3\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "service = PersonalityInsightsV3(\n",
    "    version='2017-10-13',\n",
    "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
    "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
    "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
    "\n",
    "'''\n",
    "another account credentials:\n",
    "BFsOsCFLvQgZHLicoAj_ywnJ_92h0ry0gTgKudmuDmjm\n",
    "https://gateway-lon.watsonplatform.net/personality-insights/api\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
    "                                           'openness', 'o_raw', 'o_sign',\n",
    "                                           'conscientiousness', 'c_raw', 'c_sign',\n",
    "                                           'extraversion', 'e_raw', 'e_sign',\n",
    "                                           'agreeableness', 'a_raw', 'a_sign',\n",
    "                                           'neuroticism', 'n_raw', 'n_sign'))\n",
    "\n",
    "for i in range(0, valid_user_texts_unique.shape[0]):\n",
    "    user_ = valid_user_texts_unique.iloc[i]['user']\n",
    "    email_ = valid_user_texts_unique.iloc[i]['emailAddress']\n",
    "    text_ = valid_user_texts_unique.iloc[i]['text']\n",
    "    \n",
    "    # define destination json structure Save input JSON file for each user \n",
    "    data = {}\n",
    "    data['contentItems'] = []\n",
    "    data['contentItems'].append({\n",
    "        'content': text_,\n",
    "        'contenttype': 'application/json',\n",
    "        'created': datetime.now().toordinal(),\n",
    "        'id': i,\n",
    "        'language':'en',\n",
    "        'user_name':user_,\n",
    "        'email':email_\n",
    "    })\n",
    "\n",
    "    input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
    "    with open(input_json_file, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "    # Call IBM Watson Personality Insights for each user \n",
    "    with open(join(dirname('__file__'), input_json_file)) as \\\n",
    "        profile_json:\n",
    "        profile = service.profile(\n",
    "        profile_json.read(),\n",
    "        'application/json',\n",
    "        raw_scores=True,\n",
    "        consumption_preferences=False).get_result()\n",
    "\n",
    "    # Save the personality insights result into output JSON file for each user\n",
    "    output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
    "    with open(output_json_file, 'w') as outfile:\n",
    "        json.dump(profile, outfile)\n",
    "    \n",
    "    # parse JSON, loop through each personality and get the scores\n",
    "    personalities = []\n",
    "    for p in range(0, 5):\n",
    "        trait_id = profile['personality'][p]['name']\n",
    "        percentile = profile['personality'][p]['percentile']\n",
    "        significant = profile['personality'][p]['significant']\n",
    "        raw_score = profile['personality'][p]['raw_score']\n",
    "\n",
    "        if trait_id == 'Openness':\n",
    "            big5_openness = percentile\n",
    "            big5_o_raw = raw_score\n",
    "            big5_o_sign = significant\n",
    "        elif trait_id=='Conscientiousness':\n",
    "            big5_conscientiousness = percentile\n",
    "            big5_c_raw = raw_score\n",
    "            big5_c_sign = significant\n",
    "        elif trait_id == 'Extraversion':\n",
    "            big5_extraversion = percentile\n",
    "            big5_e_raw = raw_score\n",
    "            big5_e_sign = significant\n",
    "        elif trait_id == 'Agreeableness':\n",
    "            big5_agreeableness = percentile\n",
    "            big5_a_raw = raw_score\n",
    "            big5_a_sign = significant\n",
    "        elif trait_id == 'Emotional range':\n",
    "            big5_neuroticism = percentile\n",
    "            big5_n_raw = raw_score\n",
    "            big5_n_sign = significant\n",
    "    \n",
    "    # Save the user personalities into dataset\n",
    "    user_personalities = user_personalities.append([{\n",
    "        'user':user_, 'emailAddress':email_, \n",
    "       'openness':big5_openness, 'o_raw':big5_o_raw, 'o_sign':big5_o_sign,\n",
    "       'conscientiousness':big5_conscientiousness, 'c_raw':big5_c_raw, 'c_sign':big5_c_sign,\n",
    "       'extraversion':big5_extraversion, 'e_raw':big5_e_raw, 'e_sign':big5_e_sign,\n",
    "       'agreeableness':big5_agreeableness, 'a_raw':big5_a_raw, 'a_sign':big5_a_sign,\n",
    "       'neuroticism':big5_neuroticism, 'n_raw':big5_n_raw, 'n_sign':big5_n_sign\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/><font color=\"red\" size=\"4\">**!!!** read about the raw scores here: </font> <pre>https://cloud.ibm.com/docs/services/personality-insights?topic=personality-insights-numeric#rawScores-numeric </pre><hr/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_personalities=pd.read_csv('user_personalities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user</th>\n",
       "      <th>emailAddress</th>\n",
       "      <th>openness</th>\n",
       "      <th>o_raw</th>\n",
       "      <th>o_sign</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>c_raw</th>\n",
       "      <th>c_sign</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>e_raw</th>\n",
       "      <th>e_sign</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>a_raw</th>\n",
       "      <th>a_sign</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>n_raw</th>\n",
       "      <th>n_sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.809</td>\n",
       "      <td>True</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.609</td>\n",
       "      <td>True</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.500</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.590</td>\n",
       "      <td>True</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.446</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>mpollack@gopivotal.com</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.800</td>\n",
       "      <td>True</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.631</td>\n",
       "      <td>True</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.486</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.605</td>\n",
       "      <td>True</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.487</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>david_geary</td>\n",
       "      <td>d.geary@sophiasearch.com</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.807</td>\n",
       "      <td>True</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.634</td>\n",
       "      <td>True</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.529</td>\n",
       "      <td>True</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.630</td>\n",
       "      <td>True</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.573</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>dturanski</td>\n",
       "      <td>dturanski@gopivotal.com</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.806</td>\n",
       "      <td>True</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.626</td>\n",
       "      <td>True</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.493</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.606</td>\n",
       "      <td>True</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.451</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>sabby</td>\n",
       "      <td>sanandan@pivotal.io</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.778</td>\n",
       "      <td>True</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.610</td>\n",
       "      <td>True</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.493</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.610</td>\n",
       "      <td>True</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.505</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          user              emailAddress  openness  o_raw  \\\n",
       "0           0      grussell    grussell@gopivotal.com     0.970  0.809   \n",
       "1           0  mark.pollack    mpollack@gopivotal.com     0.945  0.800   \n",
       "2           0   david_geary  d.geary@sophiasearch.com     0.965  0.807   \n",
       "3           0     dturanski   dturanski@gopivotal.com     0.963  0.806   \n",
       "4           0         sabby       sanandan@pivotal.io     0.823  0.778   \n",
       "\n",
       "   o_sign  conscientiousness  c_raw  c_sign  extraversion  e_raw  e_sign  \\\n",
       "0    True              0.339  0.609    True         0.126  0.500    True   \n",
       "1    True              0.525  0.631    True         0.072  0.486    True   \n",
       "2    True              0.555  0.634    True         0.310  0.529    True   \n",
       "3    True              0.483  0.626    True         0.097  0.493    True   \n",
       "4    True              0.349  0.610    True         0.096  0.493    True   \n",
       "\n",
       "   agreeableness  a_raw  a_sign  neuroticism  n_raw  n_sign  \n",
       "0          0.000  0.590    True        0.102  0.446    True  \n",
       "1          0.000  0.605    True        0.234  0.487    True  \n",
       "2          0.002  0.630    True        0.658  0.573    True  \n",
       "3          0.000  0.606    True        0.115  0.451    True  \n",
       "4          0.000  0.610    True        0.314  0.505    True  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "user_personalities.to_csv('user_personalities.csv')\n",
    "user_personalities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14,6), sharex=False, sharey=False)\n",
    "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive',\n",
    "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
    "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism',\n",
    "         'o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
    "\n",
    "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
    "    x = user_personalities[trait_]\n",
    "    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
    "    axx.set_title(trait_)\n",
    "\n",
    "plt.suptitle('Histogram of all users personality trait raw scores and percentiles', y=1.05, size=16)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join the personalities dataframe to projects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136 136\n"
     ]
    }
   ],
   "source": [
    "user_personalities.head()\n",
    "#valid_user_texts['project', 'user', 'emailAddress']\n",
    "p_cols = ['project']\n",
    "for col in user_personalities.columns:\n",
    "    p_cols.append(col)\n",
    "project_user_personalities = pd.merge(user_personalities, valid_user_texts, how = 'inner',\n",
    "                                   left_on = ['user', 'emailAddress'],\n",
    "                                   right_on = ['user', 'emailAddress'])[p_cols]\n",
    "\n",
    "print(valid_user_texts.shape[0], project_user_personalities.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the personality traits percentiles and raw scores distribution within the eight projects. <br>\n",
    "Define and show the plot with the projects/percentiles and scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxpl(dt, x_cols, y_cols, title):\n",
    "    n = 1\n",
    "    x_cnt = len(x_cols)\n",
    "    y_cnt = len(y_cols)\n",
    "    figure = plt.figure(figsize=(18, 3.5 * x_cnt))\n",
    "    for x_ax in x_cols:\n",
    "        for i in y_cols:\n",
    "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
    "            #ax.set_title(i)\n",
    "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
    "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
    "            plt.suptitle(title, size=16)\n",
    "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
    "            n = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x252 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x252 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "x_cols = ['project']\n",
    "title = 'Personality trait percentiles for projects'\n",
    "boxpl(project_user_personalities, x_cols, y_cols, title)\n",
    "\n",
    "y_cols = ['o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
    "x_cols = ['project']\n",
    "title = 'Perconality trait raw scores for projects'\n",
    "boxpl(project_user_personalities, x_cols, y_cols, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Calculation of metrics </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Time </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define the time spent per task by the developer: time that is passed while the task was set to 'In Progress' status.\n",
    "In technical terms, this is the time between the two log records when 1) the task status was set to 'In Progress'\n",
    "and 2)task status was changed from 'In Progress'. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the records from Jira change log, that have status set from In Progress to something else, and the records that have set status set to In Progress from something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>created</th>\n",
       "      <th>field</th>\n",
       "      <th>fieldtype</th>\n",
       "      <th>from</th>\n",
       "      <th>fromString</th>\n",
       "      <th>key</th>\n",
       "      <th>project</th>\n",
       "      <th>to</th>\n",
       "      <th>toString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>jvalkeal</td>\n",
       "      <td>2016-03-03 18:40:53.171</td>\n",
       "      <td>status</td>\n",
       "      <td>jira</td>\n",
       "      <td>10000</td>\n",
       "      <td>To Do</td>\n",
       "      <td>XD-3751</td>\n",
       "      <td>xd</td>\n",
       "      <td>3</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>grussell</td>\n",
       "      <td>2016-02-29 19:43:42.553</td>\n",
       "      <td>status</td>\n",
       "      <td>jira</td>\n",
       "      <td>10000</td>\n",
       "      <td>To Do</td>\n",
       "      <td>XD-3748</td>\n",
       "      <td>xd</td>\n",
       "      <td>3</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>2016-02-23 18:18:59.225</td>\n",
       "      <td>status</td>\n",
       "      <td>jira</td>\n",
       "      <td>10000</td>\n",
       "      <td>To Do</td>\n",
       "      <td>XD-3746</td>\n",
       "      <td>xd</td>\n",
       "      <td>3</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>2016-02-23 18:04:22.616</td>\n",
       "      <td>status</td>\n",
       "      <td>jira</td>\n",
       "      <td>10000</td>\n",
       "      <td>To Do</td>\n",
       "      <td>XD-3745</td>\n",
       "      <td>xd</td>\n",
       "      <td>3</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>grussell</td>\n",
       "      <td>2016-02-22 15:49:30.944</td>\n",
       "      <td>status</td>\n",
       "      <td>jira</td>\n",
       "      <td>10000</td>\n",
       "      <td>To Do</td>\n",
       "      <td>XD-3744</td>\n",
       "      <td>xd</td>\n",
       "      <td>3</td>\n",
       "      <td>In Progress</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        author                  created   field fieldtype   from  \\\n",
       "0      0      jvalkeal  2016-03-03 18:40:53.171  status      jira  10000   \n",
       "1     12      grussell  2016-02-29 19:43:42.553  status      jira  10000   \n",
       "2     19  mark.pollack  2016-02-23 18:18:59.225  status      jira  10000   \n",
       "3     25  mark.pollack  2016-02-23 18:04:22.616  status      jira  10000   \n",
       "4     37      grussell  2016-02-22 15:49:30.944  status      jira  10000   \n",
       "\n",
       "  fromString      key project to     toString  \n",
       "0      To Do  XD-3751      xd  3  In Progress  \n",
       "1      To Do  XD-3748      xd  3  In Progress  \n",
       "2      To Do  XD-3746      xd  3  In Progress  \n",
       "3      To Do  XD-3745      xd  3  In Progress  \n",
       "4      To Do  XD-3744      xd  3  In Progress  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changelog_from_inprogress = changelog[(changelog['field']=='status') \n",
    "                                    & (changelog['fromString']=='In Progress')\n",
    "                                    & (changelog['toString']!='In Progress')].reset_index()\n",
    "changelog_to_inprogress = changelog[(changelog['field']=='status') \n",
    "                                    & (changelog['toString']=='In Progress') \n",
    "                                    & (changelog['fromString']!='In Progress')].reset_index()\n",
    "changelog_to_inprogress.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's join these two datasets, and have one row for each from-to in progress status record.\n",
    "Technically: loop through each record where task status was changed form 'In Progress' to some other status (this record indicates that developer has stopped working on it). For each of these records, find the one latest record, where task status was set to 'In Progress' (this record indicates that developer has started working on it). With this operation we will get starting and ending time of work on the task by developer, and we will just need to calculate time difference.\n",
    "<hr> In case when several developers have worked on one task - the function will calculate time for each of them separately.\n",
    "<br> In case when one developer has set status to 'In Progress' multiple times within one task, the function will calculate the sum amount, so that one developer will have one number of minutes spent for one task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "changelog_from_inprogress['prev_status'] = np.nan\n",
    "changelog_from_inprogress['prev_status_created'] = np.nan\n",
    "changelog_from_inprogress['created'] = pd.to_datetime(changelog_from_inprogress['created'])\n",
    "changelog_to_inprogress['created'] = pd.to_datetime(changelog_to_inprogress['created'])\n",
    "\n",
    "for index, row in changelog_from_inprogress.iterrows():\n",
    "    _key = row['key']\n",
    "    _project = row['project']\n",
    "    _created = row['created']\n",
    "    _to_row = changelog_to_inprogress[(((changelog_to_inprogress['key'] == _key)\n",
    "                            & (changelog_to_inprogress['project'] == _project))\n",
    "                            & (pd.to_datetime(changelog_to_inprogress['created']) < pd.to_datetime(_created)))].sort_values('created', ascending=False).head(1)\n",
    "    for st in _to_row['fromString']:\n",
    "        _prev_st = st \n",
    "    for cr in _to_row['created']:\n",
    "        _prev_st_created = cr \n",
    "    \n",
    "    changelog_from_inprogress.loc[index,'prev_status'] = _prev_st\n",
    "    changelog_from_inprogress.loc[index,'prev_status_created'] = _prev_st_created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcualte minutes spent on each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>created</th>\n",
       "      <th>field</th>\n",
       "      <th>fieldtype</th>\n",
       "      <th>from</th>\n",
       "      <th>fromString</th>\n",
       "      <th>key</th>\n",
       "      <th>project</th>\n",
       "      <th>to</th>\n",
       "      <th>toString</th>\n",
       "      <th>prev_status</th>\n",
       "      <th>prev_status_created</th>\n",
       "      <th>minutes_spent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>jvalkeal</td>\n",
       "      <td>2016-03-03 18:41:19.429</td>\n",
       "      <td>status</td>\n",
       "      <td>jira</td>\n",
       "      <td>3</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>XD-3751</td>\n",
       "      <td>xd</td>\n",
       "      <td>10006</td>\n",
       "      <td>In PR</td>\n",
       "      <td>To Do</td>\n",
       "      <td>2016-03-03 18:40:53.171000</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>grussell</td>\n",
       "      <td>2016-02-29 21:11:29.703</td>\n",
       "      <td>status</td>\n",
       "      <td>jira</td>\n",
       "      <td>3</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>XD-3748</td>\n",
       "      <td>xd</td>\n",
       "      <td>10006</td>\n",
       "      <td>In PR</td>\n",
       "      <td>To Do</td>\n",
       "      <td>2016-02-29 19:43:42.553000</td>\n",
       "      <td>87.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>2016-02-23 18:19:04.071</td>\n",
       "      <td>status</td>\n",
       "      <td>jira</td>\n",
       "      <td>3</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>XD-3746</td>\n",
       "      <td>xd</td>\n",
       "      <td>10006</td>\n",
       "      <td>In PR</td>\n",
       "      <td>To Do</td>\n",
       "      <td>2016-02-23 18:18:59.225000</td>\n",
       "      <td>0.081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        author                 created   field fieldtype from  \\\n",
       "0      2      jvalkeal 2016-03-03 18:41:19.429  status      jira    3   \n",
       "1     14      grussell 2016-02-29 21:11:29.703  status      jira    3   \n",
       "2     20  mark.pollack 2016-02-23 18:19:04.071  status      jira    3   \n",
       "\n",
       "    fromString      key project     to toString prev_status  \\\n",
       "0  In Progress  XD-3751      xd  10006    In PR       To Do   \n",
       "1  In Progress  XD-3748      xd  10006    In PR       To Do   \n",
       "2  In Progress  XD-3746      xd  10006    In PR       To Do   \n",
       "\n",
       "          prev_status_created  minutes_spent  \n",
       "0  2016-03-03 18:40:53.171000          0.438  \n",
       "1  2016-02-29 19:43:42.553000         87.786  \n",
       "2  2016-02-23 18:18:59.225000          0.081  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changelog_from_inprogress['minutes_spent']=(pd.to_datetime(changelog_from_inprogress['created'])\n",
    "                            - pd.to_datetime(changelog_from_inprogress['prev_status_created'])) / np.timedelta64(1, 'm')\n",
    "changelog_from_inprogress.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum up the time by the same users for the same tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_key_timespent=changelog_from_inprogress.groupby(['key', 'project', 'author']).agg({'minutes_spent':'sum'})\n",
    "user_key_timespent.reset_index(level= [0,1,2], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>project</th>\n",
       "      <th>author</th>\n",
       "      <th>minutes_spent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APSTUD-1378</td>\n",
       "      <td>apstud</td>\n",
       "      <td>cwilliams</td>\n",
       "      <td>340.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APSTUD-1469</td>\n",
       "      <td>apstud</td>\n",
       "      <td>mstepanov</td>\n",
       "      <td>121.633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APSTUD-1469</td>\n",
       "      <td>apstud</td>\n",
       "      <td>sgibly</td>\n",
       "      <td>54.717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key project     author  minutes_spent\n",
       "0  APSTUD-1378  apstud  cwilliams        340.033\n",
       "1  APSTUD-1469  apstud  mstepanov        121.633\n",
       "2  APSTUD-1469  apstud     sgibly         54.717"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_key_timespent.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join the time spendings dataset with the users dataset, that are valid for personality traits check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_users_times = pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
    "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', 'minutes_spent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>emailAddress</th>\n",
       "      <th>project</th>\n",
       "      <th>key</th>\n",
       "      <th>minutes_spent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-1127</td>\n",
       "      <td>0.578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-1153</td>\n",
       "      <td>1452.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-1164</td>\n",
       "      <td>73.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-128</td>\n",
       "      <td>1688.373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-1338</td>\n",
       "      <td>20246.750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user            emailAddress project      key  minutes_spent\n",
       "0  grussell  grussell@gopivotal.com      xd  XD-1127          0.578\n",
       "1  grussell  grussell@gopivotal.com      xd  XD-1153       1452.161\n",
       "2  grussell  grussell@gopivotal.com      xd  XD-1164         73.852\n",
       "3  grussell  grussell@gopivotal.com      xd   XD-128       1688.373\n",
       "4  grussell  grussell@gopivotal.com      xd  XD-1338      20246.750"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_users_times.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 8094\n"
     ]
    }
   ],
   "source": [
    "print(user_personalities.shape[0], valid_users_times.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the times spent by the developers, let's define the high, normal and low time spend categories.\n",
    "First, let's find the values where to split the rows: first < 33.33% time spend - low time spending, from 33.33% to 66.66% medium time spent, and from 66.66% and more - high time spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698 1687.0954833333333 73.58333333333333\n",
      "nr of rows from  33.33 % to  66.67 % :  2698\n",
      "nr of rows above  66.67 %:  2698\n",
      "nr of rows below,  33.33 %:  2698\n",
      "Total number of rows in dataset:  8094\n"
     ]
    }
   ],
   "source": [
    "_percentile=0.3333\n",
    "_rownumber_within_percentile = round((int(valid_users_times.shape[0]) * _percentile))\n",
    "_top_percentile_rows_filter_value_minutes = valid_users_times.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
    "_bottom_percentile_rows_filter_value_minutes = valid_users_times.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
    "\n",
    "print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
    "print('nr of rows from ', _percentile*100, '% to ',\n",
    "      str(100-100*_percentile), '% : ',\n",
    "      valid_users_times[(valid_users_times['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
    "                        ( valid_users_times['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
    "\n",
    "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
    "      valid_users_times[valid_users_times['minutes_spent']>=_top_percentile_rows_filter_value_minutes].shape[0])\n",
    "\n",
    "print('nr of rows below, ', _percentile*100, '%: ',\n",
    "      valid_users_times[valid_users_times['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
    "\n",
    "print('Total number of rows in dataset: ', valid_users_times.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_spending_category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>2698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>2698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>2698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user\n",
       "time_spending_category      \n",
       "high                    2698\n",
       "low                     2698\n",
       "medium                  2698"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_users_times['time_spending_category'] = np.nan\n",
    "\n",
    "valid_users_times.loc[(valid_users_times['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
    "                        ( valid_users_times['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)\n",
    "                  , 'time_spending_category'] = 'medium'\n",
    "\n",
    "valid_users_times.loc[valid_users_times['minutes_spent']>=_top_percentile_rows_filter_value_minutes\n",
    "                      , 'time_spending_category']='high'\n",
    "\n",
    "valid_users_times.loc[valid_users_times['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes\n",
    "                      , 'time_spending_category']='low'\n",
    "# check the actual values\n",
    "valid_users_times[['time_spending_category', 'user']].groupby('time_spending_category').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>emailAddress</th>\n",
       "      <th>project</th>\n",
       "      <th>key</th>\n",
       "      <th>minutes_spent</th>\n",
       "      <th>time_spending_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-1127</td>\n",
       "      <td>0.578</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-1153</td>\n",
       "      <td>1452.161</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-1164</td>\n",
       "      <td>73.852</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-128</td>\n",
       "      <td>1688.373</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-1338</td>\n",
       "      <td>20246.750</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user            emailAddress project      key  minutes_spent  \\\n",
       "0  grussell  grussell@gopivotal.com      xd  XD-1127          0.578   \n",
       "1  grussell  grussell@gopivotal.com      xd  XD-1153       1452.161   \n",
       "2  grussell  grussell@gopivotal.com      xd  XD-1164         73.852   \n",
       "3  grussell  grussell@gopivotal.com      xd   XD-128       1688.373   \n",
       "4  grussell  grussell@gopivotal.com      xd  XD-1338      20246.750   \n",
       "\n",
       "  time_spending_category  \n",
       "0                    low  \n",
       "1                 medium  \n",
       "2                 medium  \n",
       "3                   high  \n",
       "4                   high  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_users_times.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's group the data the following way: to have one record for each user, and columns will be number of tasks where the user had low, medium and high time spendings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>low_timespent_tasks</th>\n",
       "      <th>medium_timespent_tasks</th>\n",
       "      <th>high_timespent_tasks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grussell</td>\n",
       "      <td>28.000</td>\n",
       "      <td>35.000</td>\n",
       "      <td>22.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>68.000</td>\n",
       "      <td>73.000</td>\n",
       "      <td>169.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dturanski</td>\n",
       "      <td>40.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>71.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sabby</td>\n",
       "      <td>67.000</td>\n",
       "      <td>29.000</td>\n",
       "      <td>87.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thomas.risberg</td>\n",
       "      <td>41.000</td>\n",
       "      <td>47.000</td>\n",
       "      <td>57.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user  low_timespent_tasks  medium_timespent_tasks  \\\n",
       "0        grussell               28.000                  35.000   \n",
       "1    mark.pollack               68.000                  73.000   \n",
       "2       dturanski               40.000                  25.000   \n",
       "3           sabby               67.000                  29.000   \n",
       "4  thomas.risberg               41.000                  47.000   \n",
       "\n",
       "   high_timespent_tasks  \n",
       "0                22.000  \n",
       "1               169.000  \n",
       "2                71.000  \n",
       "3                87.000  \n",
       "4                57.000  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandasql as ps\n",
    "\n",
    "q1 = \"\"\"\n",
    "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
    "From \n",
    "    (\n",
    "    Select distinct user From valid_users_times) AS U\n",
    "    Left Join ( SELECT user, count(*) AS low_timespent_tasks\n",
    "    FROM valid_users_times AS F WHERE time_spending_category = 'low' Group By user\n",
    "    ) AS Low ON U.user = Low.user\n",
    "    Left Join (SELECT user, count(*) AS medium_timespent_tasks\n",
    "    FROM valid_users_times AS F WHERE time_spending_category = 'medium' Group By user\n",
    "    ) AS medium on U.user = medium.user\n",
    "    Left Join ( SELECT user, count(*) AS high_timespent_tasks\n",
    "    FROM valid_users_times AS F WHERE time_spending_category = 'high' Group By user\n",
    "    ) AS high ON U.user = high.user\n",
    "\"\"\"\n",
    "\n",
    "valid_user_time_agg = ps.sqldf(q1, locals())\n",
    "print(valid_user_time_agg.shape[0])\n",
    "valid_user_time_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "print(valid_user_time_agg[(pd.isnull(valid_user_time_agg['low_timespent_tasks'])==False)\n",
    "                         & (pd.isnull(valid_user_time_agg['medium_timespent_tasks'])==False)\n",
    "                         & (pd.isnull(valid_user_time_agg['high_timespent_tasks'])==False)].shape[0])\n",
    "\n",
    "print(valid_user_time_agg[(pd.isnull(valid_user_time_agg['low_timespent_tasks'])==False)\n",
    "                         | (pd.isnull(valid_user_time_agg['medium_timespent_tasks'])==False)\n",
    "                         | (pd.isnull(valid_user_time_agg['high_timespent_tasks'])==False)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Categorical variable Metrics</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the functions that will return the dataset of given metrics and their aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_metric(field, multiple_fields, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
    "    #create table from the log with this specific field\n",
    "    if len(multiple_fields)>0:\n",
    "        log_dt = changelog[(changelog['field'].isin(multiple_fields))]\n",
    "    else:\n",
    "        log_dt = changelog[(changelog['field'].isin([field]))]\n",
    "    \n",
    "    #code when exclusion field values are passed\n",
    "    if len(field_exclusion_filter) > 0:\n",
    "        #merge to issues table and add the \n",
    "        log_dt = pd.merge(log_dt, issues[['key', field_in_issues]].drop_duplicates()\n",
    "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
    "                    'project', 'to', 'toString', field_in_issues]]\n",
    "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
    "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
    "        \n",
    "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
    "    log_dt[field] = np.nan\n",
    "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
    "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
    "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
    "    \n",
    "    #join table to user personalities table\n",
    "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
    "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
    "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
    "    return valid_users_metrics\n",
    "    \n",
    "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
    "    valid_users_metrics = data\n",
    "    #Aggregate the metrics per user\n",
    "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\", \n",
    "    CASE WHEN COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) THEN '\"\"\"+cat1_label+\"\"\"'\n",
    "         WHEN COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) AND COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN '\"\"\"+cat2_label+\"\"\"'\n",
    "         WHEN COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN '\"\"\"+cat3_label+\"\"\"'\n",
    "    END AS metric\n",
    "    FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
    "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
    "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
    "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
    "    \"\"\"\n",
    "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
    "    return valid_users_metrics_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> State </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check what statuses and status categories are present in issues and changelog datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toString</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In Progress</td>\n",
       "      <td>11986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Closed</td>\n",
       "      <td>11236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Resolved</td>\n",
       "      <td>10583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Done</td>\n",
       "      <td>3201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reopened</td>\n",
       "      <td>3171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      toString  author\n",
       "0  In Progress   11986\n",
       "1       Closed   11236\n",
       "2     Resolved   10583\n",
       "3         Done    3201\n",
       "4     Reopened    3171"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statuses = changelog[changelog['field']=='status'][['toString', 'author']].groupby(\n",
    "    'toString').count().sort_values('author', ascending=False)\n",
    "statuses.reset_index(level= [0], inplace=True)\n",
    "statuses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define our own categories and assign changelog abses status names to the ones we defined.\n",
    "Then, create separate datasets for each of these defined statuses, by filtering the changelog dataset, and then aggregate these datasets to take only the user and the number of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
    "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
    "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
    "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
    "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
    "states_df = categorical_metric('status','','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
    "states_df_agg = categorical_metric_agg(states_df, 'status','todo', 'inprogress', 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32029 32029 21585\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>emailAddress</th>\n",
       "      <th>project</th>\n",
       "      <th>key</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-3748</td>\n",
       "      <td>inprogress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-3744</td>\n",
       "      <td>inprogress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-3742</td>\n",
       "      <td>inprogress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-3742</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-3739</td>\n",
       "      <td>inprogress</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user            emailAddress project      key      status\n",
       "0  grussell  grussell@gopivotal.com      xd  XD-3748  inprogress\n",
       "2  grussell  grussell@gopivotal.com      xd  XD-3744  inprogress\n",
       "4  grussell  grussell@gopivotal.com      xd  XD-3742  inprogress\n",
       "5  grussell  grussell@gopivotal.com      xd  XD-3742        done\n",
       "6  grussell  grussell@gopivotal.com      xd  XD-3739  inprogress"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(states_df.shape[0], \n",
    "      states_df.drop_duplicates().shape[0],\n",
    "      states_df[['user', 'key']].drop_duplicates().shape[0])\n",
    "states_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many from these users have all three status tasks available, or at least one status task available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "97\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>todo</th>\n",
       "      <th>inprogress</th>\n",
       "      <th>done</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grussell</td>\n",
       "      <td>5</td>\n",
       "      <td>133.000</td>\n",
       "      <td>96.000</td>\n",
       "      <td>inprogress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>26</td>\n",
       "      <td>213.000</td>\n",
       "      <td>579.000</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dturanski</td>\n",
       "      <td>11</td>\n",
       "      <td>179.000</td>\n",
       "      <td>194.000</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sabby</td>\n",
       "      <td>32</td>\n",
       "      <td>254.000</td>\n",
       "      <td>467.000</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thomas.risberg</td>\n",
       "      <td>20</td>\n",
       "      <td>177.000</td>\n",
       "      <td>156.000</td>\n",
       "      <td>inprogress</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user  todo  inprogress    done      metric\n",
       "0        grussell     5     133.000  96.000  inprogress\n",
       "1    mark.pollack    26     213.000 579.000        done\n",
       "2       dturanski    11     179.000 194.000        done\n",
       "3           sabby    32     254.000 467.000        done\n",
       "4  thomas.risberg    20     177.000 156.000  inprogress"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
    "                         & (pd.isnull(states_df_agg['todo'])==False)\n",
    "                         & (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
    "\n",
    "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
    "                         | (pd.isnull(states_df_agg['todo'])==False)\n",
    "                         | (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
    "states_df_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Task Prioritization <h/2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what are the priority values in changelog dataset, and how many records has each of these priority value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toString</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>1267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Critical</th>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Major</th>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blocker</th>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minor</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trivial</th>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>To be reviewed</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author\n",
       "toString              \n",
       "High              1978\n",
       "Medium            1267\n",
       "Critical          1031\n",
       "Major              742\n",
       "Low                670\n",
       "Blocker            254\n",
       "Minor              240\n",
       "None               166\n",
       "Trivial             89\n",
       "To be reviewed       3"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changelog[changelog['field']=='priority'][['toString', 'author']].groupby(\n",
    "    'toString').count().sort_values('author', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our own priority categories and assign changelog priority names to the ones we defined. \n",
    "Then, create separate datasets for each of these defined priorities, by filtering the changelog dataset, and then aggregate these datasets to take only the user and the number of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "_high = ['High', 'Critical', 'Blocker']\n",
    "_medium = ['Medium', 'Major']\n",
    "_low = ['Low', 'Minor', 'None', 'Trivial', 'To be reviewed']\n",
    "\n",
    "priorities_df = categorical_metric('priority','','','','',_high,'high',_medium,'medium',_low,'low')\n",
    "priorities_df_agg = categorical_metric_agg(priorities_df, 'priority','high', 'medium', 'low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5126 5126 4796\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>emailAddress</th>\n",
       "      <th>project</th>\n",
       "      <th>key</th>\n",
       "      <th>priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grussell</td>\n",
       "      <td>grussell@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-3023</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>mpollack@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-3014</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>mpollack@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-2573</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>mpollack@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-2411</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>mpollack@gopivotal.com</td>\n",
       "      <td>xd</td>\n",
       "      <td>XD-2145</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user            emailAddress project      key priority\n",
       "0      grussell  grussell@gopivotal.com      xd  XD-3023     high\n",
       "1  mark.pollack  mpollack@gopivotal.com      xd  XD-3014   medium\n",
       "2  mark.pollack  mpollack@gopivotal.com      xd  XD-2573   medium\n",
       "3  mark.pollack  mpollack@gopivotal.com      xd  XD-2411   medium\n",
       "4  mark.pollack  mpollack@gopivotal.com      xd  XD-2145   medium"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(priorities_df.shape[0], \n",
    "      priorities_df.drop_duplicates().shape[0],\n",
    "      priorities_df[['user', 'key']].drop_duplicates().shape[0])\n",
    "priorities_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many from these users have all three priority tasks available, and the number of users that have at least one piroritization available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "81\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>high</th>\n",
       "      <th>medium</th>\n",
       "      <th>low</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grussell</td>\n",
       "      <td>1.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mark.pollack</td>\n",
       "      <td>9.000</td>\n",
       "      <td>75.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dturanski</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sabby</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thomas.risberg</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user  high  medium   low  metric\n",
       "0        grussell 1.000     nan   nan    high\n",
       "1    mark.pollack 9.000  75.000 9.000  medium\n",
       "2       dturanski   nan   2.000   nan  medium\n",
       "3           sabby 2.000   2.000 4.000     low\n",
       "4  thomas.risberg 3.000   2.000   nan    high"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
    "                         & (pd.isnull(priorities_df_agg['medium'])==False)\n",
    "                         & (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
    "\n",
    "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
    "                         | (pd.isnull(priorities_df_agg['medium'])==False)\n",
    "                         | (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
    "priorities_df_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Estimation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'apstud'),\n",
       " Text(0, 1.5, 'dnn'),\n",
       " Text(0, 2.5, 'mesos'),\n",
       " Text(0, 3.5, 'mule'),\n",
       " Text(0, 4.5, 'nexus'),\n",
       " Text(0, 5.5, 'timob'),\n",
       " Text(0, 6.5, 'tistud'),\n",
       " Text(0, 7.5, 'xd')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_keys = issues[['key', 'project', 'storypoints', 'fields.issuetype.name']].drop_duplicates()\n",
    "proj_story_tab = pd.crosstab(unique_keys['project'], unique_keys['storypoints'], values = unique_keys['key'], aggfunc='count')\n",
    "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
    "g = sns.heatmap(proj_story_tab, annot=True, cmap='Greens', fmt='g')\n",
    "g.set_yticklabels(g.get_yticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#field_in_issues, field_exclusion_filter, values_filter\n",
    "\n",
    "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
    "story_fields = ['Story Points', 'Actual Story Points']\n",
    "field = 'StoryPoints'\n",
    "e_low = ['0.5', '1', '2']\n",
    "e_medium = ['3', '5', '8', '13']\n",
    "e_high = ['20', '40', '100']\n",
    "\n",
    "\n",
    "estimates_df = categorical_metric(field,story_fields,'fields.issuetype.name','Epic',story_points,e_high,'high',e_medium,'medium',e_low,'low')\n",
    "estimates_df_agg = categorical_metric_agg(estimates_df, '`'+field+'`','high', 'medium', 'low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimates_df.shape[0],estimates_df[['user', 'key']].drop_duplicates().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimates_df_agg.shape[0], \n",
    "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
    "estimates_df_agg.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimates_df_agg[(pd.isnull(estimates_df_agg['high'])==False)\n",
    "                         & (pd.isnull(estimates_df_agg['medium'])==False)\n",
    "                         & (pd.isnull(estimates_df_agg['low'])==False)].shape[0])\n",
    "\n",
    "print(estimates_df_agg[(pd.isnull(estimates_df_agg['high'])==False)\n",
    "                         | (pd.isnull(estimates_df_agg['medium'])==False)\n",
    "                         | (pd.isnull(estimates_df_agg['low'])==False)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Association Rules</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Prepare datasets for association rule mining</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
    "    items = items_df[[var1,var2]].values\n",
    "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
    "    rules_list = list(rules)\n",
    "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
    "    for relation_recordset in rules_list:\n",
    "        for rel_recordset_element in relation_recordset[2]:\n",
    "            element_nr = 0\n",
    "            for order_statistics in list(rel_recordset_element):\n",
    "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
    "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
    "                elif element_nr == 2:confidence = order_statistics\n",
    "                elif element_nr == 3:lift = order_statistics\n",
    "                element_nr = element_nr + 1 \n",
    "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
    "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
    "                               ,  ignore_index = True)    \n",
    "    if filter_twosets==True:\n",
    "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
    "    return assoc_df.sort_values('antescedent', ascending = False)\n",
    "\n",
    "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
    "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
    "    for col in items_df.columns:\n",
    "        if (col!='user') & (col!=metric):\n",
    "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
    "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
    "            df = ps.sqldf(sql_query, locals())\n",
    "    return df\n",
    "\n",
    "def heatmap_rule_stats(df, min_support_=0, min_confidence_=0):\n",
    "    df = df[(df['support']>=min_support_) & (df['confidence']>=min_confidence_)]\n",
    "    if df.shape[0]>0:\n",
    "        tab_df = df\n",
    "        tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
    "        tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
    "        tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
    "        colors = ['Blues', 'BuPu', 'YlGnBu']\n",
    "        i=1\n",
    "        fig = plt.figure(figsize=(18, 14))\n",
    "        for var in (['support', 'confidence', 'lift']):\n",
    "            fig.add_subplot(2,2,i)\n",
    "            if var=='support':src_tab = tab_sup \n",
    "            elif var=='confidence':src_tab = tab_conf\n",
    "            elif var=='lift':src_tab = tab_lif\n",
    "            g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
    "            plt.title(var, fontsize=15)\n",
    "            plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
    "            i=i+1\n",
    "    else:\n",
    "        print('No associations match min support and confidence criteria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_personalities.head()\n",
    "user_personalities =user_personalities.drop(columns=['Is_Open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              user\n",
      "Is_open           \n",
      "No_openness     50\n",
      "Yes_openness    50 \n",
      "                        user\n",
      "Is_conscientious           \n",
      "No_conscientiousness     48\n",
      "Yes_conscientiousness    52 \n",
      "                   user\n",
      "Is_extravert          \n",
      "No_extraversion     47\n",
      "Yes_extraversion    53 \n",
      "                    user\n",
      "Is_agreeable           \n",
      "No_agreeableness     50\n",
      "Yes_agreeableness    50 \n",
      "                  user\n",
      "Is_neurotic          \n",
      "No_neuroticism     54\n",
      "Yes_neuroticism    46\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "user_personalities['Is_open']           =user_personalities['o_raw'].apply(          lambda x: 'Yes_openness' if x>np.mean(user_personalities['o_raw'])          else 'No_openness')\n",
    "user_personalities['Is_conscientious'] =user_personalities['c_raw'].apply( lambda x: 'Yes_conscientiousness' if x>np.mean(user_personalities['c_raw']) else 'No_conscientiousness')\n",
    "user_personalities['Is_extravert']      =user_personalities['e_raw'].apply(      lambda x: 'Yes_extraversion' if x>np.mean(user_personalities['e_raw'])      else 'No_extraversion')\n",
    "user_personalities['Is_agreeable']     =user_personalities['a_raw'].apply(     lambda x: 'Yes_agreeableness' if x>np.mean(user_personalities['a_raw'])     else 'No_agreeableness')\n",
    "user_personalities['Is_neurotic']       =user_personalities['n_raw'].apply(       lambda x: 'Yes_neuroticism' if x>np.mean(user_personalities['n_raw'])       else 'No_neuroticism')\n",
    "\n",
    "\n",
    "print(user_personalities[['Is_open', 'user']].groupby('Is_open').count(),'\\n',\n",
    "user_personalities[['Is_conscientious', 'user']].groupby('Is_conscientious').count(),'\\n',\n",
    "user_personalities[['Is_extravert', 'user']].groupby('Is_extravert').count(),'\\n',\n",
    "user_personalities[['Is_agreeable', 'user']].groupby('Is_agreeable').count(),'\\n',\n",
    "user_personalities[['Is_neurotic', 'user']].groupby('Is_neurotic').count())\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT U.user FROM user_personalities AS U\n",
    "INNER JOIN (SELECT DISTINCT user FROM estimates_df) AS E on U.user = E.user\n",
    "INNER JOIN (SELECT DISTINCT user FROM priorities_df) AS P on U.user = P.user\n",
    "INNER JOIN (SELECT DISTINCT user FROM states_df) AS S on U.user = S.user\n",
    "INNER JOIN (SELECT DISTINCT user FROM valid_users_times) AS T on U.user = T.user\n",
    "\"\"\"\n",
    "all_mterics_available_users = ps.sqldf(query, locals())\n",
    "print(all_mterics_available_users.shape[0])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimates_df--- total nr of records:  6813 ; unique users:  74\n",
      "priorities_df--- total nr of records:  5126 ; unique users:  81\n",
      "states_df--- total nr of records:  32029 ; unique users:  97\n",
      "valid_users_times--- total nr of records:  8094 ; unique users:  80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x0000022490211DD8>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('estimates_df---', 'total nr of records: ', estimates_df.shape[0], '; unique users: ',estimates_df.user.drop_duplicates().shape[0])\n",
    "print('priorities_df---', 'total nr of records: ',priorities_df.shape[0], '; unique users: ',priorities_df.user.drop_duplicates().shape[0])\n",
    "print('states_df---', 'total nr of records: ',states_df.shape[0], '; unique users: ',states_df.user.drop_duplicates().shape[0])\n",
    "print('valid_users_times---', 'total nr of records: ',valid_users_times.shape[0], '; unique users: ',valid_users_times.user.drop_duplicates().shape[0])\n",
    "\n",
    "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
    "    'user','StoryPoints','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
    "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
    "    'user','priority','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
    "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
    "    'user','status','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
    "times_assoc_df = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
    "    'user','time_spending_category','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 516 0\n"
     ]
    }
   ],
   "source": [
    "print(estimates_assoc_df[pd.isnull(estimates_assoc_df['StoryPoints'])==True].shape[0],\n",
    "      priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==True].shape[0],\n",
    "      states_assoc_df[pd.isnull(states_assoc_df['status'])==True].shape[0],\n",
    "      times_assoc_df[pd.isnull(times_assoc_df['time_spending_category'])==True].shape[0])\n",
    "\n",
    "priorities_assoc_df = priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
    "states_assoc_df = states_assoc_df[pd.isnull(states_assoc_df['status'])==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_assoc_openness = estimates_assoc_df[[\n",
    "    'StoryPoints','Is_open']].values\n",
    "estimates_assoc_conscientiousness = estimates_assoc_df[[\n",
    "    'StoryPoints','Is_conscientious']].values\n",
    "estimates_assoc_extraversion = estimates_assoc_df[[\n",
    "    'StoryPoints', 'Is_extravert']].values\n",
    "estimates_assoc_agreeableness = estimates_assoc_df[[\n",
    "    'StoryPoints','Is_agreeable']].values\n",
    "estimates_assoc_neuroticism = estimates_assoc_df[[\n",
    "    'StoryPoints','Is_neurotic']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "traits = ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
    "for trait in traits:\n",
    "    query=\"\"\"\n",
    "    SELECT time_spending_category AS metric, \"\"\"+trait+\"\"\" as trait  FROM times_assoc_df \n",
    "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
    "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
    "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df\"\"\"\n",
    "    if trait=='Is_open':o_all = ps.sqldf(query, locals())\n",
    "    elif trait=='Is_conscientious':c_all = ps.sqldf(query, locals())\n",
    "    elif trait=='Is_extravert':e_all = ps.sqldf(query, locals())\n",
    "    elif trait=='Is_agreeable':a_all = ps.sqldf(query, locals())\n",
    "    elif trait=='Is_neurotic':n_all = ps.sqldf(query, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_all_rules = get_association_rules_df(o_all,'metric','trait', 0.0001, 0.0001, True)\n",
    "c_all_rules = get_association_rules_df(c_all,'metric','trait', 0.0001, 0.0001, True)\n",
    "e_all_rules = get_association_rules_df(e_all,'metric','trait', 0.0001, 0.0001, True)\n",
    "a_all_rules = get_association_rules_df(a_all,'metric','trait', 0.0001, 0.0001, True)\n",
    "n_all_rules = get_association_rules_df(n_all,'metric','trait', 0.0001, 0.0001, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1008 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap_rule_stats(c_all_rules, min_support_=0.00001,min_confidence_=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>minutes_spent</th>\n",
       "      <th>time_spending_category</th>\n",
       "      <th>Is_open</th>\n",
       "      <th>Is_conscientious</th>\n",
       "      <th>Is_extravert</th>\n",
       "      <th>Is_agreeable</th>\n",
       "      <th>Is_neurotic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grussell</td>\n",
       "      <td>0.578</td>\n",
       "      <td>low</td>\n",
       "      <td>Yes_openness</td>\n",
       "      <td>No_conscientiousness</td>\n",
       "      <td>Yes_extraversion</td>\n",
       "      <td>No_agreeableness</td>\n",
       "      <td>No_neuroticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grussell</td>\n",
       "      <td>1452.161</td>\n",
       "      <td>medium</td>\n",
       "      <td>Yes_openness</td>\n",
       "      <td>No_conscientiousness</td>\n",
       "      <td>Yes_extraversion</td>\n",
       "      <td>No_agreeableness</td>\n",
       "      <td>No_neuroticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grussell</td>\n",
       "      <td>73.852</td>\n",
       "      <td>medium</td>\n",
       "      <td>Yes_openness</td>\n",
       "      <td>No_conscientiousness</td>\n",
       "      <td>Yes_extraversion</td>\n",
       "      <td>No_agreeableness</td>\n",
       "      <td>No_neuroticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grussell</td>\n",
       "      <td>1688.373</td>\n",
       "      <td>high</td>\n",
       "      <td>Yes_openness</td>\n",
       "      <td>No_conscientiousness</td>\n",
       "      <td>Yes_extraversion</td>\n",
       "      <td>No_agreeableness</td>\n",
       "      <td>No_neuroticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grussell</td>\n",
       "      <td>20246.750</td>\n",
       "      <td>high</td>\n",
       "      <td>Yes_openness</td>\n",
       "      <td>No_conscientiousness</td>\n",
       "      <td>Yes_extraversion</td>\n",
       "      <td>No_agreeableness</td>\n",
       "      <td>No_neuroticism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user  minutes_spent time_spending_category       Is_open  \\\n",
       "0  grussell          0.578                    low  Yes_openness   \n",
       "1  grussell       1452.161                 medium  Yes_openness   \n",
       "2  grussell         73.852                 medium  Yes_openness   \n",
       "3  grussell       1688.373                   high  Yes_openness   \n",
       "4  grussell      20246.750                   high  Yes_openness   \n",
       "\n",
       "       Is_conscientious      Is_extravert      Is_agreeable     Is_neurotic  \n",
       "0  No_conscientiousness  Yes_extraversion  No_agreeableness  No_neuroticism  \n",
       "1  No_conscientiousness  Yes_extraversion  No_agreeableness  No_neuroticism  \n",
       "2  No_conscientiousness  Yes_extraversion  No_agreeableness  No_neuroticism  \n",
       "3  No_conscientiousness  Yes_extraversion  No_agreeableness  No_neuroticism  \n",
       "4  No_conscientiousness  Yes_extraversion  No_agreeableness  No_neuroticism  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_personalities = pd.merge(user_personalities\n",
    "    , valid_users_times[(valid_users_times['minutes_spent']>0)&(valid_users_times['minutes_spent']<100000)]\n",
    "    , how='inner', left_on = 'user', right_on = 'user')[[\n",
    "    'user','minutes_spent', 'time_spending_category','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
    "times_personalities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxpl2(dt, x_cols, y_cols, title):\n",
    "    n = 1\n",
    "    x_cnt = len(x_cols)\n",
    "    y_cnt = len(y_cols)\n",
    "    figure = plt.figure(figsize=(18, 15))\n",
    "    for x_ax in x_cols:\n",
    "        for i in y_cols:\n",
    "            ax = figure.add_subplot(1, 5, n)\n",
    "            #ax.set_title(i)\n",
    "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
    "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
    "            plt.suptitle(title, size=16)\n",
    "            plt.subplots_adjust(bottom=0.15, wspace=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:98: MatplotlibDeprecationWarning: \n",
      "Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  \"Adding an axes using the same arguments as a previous axes \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:98: MatplotlibDeprecationWarning: \n",
      "Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  \"Adding an axes using the same arguments as a previous axes \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:98: MatplotlibDeprecationWarning: \n",
      "Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  \"Adding an axes using the same arguments as a previous axes \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:98: MatplotlibDeprecationWarning: \n",
      "Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  \"Adding an axes using the same arguments as a previous axes \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxpl2(times_personalities\n",
    "      , ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
    "      , ['minutes_spent']\n",
    "      , 'Time spending for each personality trait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">minutes_spent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_open</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No_openness</th>\n",
       "      <td>3260.817</td>\n",
       "      <td>308.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes_openness</th>\n",
       "      <td>4242.821</td>\n",
       "      <td>484.873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             minutes_spent        \n",
       "                      mean  median\n",
       "Is_open                           \n",
       "No_openness       3260.817 308.333\n",
       "Yes_openness      4242.821 484.873"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mannwhitneyu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-cf92a2dbe231>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrait_yes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimes_personalities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimes_personalities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrait\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Yes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'minutes_spent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrait_no\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimes_personalities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimes_personalities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrait\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'No'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'minutes_spent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mstatistic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmannwhitneyu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrait_yes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrait_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mp_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'different'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mp_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'equal'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mannwhitneyu' is not defined"
     ]
    }
   ],
   "source": [
    "p = 0.05\n",
    "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
    "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
    "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
    "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
    "    if p>p_value:result ='different'\n",
    "    elif p<=p_value:result ='equal'\n",
    "    print('personality trait: ',trait, 'datasets are:',result\n",
    "          , '. \\nU-test results- statistic: ', statistic\n",
    "          , ';p-value: ',format(p_value, '.28f'), ';\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <font color='red'> !!!</font> </h1>\n",
    "<h2> All Code below is still under the development and experimental. Should not be taken into account yet <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "import seaborn as sns\n",
      " 1/2:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      " 1/3: issues.head()\n",
      " 1/4:\n",
      "print(changelog[changelog['field'] == 'status'].toString.unique())\n",
      "issues[issues['fields.status.statusCategory.name'] == 'Done']['fields.status.name'].unique()\n",
      "# Clodes, Resolved, Done == Done.\n",
      "issues[issues['fields.status.statusCategory.name'] == 'Done'].groupby('fields.assignee.name').size()\n",
      "pd.value_counts(issues[issues['fields.status.statusCategory.name'] == 'Done']['fields.assignee.name']).plot(kind='bar')\n",
      "pd.DataFrame(issues.key.unique()).shape()\n",
      "tab = pd.crosstab(issues['fields.status.statusCategory.name'], issues['project'])\n",
      "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
      "sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "#['fields.assignee.name']\n",
      " 1/5:\n",
      "print(issues.project.unique())\n",
      "issues['project']\n",
      " 1/6: changelog.head()\n",
      " 1/7: changelog[changelog['from'].str.isnumeric() == False]['from'].unique()\n",
      " 1/8:\n",
      "##########------ Take the two examplary from/to texts for each unique field ------------######### \n",
      "\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_ = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_)\n",
      "    to_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row, 'field':field_row, 'from':from_row, 'to':to_row})\n",
      " 1/9:\n",
      "##########----------- save the fields and their texts as a file. check them manually ---------------#########\n",
      "## add text markers: 1- field is not textual, 2 - field is textual, 3 - possible to be textual, needs further recheck.\n",
      "## load markered file back\n",
      "\n",
      "df.to_csv(\"fields.csv\")\n",
      "fields_ranked = pd.read_csv('fields_ranked.csv')\n",
      "1/10:\n",
      "##########----------- save the fields and their texts as a file. check them manually ---------------#########\n",
      "## add text markers: 1- field is not textual, 2 - field is textual, 3 - possible to be textual, needs further recheck.\n",
      "## load markered file back\n",
      "\n",
      "df.to_csv(\"fields.csv\")\n",
      "fields_ranked = pd.read_csv('fields_ranked.csv')\n",
      "1/11:\n",
      "#####-------- filter loaded fields and take only: 2 - field is textual, 3 - possible to be textual ------------#######\n",
      "#####-------- merge markers dataset to the changelog ------------#######\n",
      "\n",
      "texts = fields_ranked[fields_ranked['text'].isin({2, 3})]\n",
      "changelog_texts = pd.merge(changelog, texts, how = 'inner', left_on = 'field', right_on = 'field')\n",
      "1/12:\n",
      "#### print all the field names of 3 - possible to be textual\n",
      "changelog_texts[changelog_texts.text == 3]['field'].unique()\n",
      "1/13:\n",
      "########---- Manual check of the fields that could possibly have textual values ------------###########\n",
      "\n",
      "changelog_texts[changelog_texts['field'] == 'labels'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Component'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'environment'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Workflow'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Requires More Info'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'security'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Release Notes'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Configuration'] # Y, needs a complex parsing!\n",
      "changelog_texts[changelog_texts['field'] == 'Affects Docs'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Release Note'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Repro Case'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'ROM'] # N\n",
      "1/14:\n",
      "cols = ['key', 'project', 'author', 'field', 'fromString', 'toString']\n",
      "data_texts = changelog_texts[changelog_texts['text'] == 2][cols]\n",
      "1/15:\n",
      "print(data_texts['field'].unique())\n",
      "data_texts.groupby('field').size()\n",
      "1/16:\n",
      "data_texts = data_texts[data_texts['toString'].str.len() >= 20 ]\n",
      "\n",
      "\n",
      "data_texts[data_texts['field'] == 'Acceptance Criteria'].to_csv('Acceptance Criteria.csv')\n",
      "1/17: data_texts.iloc[6502]\n",
      "1/18:\n",
      "\n",
      "data_texts['fromString'] = data_texts['fromString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "data_texts['toString'] = data_texts['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "1/19:\n",
      "#data_texts = data_texts.drop(6502)\n",
      "data_texts.to_csv('texts.csv')\n",
      "1/20: print(issues.project.unique())\n",
      "1/21:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "1/22: changelog[changelog['from'].str.isnumeric() == False]['from'].unique()\n",
      "1/23:\n",
      "##########------ Take the two examplary from/to texts for each unique field ------------######### \n",
      "\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_ = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_)\n",
      "    to_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row, 'field':field_row, 'from':from_row, 'to':to_row})\n",
      "1/24:\n",
      "##########----------- save the fields and their texts as a file. check them manually ---------------#########\n",
      "## add text markers: 1- field is not textual, 2 - field is textual, 3 - possible to be textual, needs further recheck.\n",
      "## load markered file back\n",
      "\n",
      "df.to_csv(\"fields.csv\")\n",
      "fields_ranked = pd.read_csv('fields_ranked.csv')\n",
      "1/25:\n",
      "#####-------- filter loaded fields and take only: 2 - field is textual, 3 - possible to be textual ------------#######\n",
      "#####-------- merge markers dataset to the changelog ------------#######\n",
      "\n",
      "texts = fields_ranked[fields_ranked['text'].isin({2, 3})]\n",
      "changelog_texts = pd.merge(changelog, texts, how = 'inner', left_on = 'field', right_on = 'field')\n",
      "1/26:\n",
      "cols = ['key', 'project', 'author', 'field', 'fromString', 'toString']\n",
      "data_texts = changelog_texts[changelog_texts['text'] == 2][cols]\n",
      "1/27:\n",
      "print(data_texts['field'].unique())\n",
      "data_texts.groupby('field').size()\n",
      "1/28:\n",
      "# -- take the needed columns from the dataset, and filter the dataset to take only the rows that have textual content\n",
      "cols = ['key', 'project', 'author', 'field', 'fromString', 'toString']\n",
      "data_texts = changelog_texts[changelog_texts['text'] == 2][cols]\n",
      "1/29:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "print(data_texts['field'].unique())\n",
      "data_texts.groupby('field').size()\n",
      "1/30: data_texts.iloc[6456]['fromString']\n",
      "1/31: data_texts.iloc[6456]['fromString']\n",
      "1/32:\n",
      "##### ---- save the file as csv and visually check the work of bad characters removal\n",
      "#data_texts = data_texts.drop(6502)\n",
      "data_texts.to_csv('texts.csv')\n",
      "1/33: from jira import JIRA\n",
      "1/34: from jira import JIRA\n",
      "1/35: from jira import JIRA\n",
      "1/36: changelog.head(6)\n",
      "1/37: changelog.group_by('key').count\n",
      "1/38: changelog.groupby('key').count\n",
      "1/39: changelog.groupby('key').count()\n",
      "1/40:\n",
      "changelog.groupby('key').count()\n",
      "\n",
      "changelog[changelog['key'] == 'APSTUD-1698']\n",
      "1/41: changelog.groupby(['key', 'field']).count()\n",
      "1/42: changelog.groupby(['key', 'field']).count()\n",
      "1/43: changelog.groupby(['key', 'field']).count().head(4)\n",
      "1/44: changelog.groupby(['key', 'field']).count().head(10)\n",
      "1/45:\n",
      "from jira import JIRA\n",
      "import re\n",
      "1/46:\n",
      "for ix, line in data_texts.iterrows():\n",
      "    m = re.search('{code}(.*){code}', line['toString'], flags=re.DOTALL)\n",
      "     if m:\n",
      "            data_texts.loc[ix, 'toString'] = line['data_texts'][:m.start(0)] + line['data_texts']\n",
      "         # issues.loc[ix, 'code'] = line['description'][m.start(0):m.end(0)]\n",
      "1/47:\n",
      "for ix, line in data_texts.iterrows():\n",
      "    m = re.search('{code}(.*){code}', line['toString'], flags=re.DOTALL)\n",
      "    if m:\n",
      "        data_texts.loc[ix, 'toString'] = line['data_texts'][:m.start(0)] + line['data_texts']\n",
      "         # issues.loc[ix, 'code'] = line['description'][m.start(0):m.end(0)]\n",
      "1/48:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "print(data_texts['field'].unique())\n",
      "data_texts.groupby('field').size()\n",
      "1/49: data_texts\n",
      "1/50: data_texts[data_texts['key'] == 'XD-3750'].head(6)\n",
      "1/51: data_texts[data_texts['field'] == 'comment'].head(6)\n",
      "1/52: data_texts[data_texts['field'] == 'comments'].head(6)\n",
      "1/53: data_texts[data_texts['field'] == 'Comments'].head(6)\n",
      "1/54: data_texts[data_texts['field'] == 'Comment'].head(6)\n",
      "1/55: data_texts[data_texts['field'] == 'Comment'].head(20)\n",
      "1/56: data_texts[data_texts['key'] == 'XD-2401'].head(20)\n",
      "1/57:\n",
      "data_texts[data_texts['key'] == 'XD-2401'].head(20)\n",
      "\n",
      "\n",
      "data_texts[data_texts['field'] == 'comment'].head(20)\n",
      "1/58:\n",
      "data_texts[data_texts['key'] == 'XD-2401'].head(20)\n",
      "\n",
      "\n",
      "data_texts[data_texts['field'] == 'Comment'].head(20)\n",
      "1/59:\n",
      "data_texts[data_texts['key'] == 'XD-2401'].head(20)\n",
      "\n",
      "\n",
      "data_texts[data_texts['field'] == 'Comment'].head(100)\n",
      "1/60: data_texts\n",
      "1/61: data_texts[data_texts['field'] == 'description']\n",
      "1/62:\n",
      "descriptions = data_texts[data_texts['field'] == 'description']\n",
      "acceptanceCriterias = data_texts[data_texts['field'] == 'Acceptance Criteria']\n",
      "comments = data_texts[data_texts['field'] == 'Comment']\n",
      "epics = data_texts[data_texts['field'].isin(['Epic Name', 'Epic/Theme'])\n",
      "summaries = data_texts[data_texts['field'] == 'summary']\n",
      "1/63:\n",
      "descriptions = data_texts[data_texts['field'] == 'description']\n",
      "acceptanceCriterias = data_texts[data_texts['field'] == 'Acceptance Criteria']\n",
      "comments = data_texts[data_texts['field'] == 'Comment']\n",
      "epics = data_texts[data_texts['field'].isin(['Epic Name', 'Epic/Theme'])]\n",
      "summaries = data_texts[data_texts['field'] == 'summary']\n",
      "1/64: #price=re.sub(\"<[b][^>]*>(.+?)</[b]>\", '', price)\n",
      "1/65: comments\n",
      "1/66: comments.frmoString.unique()\n",
      "1/67: comments.fromString.unique()\n",
      "1/68:\n",
      "descriptions = data_texts[data_texts['field'] == 'description']\n",
      "acceptanceCriterias = data_texts[data_texts['field'] == 'Acceptance Criteria']\n",
      "comments = changelog_texts[changelog_texts['field'] == 'Comment']\n",
      "epics = data_texts[data_texts['field'].isin(['Epic Name', 'Epic/Theme'])]\n",
      "summaries = data_texts[data_texts['field'] == 'summary']\n",
      "1/69: comments.fromString.unique()\n",
      "1/70: comments.fromString.unique()\n",
      "1/71: comments.from.unique()\n",
      "1/72: comments['from'].unique()\n",
      "1/73: comments\n",
      "1/74: comments.from_x.unique()\n",
      "1/75: descriptions\n",
      "1/76:\n",
      "descriptions = changelog_texts[changelog_texts['field'] == 'description']\n",
      "acceptanceCriterias = data_texts[data_texts['field'] == 'Acceptance Criteria']\n",
      "comments = changelog_texts[changelog_texts['field'] == 'Comment']\n",
      "epics = data_texts[data_texts['field'].isin(['Epic Name', 'Epic/Theme'])]\n",
      "summaries = data_texts[data_texts['field'] == 'summary']\n",
      "1/77: descriptions\n",
      " 3/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "import seaborn as sns\n",
      " 3/2:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      " 3/3: changelog.head()\n",
      " 3/4: print(issues.project.unique())\n",
      " 3/5: changelog.groupby(['key', 'field']).count().head(10)\n",
      " 3/6: changelog.head()\n",
      " 3/7: changelog[changelog['from'].str.isnumeric() == False]['from'].unique()\n",
      " 3/8:\n",
      "##########------ Take the two examplary from/to texts for each unique field ------------######### \n",
      "\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_ = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_)\n",
      "    to_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row, 'field':field_row, 'from':from_row, 'to':to_row})\n",
      " 3/9:\n",
      "##########----------- save the fields and their texts as a file. check them manually ---------------#########\n",
      "## add text markers: 1- field is not textual, 2 - field is textual, 3 - possible to be textual, needs further recheck.\n",
      "## load markered file back\n",
      "\n",
      "df.to_csv(\"fields.csv\")\n",
      "fields_ranked = pd.read_csv('fields_ranked.csv')\n",
      "3/10:\n",
      "# -- take the needed columns from the dataset, and filter the dataset to take only the rows that have textual content\n",
      "cols = ['key', 'project', 'author', 'field', 'fromString', 'toString']\n",
      "data_texts = changelog_texts[changelog_texts['text'] == 2][cols]\n",
      "3/11:\n",
      "# -- take the needed columns from the dataset, and filter the dataset to take only the rows that have textual content\n",
      "cols = ['key', 'project', 'author', 'field', 'fromString', 'toString']\n",
      "data_texts = changelog_texts[changelog_texts['text'] == 2][cols]\n",
      "3/12:\n",
      "#####-------- filter loaded fields and take only: 2 - field is textual, 3 - possible to be textual ------------#######\n",
      "#####-------- merge markers dataset to the changelog ------------#######\n",
      "\n",
      "texts = fields_ranked[fields_ranked['text'].isin({2, 3})]\n",
      "changelog_texts = pd.merge(changelog, texts, how = 'inner', left_on = 'field', right_on = 'field')\n",
      "3/13:\n",
      "##########----------- save the fields and their texts as a file. check them manually ---------------#########\n",
      "## add text markers: 1- field is not textual, 2 - field is textual, 3 - possible to be textual, needs further recheck.\n",
      "## load markered file back\n",
      "\n",
      "df.to_csv(\"fields.csv\")\n",
      "fields_ranked = pd.read_csv('fields_ranked.csv')\n",
      "3/14:\n",
      "#####-------- filter loaded fields and take only: 2 - field is textual, 3 - possible to be textual ------------#######\n",
      "#####-------- merge markers dataset to the changelog ------------#######\n",
      "\n",
      "texts = fields_ranked[fields_ranked['text'].isin({2, 3})]\n",
      "changelog_texts = pd.merge(changelog, texts, how = 'inner', left_on = 'field', right_on = 'field')\n",
      "3/15:\n",
      "#### print all the field names of 3 - possible to be textual\n",
      "changelog_texts[changelog_texts.text == 3]['field'].unique()\n",
      "3/16:\n",
      "########---- Manual check of the fields that could possibly have textual values ------------###########\n",
      "\n",
      "changelog_texts[changelog_texts['field'] == 'labels'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Component'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'environment'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Workflow'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Requires More Info'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'security'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Release Notes'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Configuration'] # Y, needs a complex parsing!\n",
      "changelog_texts[changelog_texts['field'] == 'Affects Docs'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Release Note'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'Repro Case'] # N\n",
      "changelog_texts[changelog_texts['field'] == 'ROM'] # N\n",
      "3/17:\n",
      "# -- take the needed columns from the dataset, and filter the dataset to take only the rows that have textual content\n",
      "cols = ['key', 'project', 'author', 'field', 'fromString', 'toString']\n",
      "data_texts = changelog_texts[changelog_texts['text'] == 2][cols]\n",
      "3/18:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "print(data_texts['field'].unique())\n",
      "data_texts.groupby('field').size()\n",
      "3/19:\n",
      "##### -------- Filter the dataset to take onlt the rows, that have textual content with more than 20 characters, \n",
      "##### ----- and save it as csv\n",
      "data_texts = data_texts[data_texts['toString'].str.len() >= 20 ]\n",
      "data_texts[data_texts['field'] == 'Acceptance Criteria'].to_csv('Acceptance Criteria.csv')\n",
      "3/20:\n",
      "##### --- Some column values contain hidden or bad characters, see one example\n",
      "data_texts.iloc[6502]\n",
      "3/21:\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "data_texts['fromString'] = data_texts['fromString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "data_texts['toString'] = data_texts['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "3/22:\n",
      "##### ---- save the file as csv and visually check the work of bad characters removal\n",
      "#data_texts = data_texts.drop(6502)\n",
      "data_texts.to_csv('texts.csv')\n",
      "3/23: data_texts.iloc[6456]['fromString']\n",
      "3/24:\n",
      "from jira import JIRA\n",
      "import re\n",
      "3/25:\n",
      "descriptions = changelog_texts[changelog_texts['field'] == 'description']\n",
      "acceptanceCriterias = data_texts[data_texts['field'] == 'Acceptance Criteria']\n",
      "comments = changelog_texts[changelog_texts['field'] == 'Comment']\n",
      "epics = data_texts[data_texts['field'].isin(['Epic Name', 'Epic/Theme'])]\n",
      "summaries = data_texts[data_texts['field'] == 'summary']\n",
      "3/26: data_texts.head(6)\n",
      "3/27: df.groupby(['project', 'key'])['author'].transform(count)\n",
      "3/28: df.groupby(['project', 'key'])['author'].transform(max)\n",
      "3/29: df.groupby(['project', 'key'])['author'].transform(max)\n",
      "3/30: df.groupby(c(['project', 'key']))['author'].transform(max)\n",
      "3/31: df.groupby((['project', 'key']))['author'].transform(max)\n",
      "3/32:\n",
      "# -- take the needed columns from the dataset, and filter the dataset to take only the rows that have textual content\n",
      "cols = ['key', 'project', 'author', 'field', 'fromString', 'toString', 'created']\n",
      "data_texts = changelog_texts[changelog_texts['text'] == 2][cols]\n",
      "3/33:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "print(data_texts['field'].unique())\n",
      "data_texts.groupby('field').size()\n",
      "3/34:\n",
      "##### -------- Filter the dataset to take onlt the rows, that have textual content with more than 20 characters, \n",
      "##### ----- and save it as csv\n",
      "data_texts = data_texts[data_texts['toString'].str.len() >= 20 ]\n",
      "data_texts[data_texts['field'] == 'Acceptance Criteria'].to_csv('Acceptance Criteria.csv')\n",
      "3/35:\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "data_texts['fromString'] = data_texts['fromString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "data_texts['toString'] = data_texts['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "3/36:\n",
      "descriptions = changelog_texts[changelog_texts['field'] == 'description']\n",
      "acceptanceCriterias = data_texts[data_texts['field'] == 'Acceptance Criteria']\n",
      "comments = changelog_texts[changelog_texts['field'] == 'Comment']\n",
      "epics = data_texts[data_texts['field'].isin(['Epic Name', 'Epic/Theme'])]\n",
      "summaries = data_texts[data_texts['field'] == 'summary']\n",
      "3/37: data_texts.groupby((['project', 'key'])).agg({'author':'mean'})\n",
      "3/38: data_texts.groupby((['project', 'key'])).agg({'author':'max'})\n",
      "3/39: data_texts.groupby((['project', 'key'])).agg({'created':'max'})\n",
      "3/40: data_texts.groupby((['project', 'key'])).agg({'created':'min'})\n",
      "3/41: data_texts.groupby((['project', 'key'])).agg({'created':'min'})\n",
      "3/42: data_texts.groupby((['project', 'key'])).agg({'created':'max'})\n",
      "3/43: data_texts.groupby((['project', 'key'])).agg({'created':'min'})\n",
      "3/44: data_texts.groupby((['project', 'key'])).agg({'created':'max'})\n",
      "3/45: data_texts.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "3/46: data_texts.groupby((['project', 'key', 'field'])).agg({'created':'min'})\n",
      " 5/1:\n",
      "def LongestWord(sen):\n",
      "    return '1'\n",
      "\n",
      "LongestWord('x')\n",
      " 5/2:\n",
      "\n",
      "\n",
      "def LongestWord(sen):\n",
      "    return split(sen)\n",
      "\n",
      "sen = 'shorttttttt longgggggggggg shortest'\n",
      "\n",
      "LongestWord(sen)\n",
      " 5/3:\n",
      "\n",
      "\n",
      "def LongestWord(sen):\n",
      "    return sen.split()\n",
      "\n",
      "sen = 'shorttttttt longgggggggggg shortest'\n",
      "\n",
      "LongestWord(sen)\n",
      " 5/4:\n",
      "\n",
      "\n",
      "def LongestWord(sen):\n",
      "    w_length = 0\n",
      "    w_longest = ''\n",
      "    for word in sen.split():\n",
      "        if len(word) > w_length:\n",
      "            w_longest = word\n",
      "            w_length = len(word)\n",
      "    return sen.split()\n",
      "\n",
      "sen = 'shorttttttt longgggggggggg shortest'\n",
      "\n",
      "LongestWord(sen)\n",
      " 5/5:\n",
      "\n",
      "\n",
      "def LongestWord(sen):\n",
      "    w_length = 0\n",
      "    w_longest = ''\n",
      "    for word in sen.split():\n",
      "        if len(word) > w_length:\n",
      "            w_longest = word\n",
      "            w_length = len(word)\n",
      "    return w_longest\n",
      "\n",
      "sen = 'shorttttttt longgggggggggg shortest'\n",
      "\n",
      "LongestWord(sen)\n",
      " 5/6:\n",
      "def LongestWord(sen):\n",
      "    w_length = 0\n",
      "    w_longest = ''\n",
      "    for word in sen.split():\n",
      "        if len(word) > w_length:\n",
      "            w_longest = word\n",
      "            w_length = len(word)\n",
      "    return w_longest\n",
      "\n",
      "sen = 'Hello guys my name is Bot. I'm a bot created by the amazing Junction team, and my job is to be your friend and bedazzle you!'\n",
      "\n",
      "LongestWord(sen)\n",
      " 5/7:\n",
      "def LongestWord(sen):\n",
      "    w_length = 0\n",
      "    w_longest = ''\n",
      "    for word in sen.split():\n",
      "        if len(word) > w_length:\n",
      "            w_longest = word\n",
      "            w_length = len(word)\n",
      "    return w_longest\n",
      "\n",
      "sen = \"Hello guys my name is Bot. I'm a bot created by the amazing Junction team, and my job is to be your friend and bedazzle you!\"\"\n",
      "\n",
      "LongestWord(sen)\n",
      " 5/8:\n",
      "def LongestWord(sen):\n",
      "    w_length = 0\n",
      "    w_longest = \"\"\n",
      "    for word in sen.split():\n",
      "        if len(word) > w_length:\n",
      "            w_longest = word\n",
      "            w_length = len(word)\n",
      "    return w_longest\n",
      "\n",
      "sen = \"Hello guys my name is Bot. I'm a bot created by the amazing Junction team, and my job is to be your friend and bedazzle you!\"\"\n",
      "\n",
      "LongestWord(sen)\n",
      " 5/9:\n",
      "def LongestWord(sen):\n",
      "    w_length = 0\n",
      "    w_longest = \"\"\n",
      "    for word in sen.split():\n",
      "        if len(word) > w_length:\n",
      "            w_longest = word\n",
      "            w_length = len(word)\n",
      "    return w_longest\n",
      "\n",
      "sen = \"Hello guys my name is Bot. I'm a bot created by the amazing Junction team, and my job is to be your friend and bedazzle you!\"\n",
      "\n",
      "LongestWord(sen)\n",
      "5/10:\n",
      "ef divisors(n):\n",
      "    # get factors and their counts\n",
      "    factors = {}\n",
      "    nn = n\n",
      "    i = 2\n",
      "    while i*i <= nn:\n",
      "        while nn % i == 0:\n",
      "            factors[i] = factors.get(i, 0) + 1\n",
      "            nn //= i\n",
      "        i += 1\n",
      "    if nn > 1:\n",
      "        factors[nn] = factors.get(nn, 0) + 1\n",
      "\n",
      "    primes = list(factors.keys())\n",
      "\n",
      "    # generates factors from primes[k:] subset\n",
      "    def generate(k):\n",
      "        if k == len(primes):\n",
      "            yield 1\n",
      "        else:\n",
      "            rest = generate(k+1)\n",
      "            prime = primes[k]\n",
      "            for factor in rest:\n",
      "                prime_to_i = 1\n",
      "                # prime_to_i iterates prime**i values, i being all possible exponents\n",
      "                for _ in range(factors[prime] + 1):\n",
      "                    yield factor * prime_to_i\n",
      "                    prime_to_i *= prime\n",
      "\n",
      "    # in python3, `yield from generate(0)` would also work\n",
      "    for factor in generate(0):\n",
      "        yield factor\n",
      "5/11:\n",
      "def divisors(n):\n",
      "    # get factors and their counts\n",
      "    factors = {}\n",
      "    nn = n\n",
      "    i = 2\n",
      "    while i*i <= nn:\n",
      "        while nn % i == 0:\n",
      "            factors[i] = factors.get(i, 0) + 1\n",
      "            nn //= i\n",
      "        i += 1\n",
      "    if nn > 1:\n",
      "        factors[nn] = factors.get(nn, 0) + 1\n",
      "\n",
      "    primes = list(factors.keys())\n",
      "\n",
      "    # generates factors from primes[k:] subset\n",
      "    def generate(k):\n",
      "        if k == len(primes):\n",
      "            yield 1\n",
      "        else:\n",
      "            rest = generate(k+1)\n",
      "            prime = primes[k]\n",
      "            for factor in rest:\n",
      "                prime_to_i = 1\n",
      "                # prime_to_i iterates prime**i values, i being all possible exponents\n",
      "                for _ in range(factors[prime] + 1):\n",
      "                    yield factor * prime_to_i\n",
      "                    prime_to_i *= prime\n",
      "\n",
      "    # in python3, `yield from generate(0)` would also work\n",
      "    for factor in generate(0):\n",
      "        yield factor\n",
      "5/12:\n",
      "def divisors(n):\n",
      "    # get factors and their counts\n",
      "    factors = {}\n",
      "    nn = n\n",
      "    i = 2\n",
      "    while i*i <= nn:\n",
      "        while nn % i == 0:\n",
      "            factors[i] = factors.get(i, 0) + 1\n",
      "            nn //= i\n",
      "        i += 1\n",
      "    if nn > 1:\n",
      "        factors[nn] = factors.get(nn, 0) + 1\n",
      "\n",
      "    primes = list(factors.keys())\n",
      "\n",
      "    # generates factors from primes[k:] subset\n",
      "    def generate(k):\n",
      "        if k == len(primes):\n",
      "            yield 1\n",
      "        else:\n",
      "            rest = generate(k+1)\n",
      "            prime = primes[k]\n",
      "            for factor in rest:\n",
      "                prime_to_i = 1\n",
      "                # prime_to_i iterates prime**i values, i being all possible exponents\n",
      "                for _ in range(factors[prime] + 1):\n",
      "                    yield factor * prime_to_i\n",
      "                    prime_to_i *= prime\n",
      "\n",
      "    # in python3, `yield from generate(0)` would also work\n",
      "    for factor in generate(0):\n",
      "        yield factor\n",
      "divisors(100)\n",
      "5/13:\n",
      "def divisors(n):\n",
      "    # get factors and their counts\n",
      "    factors = {}\n",
      "    nn = n\n",
      "    i = 2\n",
      "    while i*i <= nn:\n",
      "        while nn % i == 0:\n",
      "            factors[i] = factors.get(i, 0) + 1\n",
      "            nn //= i\n",
      "        i += 1\n",
      "    if nn > 1:\n",
      "        factors[nn] = factors.get(nn, 0) + 1\n",
      "\n",
      "    primes = list(factors.keys())\n",
      "\n",
      "    # generates factors from primes[k:] subset\n",
      "    def generate(k):\n",
      "        if k == len(primes):\n",
      "            yield 1\n",
      "        else:\n",
      "            rest = generate(k+1)\n",
      "            prime = primes[k]\n",
      "            for factor in rest:\n",
      "                prime_to_i = 1\n",
      "                # prime_to_i iterates prime**i values, i being all possible exponents\n",
      "                for _ in range(factors[prime] + 1):\n",
      "                    yield factor * prime_to_i\n",
      "                    prime_to_i *= prime\n",
      "\n",
      "    # in python3, `yield from generate(0)` would also work\n",
      "    for factor in generate(0):\n",
      "        yield factor\n",
      "print(divisors(100))\n",
      "5/14:\n",
      "def divisors(n):\n",
      "    # get factors and their counts\n",
      "    factors = {}\n",
      "    nn = n\n",
      "    i = 2\n",
      "    while i*i <= nn:\n",
      "        while nn % i == 0:\n",
      "            factors[i] = factors.get(i, 0) + 1\n",
      "            nn //= i\n",
      "        i += 1\n",
      "    if nn > 1:\n",
      "        factors[nn] = factors.get(nn, 0) + 1\n",
      "\n",
      "    primes = list(factors.keys())\n",
      "\n",
      "    # generates factors from primes[k:] subset\n",
      "    def generate(k):\n",
      "        if k == len(primes):\n",
      "            yield 1\n",
      "        else:\n",
      "            rest = generate(k+1)\n",
      "            prime = primes[k]\n",
      "            for factor in rest:\n",
      "                prime_to_i = 1\n",
      "                # prime_to_i iterates prime**i values, i being all possible exponents\n",
      "                for _ in range(factors[prime] + 1):\n",
      "                    yield factor * prime_to_i\n",
      "                    prime_to_i *= prime\n",
      "\n",
      "    # in python3, `yield from generate(0)` would also work\n",
      "    for factor in generate(0):\n",
      "        yield factor\n",
      "divisors(1000)\n",
      "5/15:\n",
      "def divisors(n):\n",
      "    # get factors and their counts\n",
      "    factors = {}\n",
      "    nn = n\n",
      "    i = 2\n",
      "    while i*i <= nn:\n",
      "        while nn % i == 0:\n",
      "            factors[i] = factors.get(i, 0) + 1\n",
      "            nn //= i\n",
      "        i += 1\n",
      "    if nn > 1:\n",
      "        factors[nn] = factors.get(nn, 0) + 1\n",
      "\n",
      "    primes = list(factors.keys())\n",
      "\n",
      "    # generates factors from primes[k:] subset\n",
      "    def generate(k):\n",
      "        if k == len(primes):\n",
      "            yield 1\n",
      "        else:\n",
      "            rest = generate(k+1)\n",
      "            prime = primes[k]\n",
      "            for factor in rest:\n",
      "                prime_to_i = 1\n",
      "                # prime_to_i iterates prime**i values, i being all possible exponents\n",
      "                for _ in range(factors[prime] + 1):\n",
      "                    yield factor * prime_to_i\n",
      "                    prime_to_i *= prime\n",
      "\n",
      "    # in python3, `yield from generate(0)` would also work\n",
      "    for factor in generate(0):\n",
      "        yield factor\n",
      "divisors(2)\n",
      "5/16:\n",
      "def divisors(n):\n",
      "    # get factors and their counts\n",
      "    factors = {}\n",
      "    nn = n\n",
      "    i = 2\n",
      "    while i*i <= nn:\n",
      "        while nn % i == 0:\n",
      "            factors[i] = factors.get(i, 0) + 1\n",
      "            nn //= i\n",
      "        i += 1\n",
      "    if nn > 1:\n",
      "        factors[nn] = factors.get(nn, 0) + 1\n",
      "\n",
      "    primes = list(factors.keys())\n",
      "\n",
      "    # generates factors from primes[k:] subset\n",
      "    def generate(k):\n",
      "        if k == len(primes):\n",
      "            yield 1\n",
      "        else:\n",
      "            rest = generate(k+1)\n",
      "            prime = primes[k]\n",
      "            for factor in rest:\n",
      "                prime_to_i = 1\n",
      "                # prime_to_i iterates prime**i values, i being all possible exponents\n",
      "                for _ in range(factors[prime] + 1):\n",
      "                    yield factor * prime_to_i\n",
      "                    prime_to_i *= prime\n",
      "\n",
      "    # in python3, `yield from generate(0)` would also work\n",
      "    for factor in generate(0):\n",
      "        yield factor\n",
      "k = int(divisors(2))\n",
      "k\n",
      "5/17:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    return divs\n",
      "\n",
      "divisors(5)\n",
      "5/18:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    return divs\n",
      "\n",
      "divisors(100)\n",
      "5/19:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    return divs\n",
      "\n",
      "divisors(38)\n",
      "5/20:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    return divs\n",
      "\n",
      "divisors(28)\n",
      "5/21:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    \n",
      "    nr = len(divs)\n",
      "    return nr\n",
      "\n",
      "divisors(28)\n",
      "5/22:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    \n",
      "    nr = len(divs)\n",
      "    if nr%2 == 0:\n",
      "        res = 'even'\n",
      "    else:\n",
      "        res = 'odd'\n",
      "    return res\n",
      "\n",
      "divisors(28)\n",
      "5/23:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    \n",
      "    nr = len(divs)\n",
      "    if nr%2 == 0:\n",
      "        res = 'even'\n",
      "    else:\n",
      "        res = 'odd'\n",
      "    return res\n",
      "\n",
      "divisors(28)\n",
      "5/24:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    \n",
      "    nr = len(divs)\n",
      "    if nr%2 == 0:\n",
      "        res = 'even'\n",
      "    else:\n",
      "        res = 'odd'\n",
      "    return res\n",
      "\n",
      "divisors(27)\n",
      "5/25:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    \n",
      "    nr = len(divs)\n",
      "    if nr%2 == 0:\n",
      "        res = 'even'\n",
      "    else:\n",
      "        res = 'odd'\n",
      "    return res\n",
      "\n",
      "divisors(5)\n",
      "5/26:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    \n",
      "    nr = len(divs)\n",
      "    if nr%2 == 0:\n",
      "        res = 'even'\n",
      "    else:\n",
      "        res = 'odd'\n",
      "    return res\n",
      "\n",
      "divisors(2)\n",
      "5/27:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    \n",
      "    nr = len(divs)\n",
      "    if nr%2 == 0:\n",
      "        res = 'even'\n",
      "    else:\n",
      "        res = 'odd'\n",
      "    return res\n",
      "\n",
      "divisors(6)\n",
      "5/28:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    \n",
      "    nr = len(divs)\n",
      "    if nr%2 == 0:\n",
      "        res = 'even'\n",
      "    else:\n",
      "        res = 'odd'\n",
      "    return res\n",
      "\n",
      "divisors(8)\n",
      "5/29:\n",
      "from math import sqrt\n",
      "\n",
      "def divisors(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    \n",
      "    nr = len(divs)\n",
      "    if nr%2 == 0:\n",
      "        res = 'even'\n",
      "    else:\n",
      "        res = 'odd'\n",
      "    return res\n",
      "\n",
      "divisors(9)\n",
      "5/30:\n",
      "from math import sqrt\n",
      "\n",
      "def amOfDivis(n):\n",
      "    divs = {1,n}\n",
      "    for i in range(2,int(sqrt(n))+1):\n",
      "        if n%i == 0:\n",
      "            divs.update((i,n//i))\n",
      "    \n",
      "    nr = len(divs)\n",
      "    if nr%2 == 0:\n",
      "        res = 'even'\n",
      "    else:\n",
      "        res = 'odd'\n",
      "    return res\n",
      "\n",
      "amOfDivis(9)\n",
      "3/47: data_texts.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "3/48:\n",
      "data_texts_groupped = data_texts.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "#reset_index(level= [0,1], inplace=True)\n",
      "data_texts_groupped\n",
      "3/49:\n",
      "data_texts_groupped = data_texts.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "data_texts_groupped.reset_index(level= [0,1,2], inplace=True)\n",
      "data_texts_groupped\n",
      "3/50:\n",
      "data_texts_groupped = data_texts.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "data_texts_groupped.reset_index(level= [0,1], inplace=True)\n",
      "data_texts_groupped\n",
      "3/51:\n",
      "data_texts_groupped = data_texts.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "data_texts_groupped.reset_index(level= [0,1, 2, 3], inplace=True)\n",
      "data_texts_groupped\n",
      "3/52:\n",
      "data_texts_groupped = data_texts.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "data_texts_groupped.reset_index(level= [0,1,2], inplace=True)\n",
      "data_texts_groupped\n",
      "3/53:\n",
      "data_texts_groupped = data_texts.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "data_texts_groupped.reset_index(level= [0,1,2], inplace=True)\n",
      "data_texts_groupped\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "latest_logs_from_issues = pd.merge(data_texts, data_texts_groupped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "latest_logs_from_issues.head()\n",
      "3/54:\n",
      "data_texts_groupped = data_texts.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "data_texts_groupped.reset_index(level= [0,1,2], inplace=True)\n",
      "data_texts_groupped\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "latest_logs_from_issues = pd.merge(data_texts, data_texts_groupped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(data_texts.size[0], data_texts_groupped.size[0], latest_logs_from_issues.size[0])\n",
      "3/55:\n",
      "data_texts_groupped = data_texts.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "data_texts_groupped.reset_index(level= [0,1,2], inplace=True)\n",
      "data_texts_groupped\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "latest_logs_from_issues = pd.merge(data_texts, data_texts_groupped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(data_texts.shape[0], data_texts_groupped.shape[0], latest_logs_from_issues.shape[0])\n",
      "3/56:\n",
      "data_texts_groupped = data_texts.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "data_texts_groupped.reset_index(level= [0,1,2], inplace=True)\n",
      "data_texts_groupped\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "latest_logs_from_issues = pd.merge(data_texts, data_texts_groupped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "latest_logs_from_issues.head(6)\n",
      "print(data_texts.shape[0], data_texts_groupped.shape[0], latest_logs_from_issues.shape[0])\n",
      "3/57:\n",
      "data_texts_groupped = data_texts.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "data_texts_groupped.reset_index(level= [0,1,2], inplace=True)\n",
      "data_texts_groupped\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "latest_logs_from_issues = pd.merge(data_texts, data_texts_groupped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(data_texts.shape[0], data_texts_groupped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "3/58:\n",
      "#### print all the field names of 3 - possible to be textual\n",
      "changelog['field'].unique()\n",
      "3/59:\n",
      "# -- take the needed columns from the dataset, and filter the dataset to take only the rows that have textual content\n",
      "cols = ['key', 'project', 'author', 'field', 'created']\n",
      "data_texts = changelog[cols]\n",
      "3/60:\n",
      "# -- take the needed columns from the dataset, and filter the dataset to take only the rows that have textual content\n",
      "cols = ['key', 'project', 'author', 'field', 'created']\n",
      "data_texts = changelog[cols]\n",
      "3/61:\n",
      "# Detect the user that changed the field.\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created']\n",
      "log_cols = changelog[cols]\n",
      "3/62:\n",
      "['summary' 'description' 'Acceptance Criteria' 'Comment' 'Epic Name'\n",
      " 'Out of Scope' 'QA Test Plan' 'Epic/Theme' 'Migration Impact'\n",
      " 'Business Value']\n",
      "3/63:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "log_cols.groupby('field').size()\n",
      "3/64:\n",
      "# Detect the user that changed the field.\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name', \n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value']) ]\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created']\n",
      "log_cols = log_filtered[cols]\n",
      "3/65:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "log_cols.groupby('field').size()\n",
      "3/66:\n",
      "# Detect the user that changed the field.\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name', \n",
      " , 'Epic/Theme'])]\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created']\n",
      "log_cols = log_filtered[cols]\n",
      "3/67:\n",
      "# Detect the user that changed the field.\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name', \n",
      " 'Epic/Theme'])]\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created']\n",
      "log_cols = log_filtered[cols]\n",
      "3/68:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "log_cols.groupby('field').size()\n",
      "3/69:\n",
      "# Detect the user that changed the field.\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic/Theme'])]\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created']\n",
      "log_cols = log_filtered[cols]\n",
      "3/70:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "log_cols.groupby('field').size()\n",
      "3/71:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic/Theme'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created']\n",
      "log_cols = log_filtered[cols]\n",
      "3/72:\n",
      "##### -------- Filter the dataset to take onlt the rows, that have textual content with more than 20 characters, \n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "3/73:\n",
      "##### --- Some column values contain hidden or bad characters, see one example\n",
      "log_cols.iloc[6502]\n",
      "3/74:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic/Theme'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "3/75:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "acceptanceCriterias = log_cols[log_cols['field'] == 'Acceptance Criteria']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "epics = log_cols[log_cols['field']=='Epic/Theme']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "3/76: descriptions\n",
      "3/77:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "    \n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(data_texts.log_cols[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "3/78:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "    \n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "3/79:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "   \n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "3/80:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "\n",
      "log_cols.groupby('field').size()\n",
      "3/81:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "3/82:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "\n",
      "log_cols.groupby('field').size()\n",
      "3/83:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "#log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "\n",
      "log_cols.groupby('field').size()\n",
      "3/84:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "changelog.field.unique()\n",
      "3/85:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "3/86:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "3/87:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "#log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "\n",
      "log_cols.groupby('field').size()\n",
      "3/88:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "3/89:\n",
      "# ---- Check the number of rows per selected textual field\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "\n",
      "log_cols.groupby('field').size()\n",
      "3/90:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "\n",
      "log_cols.groupby('field').size()\n",
      "3/91:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      " \n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "\n",
      "\n",
      "log_cols.groupby('field').size()\n",
      "3/92:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/93:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "3/94:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/95:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "3/96:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/97:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/98: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "3/99:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "3/100:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "3/101:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "3/102:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "3/103:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/104: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "3/105:\n",
      "# create datasets for each field type\n",
      "descriptions = latest_logs_from_issues[latest_logs_from_issues['field'] == 'description']\n",
      "summaries = latest_logs_from_issues[latest_logs_from_issues['field'] == 'summary']\n",
      "3/106: descriptions\n",
      "3/107: descriptions.head(3)\n",
      "3/108:\n",
      "#### TASKS:\n",
      "\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "#2. in descriptions: remove code snipets, session snippets and bad chars\n",
      "#3. check the other column datasets.\n",
      "3/109:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "3/110: price=re.sub(\"<(.+?)>\", '', log_cols['toString'])\n",
      "3/111: price=re.sub(\"<(.+?)>\", '', 'If I try to use <int:message-history/> w')\n",
      "3/112: re.sub(\"<(.+?)>\", '', 'If I try to use <int:message-history/> w')\n",
      "3/113: price=re.sub(\"<(.+?)>\", '', 'log_cols['toString']')\n",
      "3/114: price=re.sub(\"<(.+?)>\", '', log_cols['toString'])\n",
      "3/115:\n",
      "def removeCodeSnipped(text):\n",
      "    return re.sub(\"<(.+?)>\", '', text)\n",
      "3/116:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "3/117: descriptions.head(3)\n",
      "3/118:\n",
      "def removeCodeSnipped(text):\n",
      "    return re.sub(\"<(.+?)>\", '', text)\n",
      "\n",
      "descriptions['toString'] = descriptions['toString'].apply(removeCodeSnippet)\n",
      "3/119:\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', text)\n",
      "\n",
      "descriptions['toString'] = descriptions['toString'].apply(removeCodeSnippet)\n",
      "3/120: descriptions.head(3)\n",
      "3/121:\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{(.+?)}\", '', text))\n",
      "\n",
      "descriptions['toString'] = descriptions['toString'].apply(removeCodeSnippet)\n",
      "3/122: descriptions.head(3)\n",
      "3/123: descriptions.head(6)\n",
      "3/124:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "3/125: descriptions.head(6)\n",
      "3/126: descriptions.head(10)\n",
      "3/127:\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{(.+?)}\", '', text))\n",
      "descriptions['toString'] = descriptions['toString'].apply(lambda x: removeCodeSnippet(x))\n",
      "3/128: descriptions.head(10)\n",
      "3/129:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "3/130:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "3/131:\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', text))\n",
      "descriptions['toString'] = descriptions['toString'].apply(lambda x: removeCodeSnippet(x))\n",
      "3/132: descriptions.head(10)\n",
      "3/133:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', text))\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: removeCodeSnippet(x)) \n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/134:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"***(.+?)***\", '', text)))\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: removeCodeSnippet(x)) \n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/135:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"**(.+?)**\", '', text)))\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: removeCodeSnippet(x)) \n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/136:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "3/137:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"**(.+?)**\", '', text)))\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: removeCodeSnippet(x)) \n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/138:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "3/139:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"*(.+?)*\", '', text)))\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: removeCodeSnippet(x)) \n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/140:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "3/141:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "3/142:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"*(.+?)*\", '', text)))\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: removeCodeSnippet(x)) \n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/143:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', text))\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: removeCodeSnippet(x)) \n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/144:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "3/145:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', text))\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: removeCodeSnippet(x)) \n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/146:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "3/147:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "3/148:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', text))\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: removeCodeSnippet(x)) \n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/149:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', text))\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: removeCodeSnippet(x))\n",
      "3/150:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', text))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "3/151: log_cols\n",
      "3/152:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', text))\n",
      "descriptions['toString'] = descriptions['toString'].apply(removeCodeSnippet)\n",
      "3/153:\n",
      "log_cols\n",
      "descriptions\n",
      "3/154:\n",
      "\n",
      "descriptions\n",
      "3/155:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', text))\n",
      "summaries['toString'] = summaries['toString'].apply(removeCodeSnippet)\n",
      "3/156:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', text))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "3/157: log_cols['toString']\n",
      "3/158: log_cols['toString'].dtype\n",
      "3/159: description['toString'].dtype\n",
      "3/160: descriptions['toString'].dtype\n",
      "3/161: log_cols['toString'].dtype\n",
      "3/162: log_cols['toString'].dtype\n",
      "3/163:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', text))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "3/164:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', str(text)))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "3/165: log_cols['toString']\n",
      "3/166:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"***(.+?)***\", '', str(text))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "3/167:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"{{(.+?)}}\", '', str(text))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "3/168:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"***(.+?)***\", '', str(text))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "3/169:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"**(.+?)**\", '', str(text))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "3/170:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"*(.+?)*\", '', str(text))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "3/171:\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"[***](.+?)[***]\", '', str(text))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "3/172: log_cols['toString']\n",
      "3/173:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"[***](.+?)[***]\", '', str(text))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/174: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "3/175:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "3/176: descriptions.head(10)\n",
      "3/177: descriptions.iloc[, 15]\n",
      "3/178: descriptions.iloc[15]\n",
      "3/179: descriptions.loc[15]\n",
      "3/180: descriptions.loc[15]['toString']\n",
      "3/181: descriptions\n",
      "3/182:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"[***](.+?)[***]\", '', str(text))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "3/183:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"[***](.+?)[***]\", '', str(re.sub(\"{code}(.+?){code}\", '', str(text)))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "3/184:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.replace('\\t', ' ').str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"[***](.+?)[***]\", '', str(re.sub(\"{code}(.+?){code}\", '', str(text))))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "3/185:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "3/186: descriptions.head(6)\n",
      "3/187:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "3/188: descriptions.head(3)\n",
      "3/189: changelog.field.unique()\n",
      "3/190:\n",
      "# filter changelog with the textual fields\n",
      "time_log_filtered = changelog[changelog['field'].isin(['timeestimate', 'timespent', 'Time Spent', 'timeoriginalestimate'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "time_log_cols = time_log_filtered[cols]\n",
      "\n",
      "time_log_cols.field.unique()\n",
      "\n",
      "\n",
      "\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "time_log_grouped = time_log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "time_log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "time_latest_logs_from_issues = pd.merge(time_log_cols, time_log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(time_log_cols.shape[0], time_log_grouped.shape[0], time_latest_logs_from_issues.shape[0]) \n",
      "time_latest_logs_from_issues.head(6)\n",
      "time_log_cols = time_latest_logs_from_issues\n",
      "3/191: time_log_filtered.shape[0]\n",
      "3/192: changelog.shape[0]\n",
      "3/193:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field')\n",
      "3/194:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').coutn\n",
      "3/195:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').count\n",
      "3/196:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').agg('sum')\n",
      "3/197:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').agg({'created':'count'})\n",
      "3/198: time_log_cols\n",
      "3/199: time_log_cols.sort_values('key')\n",
      "3/200:\n",
      "# filter changelog with the textual fields\n",
      "time_log_filtered = changelog[changelog['field'].isin(['timeestimate', 'timespent', 'Time Spent', 'timeoriginalestimate'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'to', 'toString']\n",
      "time_log_cols = time_log_filtered[cols]\n",
      "\n",
      "time_log_cols.field.unique()\n",
      "\n",
      "\n",
      "\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "time_log_grouped = time_log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "time_log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "time_latest_logs_from_issues = pd.merge(time_log_cols, time_log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(time_log_cols.shape[0], time_log_grouped.shape[0], time_latest_logs_from_issues.shape[0]) \n",
      "time_latest_logs_from_issues.head(6)\n",
      "time_log_cols = time_latest_logs_from_issues\n",
      "3/201: time_log_cols[(time_log_cols['toString'] <> 0) || (time_log_cols['to'] <> 0) ].sort_values('key')\n",
      "3/202: time_log_cols[(time_log_cols['toString'] != 0) || (time_log_cols['to'] != 0) ].sort_values('key')\n",
      "3/203: time_log_cols[(time_log_cols['toString'] != 0) | (time_log_cols['to'] != 0) ].sort_values('key')\n",
      "3/204: time_log_cols[(time_log_cols['toString'] != '0') | (time_log_cols['to'] != '0') ].sort_values('key')\n",
      "3/205: time_log_cols[(time_log_cols['toString'] != '0') | (time_log_cols['toString'] != 'NaN') ].sort_values('key')\n",
      "3/206: time_log_cols[(time_log_cols['toString'] != '0') | (time_log_cols['toString'].isna() = False) ].sort_values('key')\n",
      "3/207: time_log_cols[(time_log_cols['toString'] != '0') | (time_log_cols['toString'].na() = False) ].sort_values('key')\n",
      "3/208: time_log_cols[(time_log_cols['toString'] != '0') | (time_log_cols['toString'].isnan() = False) ].sort_values('key')\n",
      "3/209: time_log_cols[(time_log_cols['toString'] != '0') | (time_log_cols['toString'].isnan() == False) ].sort_values('key')\n",
      "3/210: time_log_cols[(time_log_cols['toString'] != '0') | (np.isnan(time_log_cols['toString']) == False) ].sort_values('key')\n",
      "3/211: time_log_cols[(time_log_cols['toString'] != '0') | (pd.isnull(time_log_cols['toString']) == False) ].sort_values('key')\n",
      "3/212:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').agg({'created':'count'}).head()\n",
      "3/213:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').agg({'created':'count'}).head()\n",
      "3/214:\n",
      "time_log_cols = time_log_cols[(time_log_cols['toString'] != '0') | (pd.isnull(time_log_cols['toString']) == False) ].sort_values('key')\n",
      "time_log_cols.head(3)\n",
      "3/215:\n",
      "# filter changelog with the textual fields\n",
      "time_log_filtered = changelog[changelog['field'].isin(['timeestimate', 'timespent', 'Time Spent', 'timeoriginalestimate'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'to', 'toString']\n",
      "time_log_cols = time_log_filtered[cols]\n",
      "\n",
      "time_log_cols.field.unique()\n",
      "\n",
      "\n",
      "\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "time_log_grouped = time_log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "time_log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "time_latest_logs_from_issues = pd.merge(time_log_cols, time_log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(time_log_cols.shape[0], time_log_grouped.shape[0], time_latest_logs_from_issues.shape[0]) \n",
      "time_latest_logs_from_issues.head(6)\n",
      "time_log_cols = time_latest_logs_from_issues\n",
      "3/216:\n",
      "time_log_cols = time_log_cols[(time_log_cols['toString'] != '0') & (pd.isnull(time_log_cols['toString']) == False) ].sort_values('key')\n",
      "time_log_cols.head(3)\n",
      "3/217:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').agg({'created':'count'}).head()\n",
      "\n",
      "time_col_coutns = time_log_cols.groupby('key', 'project', 'author').agg({'created': 'count'})\n",
      "time_col_coutns.head(6)\n",
      "#time_col_coutns.reset_index(level= [0,1,2], inplace=True)\n",
      "3/218:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').agg({'created':'count'}).head()\n",
      "\n",
      "time_col_coutns = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "time_col_coutns.head(6)\n",
      "#time_col_coutns.reset_index(level= [0,1,2], inplace=True)\n",
      "3/219:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').agg({'created':'count'}).head()\n",
      "\n",
      "time_col_coutns = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "time_col_coutns = time_col_coutns.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "time_col_coutns.head(6)\n",
      "3/220:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').agg({'created':'count'}).head()\n",
      "\n",
      "time_col_coutns = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "time_col_coutns = time_col_coutns.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "time_col_coutns\n",
      "3/221:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').agg({'created':'count'}).head()\n",
      "\n",
      "time_col_coutns = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "time_col_coutns = time_col_coutns.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "print(time_col_coutns)\n",
      "3/222:\n",
      "# filter changelog with the textual fields\n",
      "time_log_filtered = changelog[changelog['field'].isin(['timeestimate', 'timespent', 'Time Spent', 'timeoriginalestimate'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'to', 'toString']\n",
      "time_log_cols = time_log_filtered[cols]\n",
      "\n",
      "time_log_cols.field.unique()\n",
      "\n",
      "\n",
      "\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "time_log_grouped = time_log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "time_log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "time_latest_logs_from_issues = pd.merge(time_log_cols, time_log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(time_log_cols.shape[0], time_log_grouped.shape[0], time_latest_logs_from_issues.shape[0]) \n",
      "time_latest_logs_from_issues.head(6)\n",
      "time_log_cols = time_latest_logs_from_issues\n",
      "3/223:\n",
      "time_log_cols = time_log_cols[(time_log_cols['toString'] != '0') & (pd.isnull(time_log_cols['toString']) == False) ].sort_values('key')\n",
      "time_log_cols.head(3)\n",
      "3/224:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').agg({'created':'count'}).head()\n",
      "\n",
      "time_cols_counts = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "time_col_coutns_ds = time_col_coutns.reset_index(level= [0,1,2], inplace=True)\n",
      "time_col_coutns_ds\n",
      "3/225:\n",
      "# filter changelog with the textual fields\n",
      "time_log_filtered = changelog[changelog['field'].isin(['timeestimate', 'timespent', 'Time Spent', 'timeoriginalestimate'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'to', 'toString']\n",
      "time_log_cols = time_log_filtered[cols]\n",
      "\n",
      "time_log_cols.field.unique()\n",
      "\n",
      "\n",
      "\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "time_log_grouped = time_log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "time_log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "time_latest_logs_from_issues = pd.merge(time_log_cols, time_log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(time_log_cols.shape[0], time_log_grouped.shape[0], time_latest_logs_from_issues.shape[0]) \n",
      "time_latest_logs_from_issues.head(6)\n",
      "time_log_cols = time_latest_logs_from_issues\n",
      "3/226:\n",
      "time_log_cols = time_log_cols[(time_log_cols['toString'] != '0') & (pd.isnull(time_log_cols['toString']) == False) ].sort_values('key')\n",
      "time_log_cols.head(3)\n",
      "3/227:\n",
      "changelog.shape[0]\n",
      "changelog.groupby('field').agg({'created':'count'}).head()\n",
      "\n",
      "time_cols_counts = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "#time_col_coutns_ds = time_col_coutns.reset_index(level= [0,1,2], inplace=True)\n",
      "time_col_coutns\n",
      "3/228:\n",
      "time_log_cols = time_log_cols[(time_log_cols['toString'] != '0') & (pd.isnull(time_log_cols['toString']) == False) ].sort_values('key')\n",
      "time_log_cols.head(3)\n",
      "3/229:\n",
      "time_cols_counts = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "#time_col_coutns_ds = time_col_coutns.reset_index(level= [0,1,2], inplace=True)\n",
      "time_col_coutns\n",
      "3/230:\n",
      "time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "#time_col_coutns_ds = time_col_coutns.reset_index(level= [0,1,2], inplace=True)\n",
      "#time_col_coutns\n",
      "3/231:\n",
      "time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "time_log_cols_ds = time_log_cols.reset_index(level= [0,1,2], inplace=True)\n",
      "time_log_cols_ds\n",
      "3/232:\n",
      "time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "#time_log_cols_ds = time_log_cols.reset_index(level= [0,1,2], inplace=True)\n",
      "#time_log_cols_ds\n",
      "3/233:\n",
      "time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "time_log_cols.reset_index(level= [0,1,2], inplace=True)\n",
      "#time_log_cols_ds\n",
      "3/234:\n",
      "time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "time_log_cols.reset_index(level= [0,1], inplace=True)\n",
      "#time_log_cols_ds\n",
      "3/235:\n",
      "time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "time_log_cols.reset_index(level= [0,1, 2, 3], inplace=True)\n",
      "#time_log_cols_ds\n",
      "3/236:\n",
      "time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "time_log_cols.reset_index(level= [0,1, 2], inplace=False)\n",
      "#time_log_cols_ds\n",
      "3/237:\n",
      "time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "time_log_cols.reset_index(level= [0], inplace=False)\n",
      "#time_log_cols_ds\n",
      "3/238:\n",
      "time_log_cols_gr = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "time_log_cols_gr.reset_index(level= [0, 1, 2], inplace=False)\n",
      "#time_log_cols_ds\n",
      "3/239:\n",
      "time_log_cols_gr = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "ime_log_cols_gr_ds = time_log_cols_gr.reset_index(level= [0, 1, 2], inplace=False)\n",
      "ime_log_cols_gr_ds\n",
      "3/240:\n",
      "time_log_cols_gr = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "ime_log_cols_gr_ds = time_log_cols_gr.reset_index(level= [0, 1, 2], inplace=False)\n",
      "ime_log_cols_gr_ds[ime_log_cols_gr_ds['created']>1]\n",
      "3/241:\n",
      "time_log_cols_gr = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "ime_log_cols_gr_ds = time_log_cols_gr.reset_index(level= [0, 1, 2], inplace=False)\n",
      "ime_log_cols_gr_ds[ime_log_cols_gr_ds['created']>1].shape[0]\n",
      "3/242:\n",
      "time_log_cols = time_log_cols[(time_log_cols['toString'] != '0') & (pd.isnull(time_log_cols['toString']) == False) ].sort_values('key')\n",
      "time_log_cols.head(3)\n",
      "time_log_cols_gr = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "ime_log_cols_gr_ds = time_log_cols_gr.reset_index(level= [0, 1, 2], inplace=False)\n",
      "ime_log_cols_gr_ds[ime_log_cols_gr_ds['created']>1].shape[0]\n",
      "3/243:\n",
      "time_log_cols = time_log_cols[(time_log_cols['toString'] != '0') & (pd.isnull(time_log_cols['toString']) == False) ].sort_values('key')\n",
      "time_log_cols_gr = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "ime_log_cols_gr_ds = time_log_cols_gr.reset_index(level= [0, 1, 2], inplace=False)\n",
      "ime_log_cols_gr_ds[ime_log_cols_gr_ds['created']>1].shape[0]\n",
      " 7/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      " 7/2:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      " 7/3: changelog.head()\n",
      " 7/4: issues.head()\n",
      " 7/5: sprints.head()\n",
      " 7/6: users.head()\n",
      " 7/7: print(issues.project.unique())\n",
      " 7/8: changelog.groupby(['key', 'field']).count().head(10)\n",
      " 7/9: changelog.head()\n",
      "7/10:\n",
      "##########------ Take the two examplary from/to texts for each unique field ------------######### \n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_ = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_)\n",
      "    to_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row, 'field':field_row, 'from':from_row, 'to':to_row})\n",
      "7/11:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/12:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/13:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text))\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    text = re.sub(\"[***](.+?)[***]\", '', str(text))\n",
      "    return text\n",
      "                  \n",
      "#return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"[***](.+?)[***]\", '', str(re.sub(\"{code}(.+?){code}\", '', str(text))))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "7/14: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/15:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/16:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/17: descriptions.head(3)\n",
      "7/18:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text))\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    text = re.sub(\"[***](.+?)[***]\", '', str(text))\n",
      "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
      "    return text\n",
      "                  \n",
      "#return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"[***](.+?)[***]\", '', str(re.sub(\"{code}(.+?){code}\", '', str(text))))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "7/19:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/20:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/21:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text))\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    text = re.sub(\"[***](.+?)[***]\", '', str(text))\n",
      "    text = re.sub(r'^https:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
      "    return text\n",
      "                  \n",
      "#return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"[***](.+?)[***]\", '', str(re.sub(\"{code}(.+?){code}\", '', str(text))))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "7/22:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text))\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    text = re.sub(\"[***](.+?)[***]\", '', str(text))\n",
      "    text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
      "    return text\n",
      "                  \n",
      "#return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"[***](.+?)[***]\", '', str(re.sub(\"{code}(.+?){code}\", '', str(text))))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "7/23:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/24: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/25:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/26:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/27:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text))\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    text = re.sub(\"[***](.+?)[***]\", '', str(text))\n",
      "    text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', str(text), flags=re.MULTILINE)\n",
      "    return text\n",
      "                  \n",
      "#return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"[***](.+?)[***]\", '', str(re.sub(\"{code}(.+?){code}\", '', str(text))))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "7/28: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/29:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/30:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/31:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Filter the textual columns to remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "def removeCodeSnippet(text):\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text))\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    text = re.sub(\"[***](.+?)[***]\", '', str(text))\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    return text\n",
      "                  \n",
      "#return re.sub(\"<(.+?)>\", '', re.sub(\"{{(.+?)}}\", '', re.sub(\"[***](.+?)[***]\", '', str(re.sub(\"{code}(.+?){code}\", '', str(text))))))\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "7/32: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/33:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/34:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/35:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[***](.+?)[***]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "7/36: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/37:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/38:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/39:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/40:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[***](.+?)[***]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "7/41: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/42:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/43:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/44:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/45:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/46:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[***](.+?)[***]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "7/47: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/48:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/49:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/50:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/51:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/52:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[***](.+?)[***]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "7/53:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/54:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/55: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/56:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/57:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/58:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[***](.+?)[***]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "\n",
      "\n",
      "\n",
      "#{code}\n",
      "7/59: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/60:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/61:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/62:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/63:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/64: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/65:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/66:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/67:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "7/68: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/69:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/70:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/71:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "re.sub(\"[***](.+?)[***]\", '', str(descriptions.loc[15]['toString']))\n",
      "7/72:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/73: re.sub(\"{code}(.+?){code}\", '', str('asdasd {code} this is code part {code} not anymore'))\n",
      "7/74: re.sub(\"[***](.+?)[***]\", '', str('asdasd *** this is code part *** not anymore'))\n",
      "7/75: re.sub(\"***(.+?)***\", '', str('asdasd *** this is code part *** not anymore'))\n",
      "7/76: re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str('asdasd *** this is code part *** not anymore'))\n",
      "7/77:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/78:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/79:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/80: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/81:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/82:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/83:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/84:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/85: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/86:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/87:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "descriptions.loc[15]['toString']\n",
      "7/88:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"{code}(.+?){code}\", '', str(descriptions.loc[15]['toString']))\n",
      "7/89:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"<(.+?)>\", '', str(re.sub(\"{code}(.+?){code}\", '', str(descriptions.loc[15]['toString'])))\n",
      "7/90:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"<(.+?)>\", '', str(re.sub(\"{code}(.+?){code}\", '', str(descriptions.loc[15]['toString']))))\n",
      "7/91:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"<(.+?)>\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"{code}(.+?){code}\", '', str(descriptions.loc[15]['toString']))))))\n",
      "7/92:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"{code}(.+?){code}\", '', str(descriptions.loc[15]['toString'])))))))\n",
      "7/93:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"{code}(.+?){code}\", '', str(descriptions.loc[15]['toString'])))))))\n",
      "7/94:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"{code}(.+?){code}\", '', str(descriptions.loc[15]['toString'])))))\n",
      "7/95:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"{code}(.+?){code}\", '', str(descriptions.loc[15]['toString']))))\n",
      "7/96:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"{code}(.+?){code}\", '', str(descriptions.loc[15]['toString'])))))))\n",
      "7/97:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"{code}(.+?){code}\", '', str(descriptions.loc[15]['toString']))))))))\n",
      "7/98:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"{code}(.+?){code}\", '', str(descriptions.loc[15]['toString']))))))))\n",
      "7/99:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub('http[s]?://\\S+', '', str(re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"<(.+?)>\", '', str(re.sub(\"{code}(.+?){code}\", '', str(descriptions.loc[15]['toString']))))))))))\n",
      "7/100:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub('http[s]?://\\S+', '', str(descriptions.loc[15]['toString']))\n",
      "7/101:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub('http[s]?://\\S+', '', str(descriptions.loc[15]['toString']))\n",
      "re.sub('[*][*][*](.+?)[*][*][*]', '', str(descriptions.loc[15]['toString']))\n",
      "7/102:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub('[*][*][*](.+?)[*][*][*]', '', str(descriptions.loc[15]['toString']))\n",
      "7/103:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub('[*][*][*](.+?)[*][*][*]', '', descriptions.loc[15]['toString'])\n",
      "7/104:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub('[*][*][*](.+?)[*][*][*]', '', '***Version\\r\\r\\n\\tSpring XD Version : spring-xd-1.3.0.RELEASE, spring-xd-1.3.0.RELEASE-yarn\\r\\r\\n\\tOS & Version: Linux 2.6.32-431.29.2.el6.x86_64 \\r\\r\\n\\tJava Version: java version \"1.7.0_65\"\\r\\r\\n\\r\\r\\n***Descrip')\n",
      "7/105:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub('[*][*][*](.+?)[*][*][*]', '', '***Version\\r\\r\\n\\tSpring XD Version ***')\n",
      "7/106:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub('[*][*][*](.+?)[*][*][*]', '', '*** asd asd ***')\n",
      "7/107:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub('[*][*][*](.+?)[*][*][*]', '', ' asd *** asd asd *** asd ')\n",
      "7/108:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/109:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub('http[s]?://\\S+', '', str(descriptions.loc[15]['toString']))\n",
      "re.sub('[*][*][*](.+?)[*][*][*]', '', str(descriptions.loc[15]['toString']))\n",
      "7/110: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/111:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/112:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "str(descriptions.loc[15]['toString'])\n",
      "7/113:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(descriptions.loc[15]['toString']))\n",
      "7/114:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "#re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "str(descriptions.loc[15]['toString'])\n",
      "7/115:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "#str(descriptions.loc[15]['toString'])\n",
      "7/116:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "\n",
      "re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(descriptions.loc[15]['toString']))\n",
      "7/117:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    \n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/118: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/119:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/120:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/121:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/122:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/123:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    \n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    \n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/124: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/125:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/126:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/127:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/128:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/129:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    \n",
      "    #text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    \n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/130: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/131:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/132:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/133:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    \n",
      "    #text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/134: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/135:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/136:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/137:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/138:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/139:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    \n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    \n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/140: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/141:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/142:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/143:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/144:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/145:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    \n",
      "    text = re.sub(\"[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*](.+?)[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*]\", '', str(text))\n",
      "    \n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/146: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/147:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/148:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/149:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/150:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/151:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*](.+?)[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*]\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('module(.+?)\"', '', str(text)) \n",
      "    \n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/152: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/153:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/154:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/155:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/156:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/157:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*](.+?)[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*]\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    \n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/158: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/159:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/160:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/161:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/162:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/163:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*](.+?)[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*]\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    \n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/164: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/165:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/166:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/167:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/168:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/169:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*](.+?)[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*]\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '', str(text)) \n",
      "    \n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/170: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/171:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/172:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/173:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/174:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/175:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*](.+?)[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*]\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    \n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/176: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/177:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/178:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/179: descriptions.head(15)\n",
      "7/180: descriptions['toStrong'].str.contains('***Describe Other Components', na=False)\n",
      "7/181: descriptions['toString'].str.contains('***Describe Other Components', na=False)\n",
      "7/182: descriptions.loc[15]['toString'].str.contains('***Describe Other Components', na=False)\n",
      "7/183: descriptions.loc[15]['toString'].contains('***Describe Other Components', na=False)\n",
      "7/184: descriptions.loc[15]['toString']\n",
      "7/185: descriptions.loc[15]['toString'].str.contains('***Describe Other Components', na=False)\n",
      "7/186: descriptions.['toString'].str.contains('***Describe Other Components', na=False)\n",
      "7/187: descriptions['toString'].str.contains('***Describe Other Components', na=False)\n",
      "7/188: descriptions['toString'].contains('***Describe Other Components', na=False)\n",
      "7/189: descriptions['toString'].str.contains('***Describe Other Components', na=False)\n",
      "7/190:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/191:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/192:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*](.+?)[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*]\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    \n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    \n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    \n",
      "    text = text.str.replace('***Description', '')\n",
      "    text = text.str.replace('***Steps to recreate the problem', '')\n",
      "    \n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/193:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*](.+?)[*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*][*]\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    \n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    \n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    \n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    \n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    \n",
      "    #text = re.sub('[\"module](.+?)[\"]', '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/194: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/195:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/196:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/197:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/198:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/199:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 20 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/200: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/201:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/202:\n",
      "# There still are fields with unnecessary code texts. example:\n",
      "\n",
      "    #text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    #text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    #text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove system logs:\n",
      "    #text = re.sub(\"[*][*][*](.+?)[*][*][*]\", '', str(text))\n",
      "    #remove web links:\n",
      "    #text = re.sub('http[s]?://\\S+', '', str(text))\n",
      "descriptions.loc[15]['toString']\n",
      "7/203: descriptions.loc[15]['toString']\n",
      "7/204:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "7/205:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "7/206:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code}(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', 'link', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "log_cols.groupby('field').size()\n",
      "7/207: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "7/208:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "7/209: descriptions.loc[15]['toString']\n",
      "7/210: descriptions.head(3)\n",
      "7/211: descriptions.to_csv(\"descriptions_cleaned.csv\")\n",
      "7/212: descriptions.loc[7]['toString']\n",
      "7/213: descriptions.loc[8]['toString']\n",
      "7/214: descriptions.loc[8]['toString']\n",
      "7/215: descriptions.loc[7]['toString']\n",
      "7/216: descriptions.loc[5]['toString']\n",
      " 9/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      " 9/2:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      " 9/3:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      " 9/4:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      " 9/5:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      " 9/6:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      " 9/7:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove class snippet\n",
      "    text = re.sub(\"[class(.+?)]\", '', str(text))\n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', 'link', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "log_cols.groupby('field').size()\n",
      " 9/8:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      " 9/9:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/10:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove class snippet\n",
      "    text = re.sub(\"[class(.+?)]\", '', str(text))\n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', 'link', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "log_cols.groupby('field').size()\n",
      "9/11: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "9/12:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "9/13: descriptions.loc[5]['toString']\n",
      "9/14: descriptions.loc[6]['toString']\n",
      "9/15: descriptions.loc[6]\n",
      "9/16: descriptions.head(6)\n",
      "9/17: descriptions.loc[5].key\n",
      "9/18: descriptions.loc[6].key\n",
      "9/19: descriptions.loc[2].key\n",
      "9/20: descriptions.loc[9].key\n",
      "9/21: descriptions.loc[15].key\n",
      "9/22: descriptions.loc[0].key\n",
      "9/23: descriptions.loc[66].key\n",
      "9/24: descriptions.loc[67].key\n",
      "9/25: descriptions.loc[68].key\n",
      "9/26: descriptions.loc[69].key\n",
      "9/27: descriptions.loc[7].key\n",
      "9/28: descriptions.loc[7]\n",
      "9/29: descriptions.loc[7].toString\n",
      "9/30:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/31:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/32:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove class snippet\n",
      "    text = re.sub(\"[class(.+?)]\", '', str(text))\n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', 'link', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "log_cols.groupby('field').size()\n",
      "9/33: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "9/34:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "9/35: descriptions.loc[7].toString\n",
      "9/36: descriptions.to_csv(\"descriptions_cleaned.csv\")\n",
      "9/37:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/38:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/39:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove class snippet\n",
      "    text = re.sub(\"[[class](.+?)[]]\", '', str(text))\n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', 'link', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "log_cols.groupby('field').size()\n",
      "9/40: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "9/41:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "9/42: descriptions.loc[7].toString\n",
      "9/43: descriptions.to_csv(\"descriptions_cleaned.csv\")\n",
      "9/44: changelog.field.unique()\n",
      "9/45:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/46:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/47:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove class snippet\n",
      "    #text = re.sub(\"[[class](.+?)[]]\", '', str(text))\n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', 'link', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "log_cols.groupby('field').size()\n",
      "9/48: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "9/49:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "9/50: descriptions.loc[7].toString\n",
      "9/51: descriptions.to_csv(\"descriptions_cleaned.csv\")\n",
      "9/52: changelog.field.unique()\n",
      "9/53:\n",
      "##########------ Take the two examplary from/to texts for each unique field ------------######### \n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_ = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_)\n",
      "    to_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row, 'field':field_row, 'from':from_row, 'to':to_row})\n",
      "9/54:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/55:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/56:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove class snippet\n",
      "    #text = re.sub(\"[[class](.+?)[]]\", '', str(text))\n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', 'link', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "log_cols.groupby('field').size()\n",
      "9/57: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "9/58:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "9/59: descriptions.loc[7].toString\n",
      "9/60: descriptions.to_csv(\"descriptions_cleaned.csv\")\n",
      "9/61:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/62:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/63: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "9/64:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "9/65: descriptions.loc[47].toString\n",
      "9/66: descriptions.to_csv(\"descriptions_cleaned.csv\")\n",
      "9/67: descriptions.to_csv(\"descriptions_orig.csv\")\n",
      "9/68:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/69:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/70:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove class snippet\n",
      "    #text = re.sub(\"[[class](.+?)[]]\", '', str(text))\n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', 'link', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "log_cols.groupby('field').size()\n",
      "9/71: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "9/72:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "9/73: descriptions.loc[47].toString\n",
      "9/74: descriptions.to_csv(\"descriptions_cleaned.csv\")\n",
      "9/75:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/76:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/77: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "9/78:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "9/79: descriptions.loc[47].toString\n",
      "9/80:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/81:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/82:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove class snippet\n",
      "    #text = re.sub(\"[[class](.+?)[]]\", '', str(text))\n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', 'link', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "log_cols.groupby('field').size()\n",
      "9/83: descriptions['toString']\n",
      "9/84: descriptions['toString'].length()\n",
      "9/85: descriptions['textLength'] = Null\n",
      "9/86: descriptions['textLength'] = np.nan\n",
      "9/87: descriptions.head()\n",
      "9/88:\n",
      "descriptions['textLength'] = descriptions['textLength'].str.len() \n",
      "descriptions.head()\n",
      "9/89:\n",
      "descriptions['textLength'] = descriptions['textLength'].len() \n",
      "descriptions.head()\n",
      "9/90:\n",
      "descriptions['textLength'] = descriptions['textLength'].str.len() \n",
      "descriptions.head()\n",
      "9/91:\n",
      "descriptions['textLength'] = descriptions['textLength'].astype(str).str.len() \n",
      "descriptions.head()\n",
      "9/92: descriptions['textLength'] = np.nan\n",
      "9/93:\n",
      "descriptions['textLength'] = descriptions['textLength'].astype(object).str.len() \n",
      "descriptions.head()\n",
      "9/94: description['toString']\n",
      "9/95: descriptions['toString']\n",
      "9/96: descriptions['toString'].size\n",
      "9/97: descriptions['toString'].shape\n",
      "9/98: descriptions[1]['toString'].shape\n",
      "9/99: descriptions[0]['toString'].shape\n",
      "9/100: descriptions[4]['toString'].shape\n",
      "9/101: descriptions[4]['toString'].shape[0]\n",
      "9/102: descriptions[4]['toString'].size\n",
      "9/103: descriptions[4]['toString'].astype(str)\n",
      "9/104: descriptions[4]['toString'].type(str)\n",
      "9/105: descriptions[4]['toString'].astype('str')\n",
      "9/106: descriptions['toString'].str.len()\n",
      "9/107:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/108:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/109:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove code snippet 2:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove class snippet\n",
      "    #text = re.sub(\"[[class](.+?)[]]\", '', str(text))\n",
      "    # remove job calls\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', 'link', str(text))\n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "log_cols.groupby('field').size()\n",
      "9/110: log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "9/111:\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "9/112: descriptions['textLength'] = descriptions['toString'].str.len()\n",
      "9/113: descriptions\n",
      "9/114: descriptions['lencat'] = round(descriptions['toString'].str.len() / 1000)\n",
      "9/115: descriptions\n",
      "9/116: descriptions.groupby('lencat')\n",
      "9/117: descriptions.groupby('lencat').size\n",
      "9/118: descriptions.groupby('lencat').shape\n",
      "9/119: descriptions.groupby('lencat').size\n",
      "9/120: descriptions.groupby('lencat').count()\n",
      "9/121: descriptions[descriptions['lencat']>1].to_csv('descriptions_longtext.csv')\n",
      "9/122: sub(r'(\\/.*?\\.[\\w:]+)', '', str(descriptions.loc[70]['toString']))\n",
      "9/123: re.sub(r'(\\/.*?\\.[\\w:]+)', '', str(descriptions.loc[70]['toString']))\n",
      "9/124: re.findall(r'(\\/.*?\\.[\\w:]+)', str(descriptions.loc[70]['toString']))\n",
      "9/125: re.findall(\"(/[a-zA-Z\\./]*[\\s]?)\", str(descriptions.loc[70]['toString']))\n",
      "9/126: re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[70]['toString']))\n",
      "9/127: re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[105]['toString']))\n",
      "9/128: re.sub(r\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[105]['toString']))\n",
      "9/129: re.sub(r\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[105]['toString']))\n",
      "9/130: re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[105]['toString']))\n",
      "9/131: re.sub(r\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[105]['toString']))\n",
      "9/132: re.sub(r\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[105]['toString']))\n",
      "9/133:\n",
      "#re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[105]['toString']))\n",
      "re.sub(r'(?:/[^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[105]['toString']))\n",
      "9/134:\n",
      "#re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[105]['toString']))\n",
      "re.sub(r'(?:/[^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[70]['toString']))\n",
      "9/135:\n",
      "re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[70]['toString']))\n",
      "#re.sub(r'(?:/[^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[70]['toString']))\n",
      "9/136:\n",
      "#re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[70]['toString']))\n",
      "re.sub(r'(?:/[^/]+)+?/\\w+\\.\\w+', '', re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[70]['toString'])))\n",
      "9/137:\n",
      "re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[140]['toString']))\n",
      "#re.sub(r'(?:/[^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[70]['toString']))\n",
      "9/138:\n",
      "#re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[140]['toString']))\n",
      "re.sub(r'(?:/[^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[70]['toString']))\n",
      "9/139:\n",
      "#re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[140]['toString']))\n",
      "re.sub(r'(?:/[^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[140]['toString']))\n",
      "9/140:\n",
      "#re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[140]['toString']))\n",
      "re.sub(r'(?:[/][^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[140]['toString']))\n",
      "9/141:\n",
      "#re.sub(\"(/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[140]['toString']))\n",
      "re.sub(r'(?:[/][^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[140]['toString']))\n",
      "9/142:\n",
      "re.sub(\"(\\/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[140]['toString']))\n",
      "#re.sub(r'(?:[/][^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[140]['toString']))\n",
      "9/143:\n",
      "#re.sub(\"(\\/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[140]['toString']))\n",
      "re.sub(r'(?:[/][^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[140]['toString']))\n",
      "9/144:\n",
      "re.sub(\"(\\/[a-zA-Z\\./]*[\\s]?)\", '', str(descriptions.loc[140]['toString']))\n",
      "#re.sub(r'(?:[/][^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[140]['toString']))\n",
      "9/145:\n",
      "re.sub(\"(\\/[a-zA-Z\\.\\/]*[\\s]?)\", '', str(descriptions.loc[140]['toString']))\n",
      "#re.sub(r'(?:[/][^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[140]['toString']))\n",
      "9/146:\n",
      "re.sub(\"(\\/[a-zA-Z\\.\\/]*[\\s]?)\", '', str(descriptions.loc[140]['toString']))\n",
      "#re.sub(r'(?:[/][^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[140]['toString']))\n",
      "9/147: descriptions.loc[47].toString\n",
      "9/148:\n",
      "# detect very long texts. likely, these are log traces, so in case if they are very low number, we can eliminate from them.\n",
      "\n",
      "# TO-DO: export to CSV file the data where `lencat` >= 2. and then check one by one.\n",
      "\n",
      "descriptions['textLength'] = descriptions['toString'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['toString'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['lencat']>1].to_csv('descriptions_longtext.csv')\n",
      "9/149: descriptions = descriptions[descriptions['lencat']<2]\n",
      "9/150:\n",
      "#re.sub(\"(\\/[a-zA-Z\\.\\/]*[\\s]?)\", '', str(descriptions.loc[140]['toString']))\n",
      "#re.sub(r'(?:[/][^/]+)+?/\\w+\\.\\w+', '', str(descriptions.loc[140]['toString']))\n",
      "9/151: changelog.field.unique()\n",
      "10/1: runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')\n",
      "10/2: runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')\n",
      "10/3: runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')\n",
      "10/4: runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')\n",
      "10/5: runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')\n",
      "10/6: runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')\n",
      "10/7: runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')\n",
      "10/8: runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')\n",
      "10/9: runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')\n",
      "10/10: runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')\n",
      "10/11: runfile('C:/Users/admin/.spyder-py3/temp.py', wdir='C:/Users/admin/.spyder-py3')\n",
      "11/1: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/extract_from_docs.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/2: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/extract_from_docs.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/3: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/4: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/5: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/6: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/7: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/8: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/9: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/10: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "9/152: changelog.field.unique()\n",
      "9/153: descriptions = descriptions[descriptions['lencat']<2]\n",
      "11/11: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/12: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/13: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/14: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/15: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/16: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/17: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/18: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/19: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/20: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/21: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/22: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/23: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/24: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/25: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/26: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/27: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/28: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/29: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/30: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/31: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/32: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/33: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/34: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/35: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/36: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/37: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/38: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/39: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/40: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/41: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/42: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/43: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/44: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/45: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/46: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/47: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/48: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/49: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/50: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/51: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/52: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/53: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/54: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/55: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "11/56: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "9/154:\n",
      "##########------ Take the two examplary from/to texts for each unique field ------------######### \n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_ = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_)\n",
      "    to_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row, 'field':field_row, 'from':from_row, 'to':to_row})\n",
      "df.head()\n",
      "9/155: summaries.head(6)\n",
      "9/156: summaries.to_csv('summaries_cleaned.csv')\n",
      "9/157:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/158:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/159:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', ' ', str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/160:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*\", ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', ' ', str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/161:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', ' ', str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/162:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\"\\S+?(?=\\/)\\/\\S*\\/\\S*\"', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\"\\S+?(?=\\\\)\\\\\\S*\\\\\\S*\"', ' ', str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/163:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?\\(?=\\/\\)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?\\(?=\\\\\\)\\\\\\S*\\\\\\S*', ' ', str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/164:\n",
      "a = '\\S+?(?=\\/)\\/\\S*\\/\\S*'\n",
      "a\n",
      "9/165:\n",
      "a = '\\S+?(?=\\/)\\/\\S*\\/\\S*'\n",
      "print(a)\n",
      "9/166:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', ' ', str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/167:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    #text = re.sub('\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', ' ', str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/168:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/169:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/170:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    #text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', ' ', str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/171:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(\"\\S+?(?=\\\\)\\\\\\S*\\\\\\S*\", \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/172:\n",
      "a = \"\\S+?(?=\\\\)\\\\\\S*\\\\\\S*\"\n",
      "a\n",
      "9/173:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    a = \"\\S+?(?=\\\\)\\\\\\S*\\\\\\S*\"\n",
      "    text = re.sub(a, \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/174:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    a = str('\\S+?(?=\\\\)\\\\\\S*\\\\\\S*')\n",
      "    text = re.sub(a, \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/175:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/176:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "9/177: summaries.to_csv('summaries_cleaned.csv')\n",
      "9/178:\n",
      "# detect very long texts. likely, these are log traces, so in case if they are very low number, we can eliminate from them.\n",
      "descriptions['textLength'] = descriptions['toString'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['toString'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['lencat']>1].to_csv('descriptions_longtext.csv')\n",
      "9/179:\n",
      "# detect very long texts. likely, these are log traces, so in case if they are very low number, we can eliminate from them.\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 1000)\n",
      "print(summaries.groupby('lencat').count())\n",
      "9/180:\n",
      "# detect very long texts. likely, these are log traces, so in case if they are very low number, we can eliminate from them.\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 100)\n",
      "print(summaries.groupby('lencat').count())\n",
      "9/181:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "\n",
      "#descriptions.loc[47].toString\n",
      "9/182:\n",
      "# detect very long texts. likely, these are log traces, so in case if they are very low number, we can eliminate from them.\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 100)\n",
      "print(summaries.groupby('lencat').count())\n",
      "summaries[summaries['lencat']>1].to_csv('descriptions_longtext.csv')\n",
      "9/183:\n",
      "# detect very long texts. likely, these are log traces, so in case if they are very low number, we can eliminate from them.\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 100)\n",
      "print(summaries.groupby('lencat').count())\n",
      "summaries[summaries['lencat']>1].to_csv('summaries_longtext.csv')\n",
      "9/184:\n",
      "# detect very long texts. likely, these are log traces, so in case if they are very low number, we can eliminate from them.\n",
      "descriptions['textLength'] = descriptions['toString'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['toString'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['lencat']>1].to_csv('descriptions_longtext.csv')\n",
      "9/185:\n",
      "a = 'Update the Android distribution wizard/launch configuration to allow user to pick from the existing list of certificates (combo box), or to create a new one'\n",
      "\n",
      "a.length()\n",
      "9/186:\n",
      "a = 'Update the Android distribution wizard/launch configuration to allow user to pick from the existing list of certificates (combo box), or to create a new one'\n",
      "\n",
      "a.len()\n",
      "9/187:\n",
      "a = 'Update the Android distribution wizard/launch configuration to allow user to pick from the existing list of certificates (combo box), or to create a new one'\n",
      "\n",
      "a.str.len()\n",
      "9/188:\n",
      "a = 'Update the Android distribution wizard/launch configuration to allow user to pick from the existing list of certificates (combo box), or to create a new one'\n",
      "\n",
      "a.len()\n",
      "9/189:\n",
      "a = 'Update the Android distribution wizard/launch configuration to allow user to pick from the existing list of certificates (combo box), or to create a new one'\n",
      "\n",
      "length(a)\n",
      "9/190:\n",
      "a = 'Update the Android distribution wizard/launch configuration to allow user to pick from the existing list of certificates (combo box), or to create a new one'\n",
      "\n",
      "len(a)\n",
      "9/191:\n",
      "# detect very long texts. likely, these are log traces, so in case if they are very low number, we can eliminate from them.\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 1000)\n",
      "print(summaries.groupby('lencat').count())\n",
      "9/192:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/193:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/194:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/195:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/196:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/197:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "#log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/198:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "9/199:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/200: log_cols[log_cols['field']=='Comment']\n",
      "9/201:\n",
      "#log_cols[log_cols['field']=='Comment']\n",
      "log_filtered[log_cols['field']=='Comment']\n",
      "9/202:\n",
      "#log_cols[log_cols['field']=='Comment']\n",
      "log_filtered[log_filtered['field']=='Comment']\n",
      "9/203:\n",
      "#log_cols[log_cols['field']=='Comment']\n",
      "log_filtered[log_filtered['field']=='Comment']\n",
      "9/204:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "9/205:\n",
      "#log_cols[log_cols['field']=='Comment']\n",
      "changelog[changelog['field']=='Comment']\n",
      "9/206:\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "Comments = changelog[changelog['field']=='Comment']\n",
      "9/207:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/208:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/209:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments = log_cols['from'].apply(removeCodeSnippet)\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "#log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/210:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments = comments['from'].apply(removeCodeSnippet)\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "#log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/211:\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "comments = changelog[changelog['field']=='Comment']\n",
      "comments.head(6)\n",
      "9/212:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments = comments['from'].apply(removeCodeSnippet)\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "#log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/213:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/214:\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "comments = changelog[changelog['field']=='Comment']\n",
      "comments.head(6)\n",
      "9/215:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/216:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments = comments['from'].apply(removeCodeSnippet)\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "#log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "log_cols.groupby('field').size()\n",
      "9/217:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/218:\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "comments = changelog[changelog['field']=='Comment']\n",
      "comments.head(6)\n",
      "9/219:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "9/220:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/221:\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "comments = changelog[changelog['field']=='Comment'].copy(deep=True)\n",
      "comments.head(6)\n",
      "9/222:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/223:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments = comments['from'].apply(removeCodeSnippet)\n",
      "\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "comments = comments[comments['from'].str.len() >= 10]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "print(comments.shape[0])\n",
      "9/224:\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "comments = changelog[changelog['field']=='Comment'].copy(deep=True)\n",
      "comments.head(6)\n",
      "9/225:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/226:\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "comments = changelog[changelog['field']=='Comment'].copy(deep=True)\n",
      "comments.head(6)\n",
      "9/227:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/228:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments = comments['from'].apply(removeCodeSnippet)\n",
      "9/229: comments.head()\n",
      "9/230: comments\n",
      "9/231:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/232:\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "comments = changelog[changelog['field']=='Comment'].copy(deep=True)\n",
      "comments.head(6)\n",
      "9/233:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/234:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))\n",
      "    \n",
      "    \n",
      "    # remove local path links:\n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "9/235: comments\n",
      "9/236:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "9/237:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "\n",
      "log_cols = log_filtered[cols]\n",
      "comments = log_filtered[cols_for_comment]\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/238: comments.head(6)\n",
      "9/239:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "9/240:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = log_filtered[cols_for_comment].copy(deep=True)\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/241: comments.head(6)\n",
      "9/242:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "9/243:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "9/244: comments.head(6)\n",
      "9/245: changelog.head(6)\n",
      "9/246: log_filtered.head(6)\n",
      "9/247: log_filtered[log_filtered['field']=='comment'].head(6)\n",
      "9/248: log_filtered[log_filtered['field']=='Comment'].head(6)\n",
      "9/249:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = log_filtered[cols_for_comment].copy(deep=True)\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/250: log_filtered[log_filtered['field']=='Comment'].head(6)\n",
      "9/251: log_filtered[cols_for_comment]\n",
      "9/252: log_filtered\n",
      "9/253: log_filtered[log_filtered['field']=='Comment']\n",
      "9/254:\n",
      "log_filtered[log_filtered['field']=='Comment']\n",
      "cols_for_comment\n",
      "9/255:\n",
      "log_filtered[log_filtered['field']=='Comment']\n",
      "log_filtered[cols_for_comment]\n",
      "9/256:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = log_filtered[cols_for_comment].copy(deep=True)\n",
      "comments = comments[commets['field']=='Comment']\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/257:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = log_filtered[cols_for_comment].copy(deep=True)\n",
      "comments = comments[commets['field']=='Comment']\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/258:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = log_filtered[cols_for_comment].copy(deep=True)\n",
      "comments = comments[comments['field']=='Comment']\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/259:\n",
      "log_filtered[log_filtered['field']=='Comment']\n",
      "log_filtered[cols_for_comment]\n",
      "9/260: log_filtered[log_filtered['field']=='Comment']\n",
      "9/261: comments\n",
      "9/262: comments.head()\n",
      "9/263:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = changelog[cols_for_comment].copy(deep=True)\n",
      "comments = comments[comments['field']=='Comment']\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/264: comments.head()\n",
      "9/265:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/266:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/267:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/268:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "9/269:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "9/270:\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "comments = comments[comments['from'].str.len() >= 10]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "print(comments.shape[0])\n",
      "9/271:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "9/272:\n",
      "# detect very long texts. likely, these are log traces, so in case if they are very low number, we can eliminate from them.\n",
      "descriptions['textLength'] = descriptions['toString'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['toString'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['lencat']>1].to_csv('descriptions_longtext.csv')\n",
      "\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 1000)\n",
      "print(summaries.groupby('lencat').count())\n",
      "summaries[summaries['lencat']>1].to_csv('summaries_longtext.csv')\n",
      "\n",
      "comments['textLength'] = comments['toString'].str.len()\n",
      "comments['lencat'] = round(comments['toString'].str.len() / 1000)\n",
      "print(comments.groupby('lencat').count())\n",
      "comments[comments['lencat']>1].to_csv('comments_longtext.csv')\n",
      "9/273:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "9/274:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "9/275:\n",
      "# detect very long texts. likely, these are log traces, so in case if they are very low number, we can eliminate from them.\n",
      "descriptions['textLength'] = descriptions['toString'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['toString'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['lencat']>1].to_csv('descriptions_longtext.csv')\n",
      "\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 1000)\n",
      "print(summaries.groupby('lencat').count())\n",
      "summaries[summaries['lencat']>1].to_csv('summaries_longtext.csv')\n",
      "\n",
      "comments['textLength'] = comments['from'].str.len()\n",
      "comments['lencat'] = round(comments['from'].str.len() / 1000)\n",
      "print(comments.groupby('lencat').count())\n",
      "comments[comments['lencat']>1].to_csv('comments_longtext.csv')\n",
      "9/276:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "9/277:\n",
      "# detect very long texts. likely, these are log traces, so in case if they are very low number, we can eliminate from them.\n",
      "descriptions['textLength'] = descriptions['toString'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['toString'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['lencat']>1].to_csv('descriptions_longtext.csv')\n",
      "\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 1000)\n",
      "print(summaries.groupby('lencat').count())\n",
      "summaries[summaries['lencat']>1].to_csv('summaries_longtext.csv')\n",
      "\n",
      "comments['textLength'] = comments['from'].str.len()\n",
      "comments['lencat'] = round(comments['from'].str.len() / 1000)\n",
      "print(comments.groupby('lencat').count())\n",
      "comments[comments['lencat']>0].to_csv('comments_longtext.csv')\n",
      "9/278: comments\n",
      "9/279: comments.shape[0]\n",
      "9/280: comments.shape\n",
      "9/281: comments.shape[]\n",
      "9/282: comments.shape[0]\n",
      "9/283:\n",
      "a = int(comments.shape[0])\n",
      "a\n",
      "9/284:\n",
      "# TO-DO: \n",
      "# 1) Calculate actual completion time by: Dataset Issues, column 'created', column 'resolutiondate' when 'resolution.name'=='Complete'\n",
      "# 2) Calculate the story points for the issues calculated above.\n",
      "# 3) \n",
      "\n",
      "comments\n",
      "9/285:\n",
      "for column in comments.columns:\n",
      "    print(comments[comments[column]=='xd'].shape[0])\n",
      "9/286:\n",
      "for column in comments.columns:\n",
      "    print(comments[comments[column]=='xd'].shape[0])\n",
      "    print (column)\n",
      "9/287: str.strip('a')\n",
      "9/288: str.strip('a ')\n",
      "9/289: str.strip(' a ')\n",
      "9/290: str.strip(' a ')\n",
      "9/291:\n",
      "for column in comments.columns:\n",
      "    comments[column].dtype\n",
      "9/292:\n",
      "for column in comments.columns:\n",
      "    print(comments[column].dtype)\n",
      "9/293:\n",
      "for column in comments.columns:\n",
      "    if comments[column].dtype == 'object':\n",
      "        print (column)\n",
      "    else:\n",
      "        print('not')\n",
      "9/294:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "9/295:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "descriptions['textLength'] = descriptions['toString'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['toString'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['textLength']>1000].to_csv('descriptions_longtext.csv')\n",
      "\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 1000)\n",
      "print(summaries.groupby('lencat').count())\n",
      "summaries[summaries['textLength']>1000].to_csv('summaries_longtext.csv')\n",
      "\n",
      "comments['textLength'] = comments['from'].str.len()\n",
      "comments['lencat'] = round(comments['from'].str.len() / 1000)\n",
      "print(comments.groupby('lencat').count())\n",
      "comments[comments['textLength']>1000].to_csv('comments_longtext.csv')\n",
      "9/296:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "9/297:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "descriptions['textLength'] = descriptions['toString'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['toString'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['textLength']>1000].to_csv('descriptions_longtext.csv')\n",
      "descriptions[descriptions['textLength']<=1000].to_csv('descriptions_shorttext.csv')\n",
      "\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 1000)\n",
      "print(summaries.groupby('lencat').count())\n",
      "summaries[summaries['textLength']>1000].to_csv('summaries_longtext.csv')\n",
      "summaries[summaries['textLength']<=1000].to_csv('summaries_shorttext.csv')\n",
      "\n",
      "comments['textLength'] = comments['from'].str.len()\n",
      "comments['lencat'] = round(comments['from'].str.len() / 1000)\n",
      "print(comments.groupby('lencat').count())\n",
      "comments[comments['textLength']>1000].to_csv('comments_longtext.csv')\n",
      "comments[comments['textLength']<=1000].to_csv('comments_shorttext.csv')\n",
      "9/298: len('integration.bus.kafka.KafkaMessageBusTest')\n",
      "9/299:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = changelog[cols_for_comment].copy(deep=True)\n",
      "comments = comments[comments['field']=='Comment']\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/300:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/301:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove text with more than 30 character, that usually are the command codes.\n",
      "    text = re.sub('\\b\\S{30,}\\b', \" \", str(text))   \n",
      "\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9]\\/[0-9][0-9]\\/[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9]\\/[0-9][0-9]\\/[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9]\\.[0-9][0-9]\\.[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9]\\.[0-9][0-9]\\.[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9]\\-[0-9][0-9]\\-[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9]\\-[0-9][0-9]\\-[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    \n",
      "    \n",
      "    # remove system logs:\n",
      "    text = re.sub(\"[*****************************************************](.+?)[*****************************************************]\", '', str(text))\n",
      "    return text\n",
      "9/302:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "comments['from'] = comments['from'].apply(lambda x: str.strip(x))\n",
      "9/303:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/304:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove text with more than 30 character, that usually are the command codes.\n",
      "    text = re.sub('\\b\\S{30,}\\b', \" \", str(text))   \n",
      "\n",
      "    #remove dates:\n",
      "    \n",
      "    (Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\n",
      "    \n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "\n",
      "    \n",
      "    # remove system logs:\n",
      "    return text\n",
      "9/305:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "comments['from'] = comments['from'].apply(lambda x: str.strip(x))\n",
      "9/306:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove text with more than 30 character, that usually are the command codes.\n",
      "    text = re.sub('\\b\\S{30,}\\b', \" \", str(text))   \n",
      "\n",
      "    #remove dates:\n",
      "    \n",
      "    \n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "\n",
      "    \n",
      "    # remove system logs:\n",
      "    return text\n",
      "9/307:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "comments['from'] = comments['from'].apply(lambda x: str.strip(x))\n",
      "9/308:\n",
      "##########------ Take the two examplary from/to texts for each unique field ------------######### \n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.head()\n",
      "9/309:\n",
      "##########------ Take the two examplary from/to texts for each unique field ------------######### \n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.head()\n",
      "9/310:\n",
      "##########------ Take the two examplary from/to texts for each unique field ------------######### \n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "9/311:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "comments['from'] = comments['from'].apply(lambda x: str.strip(x))\n",
      "9/312:\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "comments = comments[comments['from'].str.len() >= 10]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "print(comments.shape[0])\n",
      "9/313:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "9/314:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "descriptions['textLength'] = descriptions['toString'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['toString'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['textLength']>1000].to_csv('descriptions_longtext.csv')\n",
      "descriptions[descriptions['textLength']<=1000].to_csv('descriptions_shorttext.csv')\n",
      "\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 1000)\n",
      "print(summaries.groupby('lencat').count())\n",
      "summaries[summaries['textLength']>1000].to_csv('summaries_longtext.csv')\n",
      "summaries[summaries['textLength']<=1000].to_csv('summaries_shorttext.csv')\n",
      "\n",
      "comments['textLength'] = comments['from'].str.len()\n",
      "comments['lencat'] = round(comments['from'].str.len() / 1000)\n",
      "print(comments.groupby('lencat').count())\n",
      "comments[comments['textLength']>1000].to_csv('comments_longtext.csv')\n",
      "comments[comments['textLength']<=1000].to_csv('comments_shorttext.csv')\n",
      "9/315:\n",
      "text = 'spring-cloud-dataflow-yarn-build-tests is my local new sub-project to run tests on a hadoop minicluster'\n",
      "\n",
      "text = re.sub('\\b\\S{30,}\\b', \" \", str(text))  \n",
      "text\n",
      "9/316:\n",
      "text = 'spring-cloud-dataflow-yarn-build-test'\n",
      "\n",
      "text = re.sub('\\b\\S{30,}\\b', \" \", str(text))  \n",
      "text\n",
      "9/317:\n",
      "text = 'springclouddataflowyarnbuildtest'\n",
      "\n",
      "text = re.sub('\\b\\S{30,}\\b', \" \", str(text))  \n",
      "text\n",
      "9/318:\n",
      "text = 'springclouddataflowyarnbuildtest'\n",
      "\n",
      "text = re.sub('\\b.{30,}\\b', \" \", str(text))  \n",
      "text\n",
      "9/319:\n",
      "text = 'springclouddataflowyarnbuildtest'\n",
      "\n",
      "text = re.sub('\\b.{30,}\\b', \" \", str(text))  \n",
      "text\n",
      "9/320:\n",
      "text = 'springclouddataflowyarnbuildtests sadc'\n",
      "\n",
      "text = re.sub('\\b.{30,}\\b', \" \", str(text))  \n",
      "text\n",
      "9/321:\n",
      "text = 'springclouddataflowyarnbuildtests sadc'\n",
      "\n",
      "text = re.sub('\\b.{30,}\\b', \"\", str(text))  \n",
      "text\n",
      "9/322:\n",
      "text = 'springclouddataflowyarnbuildtests sadc'\n",
      "\n",
      "text = re.sub('\\b.{30,}\\b', \"\", str(text))  \n",
      "text\n",
      "9/323:\n",
      "text = 'springclouddataflowyarnbuildtests sadc'\n",
      "\n",
      "text = re.sub('\\b.{30,}\\b', \"\", text)  \n",
      "text\n",
      "9/324:\n",
      "text = 'springclouddataflowyarnbuildtests sadc'\n",
      "\n",
      "text = re.sub('.\\S{30,}\\s', \"\", text)  \n",
      "text\n",
      "9/325:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove text with more than 30 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{30,}\\s', \" \", str(text))  \n",
      "    #remove dates:\n",
      "    \n",
      "    \n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "\n",
      "    \n",
      "    # remove system logs:\n",
      "    return text\n",
      "9/326:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "comments['from'] = comments['from'].apply(lambda x: str.strip(x))\n",
      "9/327:\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "comments = comments[comments['from'].str.len() >= 10]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "print(comments.shape[0])\n",
      "9/328:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "9/329:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "9/330:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove text with more than 30 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}\\s', \" \", str(text))  \n",
      "    #remove dates:\n",
      "    \n",
      "    \n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "\n",
      "    \n",
      "    # remove system logs:\n",
      "    return text\n",
      "9/331:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "comments['from'] = comments['from'].apply(lambda x: str.strip(x))\n",
      "9/332:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = changelog[cols_for_comment].copy(deep=True)\n",
      "comments = comments[comments['field']=='Comment']\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/333:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/334:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "9/335:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove text with more than 30 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}\\s', \" \", str(text))  \n",
      "    #remove dates:\n",
      "    \n",
      "    \n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "\n",
      "    \n",
      "    # remove system logs:\n",
      "    return text\n",
      "9/336:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "comments['from'] = comments['from'].apply(lambda x: str.strip(x))\n",
      "9/337:\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "comments = comments[comments['from'].str.len() >= 10]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "print(comments.shape[0])\n",
      "9/338:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "9/339:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove text with more than 30 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}\\s', \" \", str(text))  \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text)) \n",
      "    \n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "\n",
      "    \n",
      "    # remove system logs:\n",
      "    return text\n",
      "9/340:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "comments['from'] = comments['from'].apply(lambda x: str.strip(x))\n",
      "9/341:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = changelog[cols_for_comment].copy(deep=True)\n",
      "comments = comments[comments['field']=='Comment']\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/342: comments.head()\n",
      "9/343:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/344:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove text with more than 30 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}\\s', \" \", str(text))  \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text)) \n",
      "    \n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "\n",
      "    \n",
      "    # remove system logs:\n",
      "    return text\n",
      "9/345:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "comments['from'] = comments['from'].apply(lambda x: str.strip(x))\n",
      "9/346:\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "comments = comments[comments['from'].str.len() >= 10]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "print(comments.shape[0])\n",
      "9/347:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "9/348:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = changelog[cols_for_comment].copy(deep=True)\n",
      "comments = comments[comments['field']=='Comment']\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/349: comments.head()\n",
      "9/350:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/351:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove text with more than 30 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}\\s', \" \", str(text))  \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text)) \n",
      "    \n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "\n",
      "    \n",
      "    # remove system logs:\n",
      "    return text\n",
      "9/352:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "comments['from'] = comments['from'].apply(lambda x: str.strip(x))\n",
      "9/353:\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "comments = comments[comments['from'].str.len() >= 10]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "print(comments.shape[0])\n",
      "9/354:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "9/355:\n",
      "text ='3.4.6.2.3.2.0-2950Hadoop deploymentData Platform : Hortonworks HDP 2.3.2.0-2950RDBMS: MySQL ***Error Message: *****************************************************05:29:52,673   INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying job 'test_mr_job'05:29:53,655   INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying module [ModuleDescriptor@6e5af900 moduleName = 'test_mr_module', moduleLabel = 'test_mr_module', group = 'test_mr_job', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map[[empty]], children = list[[empty]]]05:30:24,351  ERROR inbound.job:test_mr_job-redis:queue-inbound-channel-adapter1 step.AbstractStep - Encountered an error executing step teststep in job test_mr_jobjava.lang.IllegalArgumentException: Unable to parse /apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framewoas a URI, check the setting for mapreduce.application.framework.path        at org.apache.hadoop.mapreduce.JobSubmitter.addMRFrameworkToDistributedCache(JobSubmitter.java:443)   .   .       .        at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:54)        at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:323)        at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:55)        at java.lang.Thread.run(Thread.java:745)Caused by: java.net.URISyntaxException: Illegal character in path at index 11: /hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework        at java.net.URI$Parser.fail(URI.java:2848)        at java.net.URI$Parser.checkChars(URI.java:3021)        at java.net.URI$Parser.parseHierarchical(URI.java:3105)        at java.net.URI$Parser.parse(URI.java:3063)        at java.net.URI.<init>(URI.java:588)        at org.apache.hadoop.mapreduce.JobSubmitter.addMRFrameworkToDistributedCache(JobSubmitter.java:441)*****************************************************'\n",
      "text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "text\n",
      "9/356:\n",
      "text ='3.4.6.2.3.2.0-2950Hadoop deploymentData Platform : Hortonworks HDP 2.3.2.0-2950RDBMS: MySQL ***Error Message: *****************************************************05:29:52,673   INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deployi29:53,655   INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying module [ModuleDescriptor@6e5af900 moduleName = 'test_mr_module', moduleLabel = 'test_mr_module', group = 'test_mr_job', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map[[empty]], children = list[[empty]]]05:30:24,351  ERROR inbound.job:test_mr_job-redis:queue-inbound-channel-adapter1 step.AbstractStep - Encountered an error executing step teststep in job test_mr_jobjava.lang.IllegalArgumentException: Unable to parse /apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framewoas a URI, check the setting for mapreduce.application.framework.path        at org.apache.hadoop.mapreduce.JobSubmitter.addMRFrameworkToDistributedCache(JobSubmitter.java:443)  .   .       .        at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:54)        at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:323)        at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:55)        at java.lang.Thread.run(Thread.java:745)Caused by: java.net.URISyntaxException: Illegal character in path at index 11: /hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework        at java.net.URI$Parser.fail(URI.java:2848)        at java.net.URI$Parser.checkChars(URI.java:3021)        at java.net.URI$Parser.parseHierarchical(URI.java:3105)        at java.net.URI$Parser.parse(URI.java:3063)        at java.net.URI.<init>(URI.java:588)        at org.apache.hadoop.mapreduce.JobSubmitter.addMRFrameworkToDistributedCache(JobSubmitter.java:441)*****************************************************'\n",
      "text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "text\n",
      "9/357:\n",
      "text ='3.4.6.2.3.2.0-2950Hadoop deploymentData Platform : Hortonworks HDP 2.3.2.0-2950RDBMS: MySQL ***Error Message: *****************************************************05:29:52,673   INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deployi29:53,655   INFO DeploymentsPathChildrenCatedCache(JobSubmitter.java:441)*****************************************************'\n",
      "text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "text\n",
      "9/358:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = changelog[cols_for_comment].copy(deep=True)\n",
      "comments = comments[comments['field']=='Comment']\n",
      "\n",
      "log_cols.field.unique()\n",
      "9/359: comments.head()\n",
      "9/360:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "9/361:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces.\n",
      "# remove code snippets\n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}\\s', \" \", str(text))  \n",
      "    \n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "\n",
      "    \n",
      "    # remove system logs:\n",
      "    return text\n",
      "9/362:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "comments['from'] = comments['from'].apply(removeCodeSnippet)\n",
      "comments['from'] = comments['from'].apply(lambda x: str.strip(x))\n",
      "9/363:\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "comments = comments[comments['from'].str.len() >= 10]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "print(comments.shape[0])\n",
      "9/364:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "epicThemes = log_cols[log_cols['field'] == 'Epic/Theme']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "epicThemes.to_csv('epicThemes.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "13/1: from ibm_watson import PersonalityInsightsV3\n",
      "13/2:\n",
      "api_key = 'lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00'\n",
      "url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "version='2017-10-13'\n",
      "13/3:\n",
      "_api_key = 'lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    url=_url\n",
      ")\n",
      "13/4:\n",
      "from ibm_watson import ApiException\n",
      "try:\n",
      "    # Invoke a Personality Insights method\n",
      "except ApiException as ex:\n",
      "    print \"Method failed with status code \" + str(ex.code) + \": \" + ex.message\n",
      "13/5:\n",
      "from ibm_watson import ApiException\n",
      "try:\n",
      "    \n",
      "except ApiException as ex:\n",
      "    print \"Method failed with status code \" + str(ex.code) + \": \" + ex.message\n",
      "13/6:\n",
      "from ibm_watson import ApiException\n",
      "try:\n",
      "    # Invoke a Personality Insights method\n",
      "except ApiException as ex:\n",
      "    print \"Method failed with status code \" + str(ex.code) + \": \" + ex.message\n",
      "13/7:\n",
      "personality_insights.set_detailed_response(True)\n",
      "response = personality_insights.methodName(parameters)\n",
      "# Access response from methodName\n",
      "print(json.dumps(response.get_result(), indent=2))\n",
      "# Access information in response headers\n",
      "print(response.get_headers())\n",
      "# Access HTTP response status\n",
      "print(response.get_status_code())\n",
      "13/8:\n",
      "personality_insights.set_detailed_response(True)\n",
      "response = personality_insights.methodName(parameters)\n",
      "# Access response from methodName\n",
      "print(json.dumps(response.get_result(), indent=2))\n",
      "# Access information in response headers\n",
      "print(response.get_headers())\n",
      "# Access HTTP response status\n",
      "print(response.get_status_code())\n",
      "13/9: personality_insights.set_default_headers({'x-watson-learning-opt-out': \"true\"})\n",
      "13/10:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "13/11:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/1:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/2:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "#from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/3:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core import authenticators\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/4:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import ibm_cloud_sdk_core\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/5:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import ibm_cloud_sdk_core\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/6:\n",
      "_api_key = 'lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    authenticator=authenticator\n",
      ")\n",
      "14/7:\n",
      "_api_key = 'lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator = iam_apikey(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    authenticator=authenticator\n",
      ")\n",
      "14/8:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import *\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/9:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core import iam_apikey \n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/10:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core import *\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/11:\n",
      "_api_key = 'lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator = iam_apikey(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    authenticator=authenticator\n",
      ")\n",
      "14/12:\n",
      "_api_key = 'lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  iam_access_token(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    authenticator=authenticator\n",
      ")\n",
      "14/13:\n",
      "_api_key = 'lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  icp4d_access_token(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    authenticator=authenticator\n",
      ")\n",
      "14/14:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/15:\n",
      "_api_key = 'lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    authenticator=authenticator\n",
      ")\n",
      "14/16:\n",
      "_api_key = 'lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(_api_key)\n",
      "14/17:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibmcloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/18:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/19:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "14/20:\n",
      "_api_key = 'lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(_api_key)\n",
      "14/21:\n",
      "\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    url=_url\n",
      ")\n",
      "14/22:\n",
      "_api_key = {\n",
      "  \"apikey\": \"PfkpGT4R3viAheJ6mxlcQbqZSsySGD6-B2yqAJK3UP-e\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key 42451159-1b87-4fef-a510-59aae8834dee\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-2cb38cd5-a9b6-46dc-bb69-93183a20ca98\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(_api_key)\n",
      "14/23:\n",
      "_api_key = {\n",
      "  \"apikey\": \"PfkpGT4R3viAheJ6mxlcQbqZSsySGD6-B2yqAJK3UP-e\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key 42451159-1b87-4fef-a510-59aae8834dee\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-2cb38cd5-a9b6-46dc-bb69-93183a20ca98\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "#authenticator =  IAMAuthenticator(_api_key)\n",
      "14/24:\n",
      "_api_key = {\n",
      "  \"apikey\": \"PfkpGT4R3viAheJ6mxlcQbqZSsySGD6-B2yqAJK3UP-e\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key 42451159-1b87-4fef-a510-59aae8834dee\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-2cb38cd5-a9b6-46dc-bb69-93183a20ca98\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator('PfkpGT4R3viAheJ6mxlcQbqZSsySGD6-B2yqAJK3UP-e')\n",
      "14/25:\n",
      "_api_key = {\n",
      "  \"apikey\": \"lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key 4c4345e6-4da3-486d-893b-a2440f336714\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-6d830052-263a-4849-be2f-395652c2cba2\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(_api_key)\n",
      "14/26:\n",
      "_api_key = {\n",
      "  \"apikey\": \"lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key 4c4345e6-4da3-486d-893b-a2440f336714\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-6d830052-263a-4849-be2f-395652c2cba2\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator('')\n",
      "14/27:\n",
      "_api_key = {\n",
      "  \"apikey\": \"lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key 4c4345e6-4da3-486d-893b-a2440f336714\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-6d830052-263a-4849-be2f-395652c2cba2\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator()\n",
      "14/28:\n",
      "_api_key = {\n",
      "  \"apikey\": \"lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key 4c4345e6-4da3-486d-893b-a2440f336714\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-6d830052-263a-4849-be2f-395652c2cba2\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(apikey)\n",
      "14/29:\n",
      "_api_key = {\n",
      "  \"apikey\": \"lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key 4c4345e6-4da3-486d-893b-a2440f336714\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-6d830052-263a-4849-be2f-395652c2cba2\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(1)\n",
      "14/30:\n",
      "_api_key = {\n",
      "  \"apikey\": \"lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key 4c4345e6-4da3-486d-893b-a2440f336714\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-6d830052-263a-4849-be2f-395652c2cba2\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(2)\n",
      "14/31:\n",
      "_api_key = {\n",
      "  \"apikey\": \"lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key 4c4345e6-4da3-486d-893b-a2440f336714\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-6d830052-263a-4849-be2f-395652c2cba2\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator('lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00')\n",
      "14/32:\n",
      "_api_key = {\n",
      "  \"apikey\": \"lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key 4c4345e6-4da3-486d-893b-a2440f336714\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-6d830052-263a-4849-be2f-395652c2cba2\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator('4c4345e6-4da3-486d-893b-a2440f336714')\n",
      "14/33:\n",
      "_api_key = {\n",
      "  \"apikey\": \"lxyFryEnLWMiAbBdMsbUeifOp6VbGVdMUzMQkxz9km00\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key 4c4345e6-4da3-486d-893b-a2440f336714\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-6d830052-263a-4849-be2f-395652c2cba2\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator('-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f')\n",
      "14/34:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator('-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f')\n",
      "14/35:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator('-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f')\n",
      "14/36:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(\"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\")\n",
      "14/37:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(\"apikey\":\"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\")\n",
      "14/38:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator({\"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\"})\n",
      "14/39:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.disable_SSL_verification()\n",
      "14/40:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator('-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f')\n",
      "14/41:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator(-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f)\n",
      "14/42:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator('{-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f}')\n",
      "14/43:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator('')\n",
      "14/44:\n",
      "import logging\n",
      "logging.basicConfig(level=logging.DEBUG)\n",
      "14/45:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator('')\n",
      "14/46:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator('j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f')\n",
      "14/47:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.enable_SSL_verification()\n",
      "authenticator = IAMAuthenticator('j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f')\n",
      "14/48:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator('j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f')\n",
      "14/49:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator('j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f')\n",
      "14/50:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator()\n",
      "14/51:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator(iam_apikey =\"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\")\n",
      "14/52:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator('-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f')\n",
      "14/53:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/54:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "personality_insights.disable_SSL_verification()\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/55:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/56:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/57:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/58:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/59:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/60:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/61:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/62:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/63:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/64:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/65:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/66:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/67:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/68:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "14/69:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(\"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\")\n",
      "14/70:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(_url)\n",
      "14/71:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(_url, '')\n",
      "14/72:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(_url, '', '', '', '')\n",
      "14/73:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(_url, '', '', '', '', '', '')\n",
      "14/74:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(_url, '', '', '', '', '')\n",
      "14/75:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =  IAMAuthenticator(_url, '', '', '', '', '', '', '', '')\n",
      "14/76:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =IAMAuthenticator(_url, '', '', '', '', '', '', '', '')\n",
      "14/77:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =IAMAuthenticator(_url, '', '', '', '', '', '', '', '', '')\n",
      "14/78:\n",
      "_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "\n",
      "authenticator =IAMAuthenticator(_url)\n",
      "15/1:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "15/2:\n",
      "aa_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "_api_key = '-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "authenticator =IAMAuthenticator(_url)\n",
      "15/3:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f',\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "\n",
      "authenticator = IAMAuthenticator(iam_apikey)\n",
      "15/4:\n",
      "aa_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "_api_key = '-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "authenticator =IAMAuthenticator(_url)\n",
      "15/5:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "authenticator = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    authenticator=authenticator\n",
      ")\n",
      "15/6:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "authenticator = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13'\n",
      ")\n",
      "15/7:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    authenticator=authenticatorr\n",
      ")\n",
      "15/8:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=authenticatorr\n",
      ")\n",
      "15/9:\n",
      "import logging\n",
      "logging.basicConfig(level=logging.DEBUG)\n",
      "15/10:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=authenticatorr\n",
      ")\n",
      "15/11:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=authenticatorr\n",
      ")\n",
      "15/12:\n",
      "aa_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "_api_key = '-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "authenticator =IAMAuthenticator(_url)\n",
      "15/13:\n",
      "aa_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "_api_key = '-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "authenticator =IAMAuthenticator(_api_key)\n",
      "15/14:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "15/15:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    authentication_type = \n",
      "    iam_url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "15/16:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    iam_url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "15/17:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    iam_url=_url,\n",
      "    vcap_services_name = None\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "15/18:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    iam_url=_url,\n",
      "    vcap_services_name = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "15/19:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    iam_url=_url,\n",
      "    vcap_services_name = 'a'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "15/20:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    iam_url=_url,\n",
      "    vcap_services_name = 'a',\n",
      "    a = 0\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "15/21:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    iam_url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "15/22:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    iam_url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/1:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "16/2:\n",
      "aa_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "_api_key = '-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "authenticator =IAMAuthenticator(_api_key)\n",
      "16/3:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    iam_url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/4:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/5:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/6:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key\n",
      "    #url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/7:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version\n",
      "    #iam_apikey=_api_key\n",
      "    #url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/8:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    authentication_type = ''\n",
      "    #iam_apikey=_api_key\n",
      "    #url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/9:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    authentication_type = ''\n",
      "    iam_apikey=_api_key\n",
      "    #url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/10:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    authentication_type = '',\n",
      "    iam_apikey=_api_key\n",
      "    #url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/11:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key\n",
      "    #url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/12:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key\n",
      "    url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/13:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/14:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    #version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/15:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/16:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/17:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    iam_apikey=_api_key,\n",
      "    url=_url\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/18:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    url='https://gateway.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/19:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version\n",
      "    #url='https://gateway.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/20:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/21:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/22:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    self,\n",
      "    version=_version,\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/23:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    #version=_version,\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/24:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version=_version,\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/25:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    version=_version\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/26:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version=_version\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/27:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version=\"2017-10-13\"\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/28:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version=\"2017-13-10\"\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/29:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version=\"2017-13-\"\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/30:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version=\"-13-\"\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/31:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version=\"\"\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/32:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version=0\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/33:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version=2\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/34:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-07-13'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/35:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/36:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13 00:00:00'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/37:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='20171013'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/38:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017.10.13'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/39:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/40:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/41:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/42:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/43:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    #url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/44:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    api_key = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/45:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/46:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api+key\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/47:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/48:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/49:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/50:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/51:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/52:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/53:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/54:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/55:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/56:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/57:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/58:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/59:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/60:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/61:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/62:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/63:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/64:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/65:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/66:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "16/67:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/1:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "17/2:\n",
      "aa_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "_api_key = '-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "authenticator =IAMAuthenticator(_api_key)\n",
      "17/3:\n",
      "aa_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "_api_key = '-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "authenticator =IAMAuthenticator(_api_key)\n",
      "17/4:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/5:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/6:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/7:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/8:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/9:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/10:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/11:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/12:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/13:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "#    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/14:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "#    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "#    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/15:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "#    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13'\n",
      "#    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/16:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "#    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "#    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/17:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "#    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',asdas\n",
      "#    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/18:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "#    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "#    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/19:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "#    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13'\n",
      "#    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/20:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "#    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13'\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/21:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "#    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/22:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "#    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/23:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "#    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/24:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/25:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key\n",
      "#    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/26:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key\n",
      "    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/27:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/28:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "    url = '',\n",
      "    vcap_services_name = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/29:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "    url = '',\n",
      "    vcap_services_name = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/30:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "    url = ''\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/31:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    iam_url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/32:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/33:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "    authentication_type = 'iam'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/34:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "    authentication_type = 'iam'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/35:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "    authentication_typea = 'iam'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/36:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = _api_key,\n",
      "    authentication_type = 'iam'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/37:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "    authentication_type = 'iam'\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/38:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/39:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\"\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/40:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\"\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "17/41:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\"\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "18/1:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "18/2:\n",
      "aa_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "_api_key = '-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "authenticator =IAMAuthenticator(_api_key)\n",
      "18/3:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\"\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "18/4:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\"\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "18/5:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\"\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "19/1:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "19/2:\n",
      "aa_api_key = {\n",
      "  \"apikey\": \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\",\n",
      "  \"iam_apikey_description\": \"Auto-generated for key d1c4465b-82e8-4e45-a3c8-45213f61ee52\",\n",
      "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
      "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
      "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/8f4af5e91250487cb342b2e8c99a0be7::serviceid:ServiceId-284054ac-a57d-46fb-9ee4-1a35021936e1\",\n",
      "  \"url\": \"https://gateway-lon.watsonplatform.net/personality-insights/api\"\n",
      "}\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "_api_key = '-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f'\n",
      "authenticator =IAMAuthenticator(_api_key)\n",
      "19/3:\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    url=\"https://gateway-lon.watsonplatform.net/personality-insights/api\",\n",
      "    version='2017-10-13',\n",
      "    iam_apikey = \"-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f\"\n",
      ")\n",
      "#personality_insights.disable_SSL_verification()\n",
      "19/4:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "from __future__ import print_function\n",
      "19/5:\n",
      "from __future__ import print_function\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "19/6:\n",
      "from __future__ import print_function\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "import csv\n",
      "19/7:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='-j7YXVrQgQzEblgb8YRWtX2VrIAO_QZ7mWRG0QkOmT7f')\n",
      "19/8:\n",
      "from __future__ import print_function\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "import csv\n",
      "19/9:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='P6Hm76z8y8F3w5HPwaJcXust9ZbayeQoIc4NbAMg2fBg')\n",
      "19/10:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='P6Hm76z8y8F3w5HPwaJcXust9ZbayeQoIc4NbAMg2fBg')\n",
      "19/11:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=authenticatorr,\n",
      "    vcap_services_name='personality_insights'\n",
      ")\n",
      "19/12:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=authenticatorr,\n",
      ")\n",
      "19/13:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=authenticatorr\n",
      ")\n",
      "19/14:\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "_version='2017-10-13'\n",
      "_api_key = 'oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR'\n",
      "authenticator =IAMAuthenticator(_api_key)\n",
      "19/15:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "19/16:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "19/17:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "19/18:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "19/19:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "19/20:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "19/21:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "19/22:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "19/23:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "19/24:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "19/25:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "19/26:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='72cf6735-88c7-434e-b9d6-6c0419e10fc4')\n",
      "20/1:\n",
      "from __future__ import print_function\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "import csv\n",
      "20/2:\n",
      "from __future__ import print_function\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "import csv\n",
      "20/3:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "21/1:\n",
      "from __future__ import print_function\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "import csv\n",
      "21/2:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "22/1:\n",
      "from __future__ import print_function\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "import csv\n",
      "22/2:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "22/3:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "24/1:\n",
      "from __future__ import print_function\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "import csv\n",
      "24/2:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "24/3:\n",
      "from ibm_watson import NaturalLanguageUnderstandingV1\n",
      "from ibm_watson.natural_language_understanding_v1 import Features, CategoriesOptions\n",
      "24/4:\n",
      "iam_key = 'Yqn114zF90c8iK-qZZ1Y6NH9w-TFfHWemfcKGikkx64e'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/natural-language-understanding/api'\n",
      "\n",
      "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
      "    version='2019-07-12',\n",
      "    iam_apikey=iam_key,\n",
      "    url=_url\n",
      ")\n",
      "\n",
      "response = natural_language_understanding.analyze(\n",
      "    url='www.ibm.com',\n",
      "    features=Features(categories=CategoriesOptions(limit=3))).get_result()\n",
      "\n",
      "print(json.dumps(response, indent=2))\n",
      "26/1: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "26/2: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "26/3: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "26/4: runfile('C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master/untitled0.py', wdir='C:/Users/admin/Desktop/TRANSFER_DESKTOP/Homeworks/Thesis/data/traits library/lexica-master/lexica-master')\n",
      "27/1:\n",
      "from ibm_watson import NaturalLanguageUnderstandingV1\n",
      "from ibm_watson.natural_language_understanding_v1 import Features, CategoriesOptions\n",
      "27/2:\n",
      "iam_key = 'Yqn114zF90c8iK-qZZ1Y6NH9w-TFfHWemfcKGikkx64e'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/natural-language-understanding/api'\n",
      "\n",
      "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
      "    version='2019-07-12',\n",
      "    iam_apikey=iam_key,\n",
      "    url=_url\n",
      ")\n",
      "\n",
      "response = natural_language_understanding.analyze(\n",
      "    url='www.ibm.com',\n",
      "    features=Features(categories=CategoriesOptions(limit=3))).get_result()\n",
      "\n",
      "print(json.dumps(response, indent=2))\n",
      "27/3:\n",
      "from ibm_watson import NaturalLanguageUnderstandingV1\n",
      "from ibm_watson.natural_language_understanding_v1 import Features, CategoriesOptions\n",
      "import json\n",
      "27/4:\n",
      "iam_key = 'Yqn114zF90c8iK-qZZ1Y6NH9w-TFfHWemfcKGikkx64e'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/natural-language-understanding/api'\n",
      "\n",
      "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
      "    version='2019-07-12',\n",
      "    iam_apikey=iam_key,\n",
      "    url=_url\n",
      ")\n",
      "\n",
      "response = natural_language_understanding.analyze(\n",
      "    url='www.ibm.com',\n",
      "    features=Features(categories=CategoriesOptions(limit=3))).get_result()\n",
      "\n",
      "print(json.dumps(response, indent=2))\n",
      "27/5:\n",
      "iam_key = 'Yqn114zF90c8iK-qZZ1Y6NH9w-TFfHWemfcKGikkx64e'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/natural-language-understanding/api'\n",
      "\n",
      "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
      "    version='2019-07-12',\n",
      "    iam_apikey=iam_key,\n",
      "    url=_url\n",
      ")\n",
      "\n",
      "response = natural_language_understanding.analyze(\n",
      "    url='tedo gogoladze',\n",
      "    features=Features(categories=CategoriesOptions(limit=3))).get_result()\n",
      "\n",
      "print(json.dumps(response, indent=2))\n",
      "27/6:\n",
      "iam_key = 'Yqn114zF90c8iK-qZZ1Y6NH9w-TFfHWemfcKGikkx64e'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/natural-language-understanding/api'\n",
      "\n",
      "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
      "    version='2019-07-12',\n",
      "    iam_apikey=iam_key,\n",
      "    url=_url\n",
      ")\n",
      "\n",
      "response = natural_language_understanding.analyze(\n",
      "    url='www.ibm.com',\n",
      "    features=Features(categories=CategoriesOptions(limit=3))).get_result()\n",
      "\n",
      "print(json.dumps(response, indent=2))\n",
      "27/7: iam_key = 'Yqn114zF90c8iK-qZZ1Y6NH9w-TFfHWemfcKGikkx64e'\n",
      "27/8: _api_key = 'Yqn114zF90c8iK-qZZ1Y6NH9w-TFfHWemfcKGikkx64e'\n",
      "27/9:\n",
      "authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=authenticatorr\n",
      ")\n",
      "27/10:\n",
      "authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=authenticatorr\n",
      ")\n",
      "27/11:\n",
      "from __future__ import print_function\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "import csv\n",
      "27/12:\n",
      "#authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=_api_key\n",
      ")\n",
      "27/13:\n",
      "#authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=_api_key,\n",
      "    url = _url\n",
      ")\n",
      "27/14:\n",
      "_api_key = 'oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR'\n",
      "_url = 'https://gateway-lon.watsonplatform.net/personality-insights/api'\n",
      "27/15:\n",
      "#authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=_api_key,\n",
      "    url = _url\n",
      ")\n",
      "27/16:\n",
      "#authenticatorr = IAMAuthenticator(_api_key)\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=_api_key,\n",
      "    url = _url\n",
      ")\n",
      "27/17:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "#authenticator = IAMAuthenticator('{apikey}')\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    authenticator=_api_key,\n",
      "    url=_url\n",
      ")\n",
      "\n",
      "personality_insights.set_service_url('{url}')\n",
      "27/18:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "#from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "#authenticator = IAMAuthenticator('{apikey}')\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    authenticator=_api_key,\n",
      "    url=_url\n",
      ")\n",
      "\n",
      "personality_insights.set_service_url('{url}')\n",
      "27/19:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "#from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "#authenticator = IAMAuthenticator('{apikey}')\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=_api_key,\n",
      "    url=_url\n",
      ")\n",
      "\n",
      "personality_insights.set_service_url('{url}')\n",
      "27/20:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "#from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "#authenticator = IAMAuthenticator('{apikey}')\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=_api_key\n",
      ")\n",
      "\n",
      "personality_insights.set_service_url('{url}')\n",
      "27/21:\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "#from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
      "from os.path import join, dirname\n",
      "import json\n",
      "\n",
      "#authenticator = IAMAuthenticator('{apikey}')\n",
      "personality_insights = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    iam_apikey=_api_key,\n",
      "    url=_url\n",
      ")\n",
      "\n",
      "personality_insights.set_service_url(_url)\n",
      "27/22:\n",
      "from __future__ import print_function\n",
      "import json\n",
      "from os.path import join, dirname\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import csv\n",
      "27/23:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "27/24:\n",
      "with open(join(dirname(__file__), '../resources/personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "    print(json.dumps(profile, indent=2))\n",
      "27/25:\n",
      "os.path.dirname(os.path.abspath(\"__file__\"))\n",
      "\n",
      "with open(join(dirname(__file__), '../resources/personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "    print(json.dumps(profile, indent=2))\n",
      "27/26:\n",
      "from __future__ import print_function\n",
      "import json\n",
      "from os.path import join, dirname\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import csv\n",
      "import os\n",
      "27/27:\n",
      "os.path.dirname(os.path.abspath(\"__file__\"))\n",
      "\n",
      "with open(join(dirname(__file__), '../resources/personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "    print(json.dumps(profile, indent=2))\n",
      "27/28:\n",
      "if '__file__' in vars():\n",
      "    wk_dir = os.path.dirname(os.path.realpath('__file__'))\n",
      "else:\n",
      "    print('We are running the script interactively')\n",
      "\n",
      "with open(join(dirname(__file__), '../resources/personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "    print(json.dumps(profile, indent=2))\n",
      "27/29:\n",
      "if '__file__' in vars():\n",
      "    wk_dir = os.path.dirname(os.path.realpath('__file__'))\n",
      "else:\n",
      "    print('We are running the script interactively')\n",
      "\n",
      "with open(join(dirname('__file__'), '../resources/personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "    print(json.dumps(profile, indent=2))\n",
      "27/30:\n",
      "if '__file__' in vars():\n",
      "    wk_dir = os.path.dirname(os.path.realpath('__file__'))\n",
      "else:\n",
      "    print('We are running the script interactively')\n",
      "\n",
      "with open(join(dirname('__file__'), 'personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "    print(json.dumps(profile, indent=2))\n",
      "27/31:\n",
      "if '__file__' in vars():\n",
      "    wk_dir = os.path.dirname(os.path.realpath('__file__'))\n",
      "else:\n",
      "    print('We are running the script interactively')\n",
      "\n",
      "with open(join(dirname('__file__'), 'personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "    print(json.dumps(profile, indent=2))\n",
      "27/32: log_cols.shape\n",
      "27/33: log_cols.shape()\n",
      "27/34: log_cols.shape[0]\n",
      "27/35:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "27/36:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "27/37: changelog.head()\n",
      "27/38:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = changelog[cols_for_comment].copy(deep=True)\n",
      "comments = comments[comments['field']=='Comment']\n",
      "\n",
      "log_cols.field.unique()\n",
      "27/39: comments.head()\n",
      "27/40:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "27/41: log_cols.shape[0]\n",
      "27/42: log_cols\n",
      "27/43: log_cols['project'].unique()\n",
      "27/44: log_cols\n",
      "27/45: log_cols.head(3)\n",
      "27/46:\n",
      "for proj in log_cols['project'].unique():\n",
      "    print(log_cols[log_cols['project']==proj]['author'].unique())\n",
      "27/47:\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        print(log_cols[(log_cols['project']==project & log_cols['author']==dev_user)]  )\n",
      "27/48:\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        print(log_cols[(log_cols['project']==project & (log_cols['author']==dev_user))]  )\n",
      "27/49:\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        print(project, dev_user)\n",
      "27/50:\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        #print(project, dev_user)\n",
      "27/51:\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        #print(project, dev_user)\n",
      "         log_cols[log_cols['project']==project]\n",
      "27/52:\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        #print(project, dev_user)\n",
      "         log_cols[log_cols['project']==project & log_cols['author']==dev_user]\n",
      "27/53:\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        #print(project, dev_user)\n",
      "         log_cols[log_cols['project']==project & (log_cols['author']==dev_user)]\n",
      "27/54:\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        #print(project, dev_user)\n",
      "         log_cols[log_cols['project']==project and (log_cols['author']==dev_user)]\n",
      "27/55:\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        #print(project, dev_user)\n",
      "         log_cols[(log_cols['project']==project) and (log_cols['author']==dev_user)]\n",
      "27/56:\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        #print(project, dev_user)\n",
      "         log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "27/57:\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        #print(project, dev_user)\n",
      "        user_text = ''\n",
      "        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "27/58:\n",
      "log_cols.head(3)\n",
      "for index, row in log_cols():\n",
      "    print(row['key'], row['author'])\n",
      "27/59:\n",
      "log_cols.head(3)\n",
      "for index, row in log_cols:\n",
      "    print(row['key'], row['author'])\n",
      "27/60:\n",
      "log_cols.head(3)\n",
      "for index, row in log_cols.iterrows():\n",
      "    print(row['key'], row['author'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/61:\n",
      "log_cols.head(3)\n",
      "#for index, row in log_cols.iterrows():\n",
      "#    print(row['key'], row['author'])\n",
      "27/62:\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        #print(project, dev_user)\n",
      "        user_text = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = user_txt + row['toString'] + '. '\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "27/63:\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        #print(project, dev_user)\n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = user_txt + row['toString'] + '. '\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "27/64:\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        #print(project, dev_user)\n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['toString']) + '. '\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "27/65: df_user_name\n",
      "27/66:\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        #print(project, dev_user)\n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['toString']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "27/67:\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        #print(project, dev_user)\n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['toString']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "        \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "27/68: user_text_combined.size\n",
      "27/69: user_text_combined.size()\n",
      "27/70: user_text_combined.size\n",
      "27/71:\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        #print(project, dev_user)\n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['toString']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "        \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "print(user_text_combined.head(6))       \n",
      "print(user_text_combined.size)\n",
      "27/72:\n",
      "comments.head()\n",
      "\n",
      "comments.groupby((['project', 'key', 'author', from])).agg({'created':'max'})\n",
      "27/73:\n",
      "comments.head()\n",
      "\n",
      "comments.groupby((['project', 'key', 'author', 'from'])).agg({'created':'max'})\n",
      "27/74:\n",
      "comments.head()\n",
      "\n",
      "comments.groupby((['project', 'key', 'author', 'from'])).count\n",
      "27/75:\n",
      "comments.head()\n",
      "\n",
      "comments.groupby((['project', 'key', 'author', 'from'])).count()\n",
      "27/76: comments.head()\n",
      "27/77: comments.head()\n",
      "27/78:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols['text_field'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "# Comments need to be treated differently, sinec there are more than one comment per a jira task.\n",
      "# Also, the text column for Comments is only 'from'\n",
      "cols_for_comment = ['key', 'project', 'author', 'field', 'created', 'from']\n",
      "comments = changelog[cols_for_comment].copy(deep=True)\n",
      "comments = comments[comments['field']=='Comment']\n",
      "\n",
      "log_cols.head()\n",
      "27/79:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "log_cols = log_cols['key', 'project', 'author', 'field', 'created', 'text']\n",
      "27/80:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "27/81:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "log_cols = log_cols['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols.head()\n",
      "27/82:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "27/83:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "log_filtered.head()\n",
      "27/84:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "27/85:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "27/86:\n",
      "log_cols.head(3)\n",
      "#for index, row in log_cols.iterrows():\n",
      "#    print(row['key'], row['author'])\n",
      "27/87:\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        #print(project, dev_user)\n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "        \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "print(user_text_combined.head(6))       \n",
      "print(user_text_combined.size)\n",
      "27/88:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "27/89:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "27/90:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "27/91:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "latest_logs_from_issues.head(6)\n",
      "log_cols = latest_logs_from_issues\n",
      "27/92: log_cols.head(3)\n",
      "27/93:\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "        \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "print(user_text_combined.head(6))       \n",
      "print(user_text_combined.size)\n",
      "27/94: user_text_combined.size\n",
      "27/95: user_text_combined.head()\n",
      "27/96:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "        \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "print(user_text_combined.size)\n",
      "print(user_text_combined.head(6))\n",
      "27/97:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "27/98:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces. \n",
      "# remove code snippets \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\n', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\r', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\t', ' ')\n",
      "log_cols['toString'] = log_cols['toString'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}\\s', \" \", str(text))  \n",
      "    \n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "\n",
      "    \n",
      "    return text\n",
      "27/99:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces. \n",
      "# remove code snippets \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}\\s', \" \", str(text))  \n",
      "    \n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "\n",
      "    \n",
      "    return text\n",
      "27/100:\n",
      "text ='3.4.6.2.3.2.0-2950Hadoop deploymentData Platform : Hortonworks HDP 2.3.2.0-2950RDBMS: MySQL ***Error Message: *****************************************************05:29:52,673   INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deployi29:53,655   INFO DeploymentsPathChildrenCatedCache(JobSubmitter.java:441)*****************************************************'\n",
      "text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "text\n",
      "27/101:\n",
      "log_cols['toString'] = log_cols['toString'].apply(removeCodeSnippet)\n",
      "log_cols['toString'] = log_cols['toString'].apply(lambda x: str.strip(x))\n",
      "27/102:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "27/103:\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "#log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "27/104:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "descriptions['textLength'] = descriptions['toString'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['toString'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['textLength']>1000].to_csv('descriptions_longtext.csv')\n",
      "descriptions[descriptions['textLength']<=1000].to_csv('descriptions_shorttext.csv')\n",
      "\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 1000)\n",
      "print(summaries.groupby('lencat').count())\n",
      "summaries[summaries['textLength']>1000].to_csv('summaries_longtext.csv')\n",
      "summaries[summaries['textLength']<=1000].to_csv('summaries_shorttext.csv')\n",
      "\n",
      "comments['textLength'] = comments['from'].str.len()\n",
      "comments['lencat'] = round(comments['from'].str.len() / 1000)\n",
      "print(comments.groupby('lencat').count())\n",
      "comments[comments['textLength']>1000].to_csv('comments_longtext.csv')\n",
      "comments[comments['textLength']<=1000].to_csv('comments_shorttext.csv')\n",
      "27/105:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "27/106:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "descriptions['textLength'] = descriptions['toString'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['toString'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['textLength']>1000].to_csv('descriptions_longtext.csv')\n",
      "descriptions[descriptions['textLength']<=1000].to_csv('descriptions_shorttext.csv')\n",
      "\n",
      "summaries['textLength'] = summaries['toString'].str.len()\n",
      "summaries['lencat'] = round(summaries['toString'].str.len() / 1000)\n",
      "print(summaries.groupby('lencat').count())\n",
      "summaries[summaries['textLength']>1000].to_csv('summaries_longtext.csv')\n",
      "summaries[summaries['textLength']<=1000].to_csv('summaries_shorttext.csv')\n",
      "\n",
      "comments['textLength'] = comments['from'].str.len()\n",
      "comments['lencat'] = round(comments['from'].str.len() / 1000)\n",
      "print(comments.groupby('lencat').count())\n",
      "comments[comments['textLength']>1000].to_csv('comments_longtext.csv')\n",
      "comments[comments['textLength']<=1000].to_csv('comments_shorttext.csv')\n",
      "27/107:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "descriptions['textLength'] = descriptions['text'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['text'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['textLength']>1000].to_csv('descriptions_longtext.csv')\n",
      "descriptions[descriptions['textLength']<=1000].to_csv('descriptions_shorttext.csv')\n",
      "\n",
      "summaries['textLength'] = summaries['text'].str.len()\n",
      "summaries['lencat'] = round(summaries['text'].str.len() / 1000)\n",
      "print(summaries.groupby('lencat').count())\n",
      "summaries[summaries['textLength']>1000].to_csv('summaries_longtext.csv')\n",
      "summaries[summaries['textLength']<=1000].to_csv('summaries_shorttext.csv')\n",
      "\n",
      "comments['textLength'] = comments['text'].str.len()\n",
      "comments['lencat'] = round(comments['text'].str.len() / 1000)\n",
      "print(comments.groupby('lencat').count())\n",
      "comments[comments['textLength']>1000].to_csv('comments_longtext.csv')\n",
      "comments[comments['textLength']<=1000].to_csv('comments_shorttext.csv')\n",
      "27/108: descriptions.head()\n",
      "27/109: descriptions['textLength'].hist()\n",
      "27/110: descriptions['textLength'].hist(bins = 100)\n",
      "27/111: descriptions['textLength'].hist(bins = 1)\n",
      "27/112: descriptions['textLength'].hist(bins = 10)\n",
      "27/113: descriptions['textLength'].hist(bins = 5)\n",
      "27/114: descriptions['textLength'].hist(bins = 1000)\n",
      "27/115: descriptions['textLength'].hist(bins = 100)\n",
      "27/116:\n",
      "plt.figure(figsize=(18,36))\n",
      "descriptions['textLength'].hist(bins = 100)\n",
      "27/117:\n",
      "plt.figure(figsize=(18,20))\n",
      "descriptions['textLength'].hist(bins = 100)\n",
      "27/118:\n",
      "plt.figure(figsize=(16,14))\n",
      "descriptions['textLength'].hist(bins = 100)\n",
      "27/119:\n",
      "plt.figure(figsize=(16,10))\n",
      "descriptions['textLength'].hist(bins = 100)\n",
      "27/120:\n",
      "plt.figure(figsize=(14,8))\n",
      "#df.col.rank(pct=True).hist()\n",
      "descriptions['textLength'].rank(pct=True).hist(bins = 100)\n",
      "\n",
      "#descriptions['textLength'].hist(bins = 100)\n",
      "27/121:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "descriptions['textLength'].hist(bins = 100)\n",
      "27/122:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "descriptions['textLength'].hist(bins = 1000)\n",
      "27/123:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "descriptions['textLength'].hist()\n",
      "27/124:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "descriptions['textLength'].hist(bins=200)\n",
      "27/125:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "descriptions['textLength'].hist(bins=50)\n",
      "27/126:\n",
      "plt.figure(figsize=(14,8))\n",
      "(descriptions['textLength'] / descriptions['textLength'].abs().max()).hist()\n",
      "\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "27/127:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "descriptions['textLength'].hist(bins=50)\n",
      "27/128:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "import seaborn as sns\n",
      "import re\n",
      "import seaborn as sns\n",
      "27/129:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "\n",
      "\n",
      "ax = sns.barplot(x=\"x\", y=\"x\", data=descriptions['textLength'], estimator=lambda x: len(x) / len(df) * 100)\n",
      "ax.set(ylabel=\"Percent\")\n",
      "27/130:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "\n",
      "\n",
      "df = pd.DataFrame(dict(x=np.random.poisson(4, 500)))\n",
      "ax = sns.barplot(textLength=\"x\", y=\"x\", data=descriptions['textLength'], estimator=lambda x: len(x) / len(df) * 100)\n",
      "ax.set(ylabel=\"Percent\")\n",
      "27/131:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "\n",
      "\n",
      "df = pd.DataFrame(dict(x=np.random.poisson(4, 500)))\n",
      "ax = sns.barplot(x=\"textLength\", y=\"x\", data=descriptions['textLength'], estimator=lambda x: len(x) / len(df) * 100)\n",
      "ax.set(ylabel=\"Percent\")\n",
      "27/132:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "\n",
      "ax=barplot(x=descriptions['textLength'])\n",
      "\n",
      "#ax = sns.barplot(x=\"textLength\", y=\"x\", data=descriptions['textLength'], estimator=lambda x: len(x) / len(df) * 100)\n",
      "#ax.set(ylabel=\"Percent\")\n",
      "27/133:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "\n",
      "ax=sns.barplot(x=descriptions['textLength'])\n",
      "\n",
      "#ax = sns.barplot(x=\"textLength\", y=\"x\", data=descriptions['textLength'], estimator=lambda x: len(x) / len(df) * 100)\n",
      "#ax.set(ylabel=\"Percent\")\n",
      "27/134:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "#ax = sns.barplot(x=\"textLength\", y=\"x\", data=descriptions['textLength'], estimator=lambda x: len(x) / len(df) * 100)\n",
      "#ax.set(ylabel=\"Percent\")\n",
      "27/135:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "descriptions['textLength'].hist(bins=50)\n",
      "\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "#ax = sns.barplot(x=\"textLength\", y=\"x\", data=descriptions['textLength'], estimator=lambda x: len(x) / len(df) * 100)\n",
      "#ax.set(ylabel=\"Percent\")\n",
      "27/136:\n",
      "plt.figure(figsize=(14,8))\n",
      "\n",
      "descriptions['textLength'].hist(bins=50)\n",
      "\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "#ax = sns.barplot(x=\"textLength\", y=\"x\", data=descriptions['textLength'], estimator=lambda x: len(x) / len(df) * 100)\n",
      "#ax.set(ylabel=\"Percent\")\n",
      "27/137:\n",
      "plt.figure(figsize=(14,6))\n",
      "\n",
      "descriptions['textLength'].hist(bins=50)\n",
      "\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "#ax = sns.barplot(x=\"textLength\", y=\"x\", data=descriptions['textLength'], estimator=lambda x: len(x) / len(df) * 100)\n",
      "#ax.set(ylabel=\"Percent\")\n",
      "27/138:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "27/139:\n",
      "plt.figure(figsize=(14,6))\n",
      "\n",
      "descriptions['textLength'].hist(bins=50)\n",
      "\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "#ax = sns.barplot(x=\"textLength\", y=\"x\", data=descriptions['textLength'], estimator=lambda x: len(x) / len(df) * 100)\n",
      "#ax.set(ylabel=\"Percent\")\n",
      "27/140:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "descriptions['textLength'] = descriptions['text'].str.len()\n",
      "descriptions['lencat'] = round(descriptions['text'].str.len() / 1000)\n",
      "print(descriptions.groupby('lencat').count())\n",
      "descriptions[descriptions['textLength']>1000].to_csv('descriptions_longtext.csv')\n",
      "descriptions[descriptions['textLength']<=1000].to_csv('descriptions_shorttext.csv')\n",
      "\n",
      "summaries['textLength'] = summaries['text'].str.len()\n",
      "summaries['lencat'] = round(summaries['text'].str.len() / 1000)\n",
      "print(summaries.groupby('lencat').count())\n",
      "summaries[summaries['textLength']>1000].to_csv('summaries_longtext.csv')\n",
      "summaries[summaries['textLength']<=1000].to_csv('summaries_shorttext.csv')\n",
      "\n",
      "comments['textLength'] = comments['text'].str.len()\n",
      "comments['lencat'] = round(comments['text'].str.len() / 1000)\n",
      "print(comments.groupby('lencat').count())\n",
      "comments[comments['textLength']>1000].to_csv('comments_longtext.csv')\n",
      "comments[comments['textLength']<=1000].to_csv('comments_shorttext.csv')\n",
      "27/141:\n",
      "plt.figure(figsize=(14,6))\n",
      "\n",
      "descriptions['textLength'].hist(bins=50)\n",
      "\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "#ax = sns.barplot(x=\"textLength\", y=\"x\", data=descriptions['textLength'], estimator=lambda x: len(x) / len(df) * 100)\n",
      "#ax.set(ylabel=\"Percent\")\n",
      "27/142:\n",
      "plt.figure(figsize=(14,6))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "sns.distplot(descriptions['textLength'])\n",
      "\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "#ax = sns.barplot(x=\"textLength\", y=\"x\", data=descriptions['textLength'], estimator=lambda x: len(x) / len(df) * 100)\n",
      "#ax.set(ylabel=\"Percent\")\n",
      "27/143:\n",
      "plt.figure(figsize=(14,6))\n",
      "\n",
      "descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "27/144:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "import seaborn as sns\n",
      "import re\n",
      "import seaborn as sns\n",
      "27/145:\n",
      "plt.figure(figsize=(14,6))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/146:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/147:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(2))\n",
      "plt.show()\n",
      "27/148:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(10))\n",
      "plt.show()\n",
      "27/149:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(100))\n",
      "plt.show()\n",
      "27/150:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(10000))\n",
      "plt.show()\n",
      "27/151:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(10))\n",
      "plt.show()\n",
      "27/152:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/153:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.hist().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/154:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/155:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.hist(yaxis.set_major_formatter(PercentFormatter(1)))\n",
      "plt.show()\n",
      "27/156:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/157:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(bins=50, descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/158:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']))\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/159:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']), bins=50)\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/160:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']), bins=1000)\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/161:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']), bins=200)\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/162:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']), bins=100)\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/163:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']), bins=1)\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/164:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']), bins=2)\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/165:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']), bins=10)\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/166:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']), bins=50)\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.show()\n",
      "27/167:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']), bins=50)\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.annotate(\"Annotation\",\n",
      "            xy=(x1, y1), xycoords='data',\n",
      "            xytext=(x2, y2), textcoords='offset points',\n",
      "            )\n",
      "plt.show()\n",
      "27/168:\n",
      "plt.figure(figsize=(14,4))\n",
      "\n",
      "#descriptions['textLength'].hist(bins=50)\n",
      "#ax=sns.barplot(x=descriptions['textLength'], )\n",
      "\n",
      "\n",
      "\n",
      "plt.hist(descriptions['textLength'], weights=np.ones(len(descriptions['textLength'])) / len(descriptions['textLength']), bins=50)\n",
      "\n",
      "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
      "plt.legend()\n",
      "plt.show()\n",
      "27/169:\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "27/170:\n",
      "fig, ax = plt.subplots(figsize=(8,8))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "27/171:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "27/172:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "27/173:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "27/174:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "27/175:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "27/176:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Annotation for bar values\n",
      "ax.annotate('Each bar shows count and percentage of total',\n",
      "            xy=(.85,.30), xycoords='figure fraction',\n",
      "            horizontalalignment='center', verticalalignment='bottom',\n",
      "            fontsize=10, bbox=dict(boxstyle=\"round\", fc=\"white\"),\n",
      "            rotation=-90)\n",
      "27/177:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "27/178:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "\n",
      "#create legend\n",
      "handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [perc_25_colour, perc_50_colour, perc_75_colour, perc_95_colour]]\n",
      "labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
      "plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.80, 0.99))\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/179:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
      "plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.80, 0.99))\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/180:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/181:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "\n",
      "#create legend\n",
      "handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [perc_25_colour, perc_50_colour, perc_75_colour, perc_95_colour]]\n",
      "labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
      "plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.80, 0.99))\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/182:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "import seaborn as sns\n",
      "27/183:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "\n",
      "#create legend\n",
      "handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [perc_25_colour, perc_50_colour, perc_75_colour, perc_95_colour]]\n",
      "labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
      "plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.80, 0.99))\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/184:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(10))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "\n",
      "#create legend\n",
      "handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [perc_25_colour, perc_50_colour, perc_75_colour, perc_95_colour]]\n",
      "labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
      "plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.80, 0.99))\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/185:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(1000))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "\n",
      "#create legend\n",
      "handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [perc_25_colour, perc_50_colour, perc_75_colour, perc_95_colour]]\n",
      "labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
      "plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.80, 0.99))\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/186:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(100))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "\n",
      "#create legend\n",
      "handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [perc_25_colour, perc_50_colour, perc_75_colour, perc_95_colour]]\n",
      "labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
      "plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.80, 0.99))\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/187:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray')\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "\n",
      "#create legend\n",
      "handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [perc_25_colour, perc_50_colour, perc_75_colour, perc_95_colour]]\n",
      "labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
      "plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.80, 0.99))\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/188:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "\n",
      "#create legend\n",
      "handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [perc_25_colour, perc_50_colour, perc_75_colour, perc_95_colour]]\n",
      "labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
      "plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.80, 0.99))\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/189:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=10)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "\n",
      "#create legend\n",
      "handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [perc_25_colour, perc_50_colour, perc_75_colour, perc_95_colour]]\n",
      "labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
      "plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.80, 0.99))\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/190:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=20)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "\n",
      "#create legend\n",
      "handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [perc_25_colour, perc_50_colour, perc_75_colour, perc_95_colour]]\n",
      "labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
      "plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.80, 0.99))\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/191:\n",
      "fig, ax = plt.subplots(figsize=(14,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "\n",
      "#create legend\n",
      "handles = [Rectangle((0,0),1,1,color=c,ec=\"k\") for c in [perc_25_colour, perc_50_colour, perc_75_colour, perc_95_colour]]\n",
      "labels= [\"0-25 Percentile\",\"25-50 Percentile\", \"50-75 Percentile\", \">95 Percentile\"]\n",
      "plt.legend(handles, labels, bbox_to_anchor=(0.5, 0., 0.80, 0.99))\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/192:\n",
      "fig, ax = plt.subplots(figsize=(14,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/193:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/194:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=100)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/195:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=70)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/196:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/197:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/198: bins\n",
      "27/199:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=10)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/200: bins\n",
      "27/201:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=20)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/202: bins\n",
      "27/203:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/204: bins\n",
      "27/205:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=25)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=70)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/206: bins\n",
      "27/207:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=25)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/208:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(descriptions['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=25)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/209:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "27/210:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "27/211:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "27/212:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "import seaborn as sns\n",
      "27/213:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "27/214: changelog.head(3)\n",
      "27/215: issues.head(3)\n",
      "27/216: sprints.head(3)\n",
      "27/217: users.head(3)\n",
      "27/218: changelog.head(3)\n",
      "27/219:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "27/220:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "27/221:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "27/222:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "27/223:\n",
      "# Check the number of rows per selected textual field\n",
      "# keep only the long enoug texts\n",
      "# remove the whitespaces. \n",
      "# remove code snippets \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}\\s', \" \", str(text))  \n",
      "    \n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "\n",
      "    \n",
      "    return text\n",
      "27/224:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "27/225:\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "#log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "27/226:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Epic/Theme', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "27/227:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "        \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "print(user_text_combined.size)\n",
      "print(user_text_combined.head(6))\n",
      "27/228:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "        \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "print(user_text_combined.size)\n",
      "print(user_text_combined.head(3))\n",
      "27/229:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined['text'], facecolor=perc_50_colour, edgecolor='gray', bins=25)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/230:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "user_text_combined['textLength'] = user_text_combined['text'].str.len()\n",
      "27/231:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=25)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/232:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/233: user_text_combined.sort('textLength')\n",
      "27/234: user_text_combined.sort_by('textLength')\n",
      "27/235: user_text_combined.sortby('textLength')\n",
      "27/236: user_text_combined.sort_values('textLength')\n",
      "27/237: user_text_combined.sort_values('textLength', ascending=False)\n",
      "27/238: user_text_combined.sort_values('textLength', ascending=False).head(10)\n",
      "27/239: user_text_combined.drop([500])\n",
      "27/240: user_text_combined.sort_values('textLength', ascending=False).head(10)\n",
      "27/241: user_text_combined = user_text_combined.drop([500])\n",
      "27/242: user_text_combined.sort_values('textLength', ascending=False).head(10)\n",
      "27/243:\n",
      "#remove outlier\n",
      "user_text_combined = user_text_combined.drop([500])\n",
      "27/244:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "        \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "print(user_text_combined.size)\n",
      "print(user_text_combined.head(3))\n",
      "27/245:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "user_text_combined['textLength'] = user_text_combined['text'].str.len()\n",
      "27/246: user_text_combined.sort_values('textLength', ascending=False).head(10)\n",
      "27/247:\n",
      "#remove outlier\n",
      "user_text_combined = user_text_combined.drop([500])\n",
      "27/248:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/249:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/250: user_text_combined.sort_values('textLength', ascending=False).head(20)\n",
      "27/251: user_text_combined.sort_values('textLength', ascending=False).head(30)\n",
      "27/252: user_text_combined.sort_values('textLength', ascending=False).head(20)\n",
      "27/253:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "        \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "print(user_text_combined.size)\n",
      "print(user_text_combined.head(3))\n",
      "27/254:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "user_text_combined['textLength'] = user_text_combined['text'].str.len()\n",
      "27/255: user_text_combined.sort_values('textLength', ascending=False).head(20)\n",
      "27/256:\n",
      "#remove outlier\n",
      "user_text_combined = user_text_combined.drop([500, 499, 302, 120, 496, 498, 386, 76, 63, 301, 3])\n",
      "27/257:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(2))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/258:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(1))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/259: user_text_combined[user_text_combined['textLength']<100].size\n",
      "27/260:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined[user_text_combined['textLength']<1243]['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(1))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/261:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=40)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(1))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/262:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined[user_text_combined['textLength']<932]['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(1))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/263:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined[user_text_combined['textLength']<932]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(1))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/264:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined[user_text_combined['textLength']<50]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(1))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/265:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined[user_text_combined['textLength']<=50]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(1))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/266:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined[user_text_combined['textLength']<=100]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(1))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/267:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined[user_text_combined['textLength']<=100]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/268:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=40)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/269: user_text_combined[user_text_combined['textLength']<932].size\n",
      "27/270:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined[user_text_combined['textLength']<=932]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/271:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined[user_text_combined['textLength']<=1000]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/272:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined[user_text_combined['textLength']<=1000]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=49)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/273:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined[user_text_combined['textLength']<=1000]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/274: user_text_combined.size\n",
      "27/275: user_text_combined.sort_values('textLength', ascending=False).head(115)\n",
      "27/276:\n",
      "cutoff_5percent_top = (int(user_text_combined.size) * 0.05).round(0)\n",
      "print(cutoff_5percent_top)\n",
      "\n",
      "user_text_combined.sort_values('textLength', ascending=False).head()\n",
      "27/277:\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.05), 0)\n",
      "print(cutoff_5percent_top)\n",
      "\n",
      "user_text_combined.sort_values('textLength', ascending=False).head()\n",
      "27/278:\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.05))\n",
      "print(cutoff_5percent_top)\n",
      "\n",
      "user_text_combined.sort_values('textLength', ascending=False).head()\n",
      "27/279:\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.05))\n",
      "print(cutoff_5percent_top)\n",
      "\n",
      "user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top)\n",
      "27/280:\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.05))\n",
      "print(cutoff_5percent_top)\n",
      "\n",
      "user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_5percent_top]\n",
      "27/281:\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.05))\n",
      "print(cutoff_5percent_top)\n",
      "\n",
      "user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1]\n",
      "27/282:\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.05))\n",
      "print(cutoff_5percent_top)\n",
      "\n",
      "user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "27/283:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.05))\n",
      "print('number of top 5% of the text values: ', str(cutoff_5percent_top))\n",
      "\n",
      "\n",
      "user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "27/284:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.05))\n",
      "print('Total number of rows in dataset:', str(user_text_combined.size))\n",
      "print('Number of top 5% of the text values: ', str(cutoff_5percent_top))\n",
      "\n",
      "\n",
      "user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "27/285:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.05))\n",
      "print('Total number of rows in dataset:', str(user_text_combined.size))\n",
      "print('Number of 5% of the rows: ', str(cutoff_5percent_top))\n",
      "\n",
      "top_5_length = user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "top_5_length\n",
      "27/286:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.05))\n",
      "print('Total number of rows in dataset:', str(user_text_combined.size))\n",
      "print('Number of 5% of the rows: ', str(cutoff_5percent_top))\n",
      "\n",
      "top_5_length = user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "top_5_length\n",
      "27/287:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.05))\n",
      "print('Total number of rows in dataset:', str(user_text_combined.size))\n",
      "print('Number of 5% of the rows: ', str(cutoff_5percent_top))\n",
      "\n",
      "value_Longest5 = user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "value_shortest5 = user_text_combined.sort_values('textLength', ascending=True).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/288:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(user_text_combined['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=20)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/289:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.05))\n",
      "print('Total number of rows in dataset:', str(user_text_combined.size))\n",
      "print('Number of 5% of the rows: ', str(cutoff_5percent_top))\n",
      "\n",
      "value_Longest5 = user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "value_shortest5 = user_text_combined.sort_values('textLength', ascending=True).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/290:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.07))\n",
      "cutoff_3percent_top = round((int(user_text_combined.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(user_text_combined.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_5percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_5percent_top))\n",
      "\n",
      "value_Longest5 = user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "value_shortest5 = user_text_combined.sort_values('textLength', ascending=True).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/291:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_5percent_top = round((int(user_text_combined.size) * 0.07))\n",
      "cutoff_3percent_top = round((int(user_text_combined.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(user_text_combined.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_5percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "value_shortest5 = user_text_combined.sort_values('textLength', ascending=True).head(cutoff_5percent_top).iloc[cutoff_5percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/292:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(user_text_combined.size) * 0.07))\n",
      "cutoff_3percent_top = round((int(user_text_combined.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(user_text_combined.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = user_text_combined.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = user_text_combined.sort_values('textLength', ascending=True).head(cutoff_5percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/293: user_text_combined.to_csv('user_text_combined.csv')\n",
      "27/294:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "import seaborn as sns\n",
      "27/295:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "import seaborn as sns\n",
      "27/296:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "27/297:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "27/298:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "27/299:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "27/300:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "27/301:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "27/302:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}\\s', \" \", str(text))  \n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    return text\n",
      "27/303:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "27/304:\n",
      "# remove the rows that contain less than 10 symbols (due to less informative content)\n",
      "#log_cols = log_cols[log_cols['toString'].str.len() >= 10 ]\n",
      "\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "27/305:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "27/306:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "27/307: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "27/308:\n",
      "#remove outlier\n",
      "log_cols = log_cols.drop([7085])\n",
      "27/309:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=20)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/310:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=25)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/311:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/312:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=40)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/313:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/314: log_cols.sort_values('textLength', ascending=False).head(30)\n",
      "27/315: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "27/316:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=20)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/317: log_cols[log_cols['textLength']<932].size\n",
      "27/318: log_cols[log_cols['textLength']<1023].size\n",
      "27/319:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=1023]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/320:\n",
      "print(log_cols[log_cols['textLength']<1023].size)\n",
      "primt(log_cols.size)\n",
      "27/321:\n",
      "print(log_cols[log_cols['textLength']<1023].size)\n",
      "print(log_cols.size)\n",
      "27/322:\n",
      "print(log_cols[log_cols['textLength']<1000].size)\n",
      "print(log_cols.size)\n",
      "27/323:\n",
      "print(log_cols[log_cols['textLength']<1023].size)\n",
      "print(log_cols.size)\n",
      "27/324:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_5percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_5percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/325:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/326:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/327:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.05))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/328:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.01))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/329:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.0001))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/330:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.05))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/331:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.04))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/332:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.03))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/333:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.01))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/334:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.05))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/335:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/336: log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top)\n",
      "27/337:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.1))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/338:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.5))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/339:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.1))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.size))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/340: log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).size\n",
      "27/341:\n",
      "log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).size\n",
      "cutoff_7percent_top\n",
      "27/342:\n",
      "print(log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).size)\n",
      "print(cutoff_7percent_top)\n",
      "27/343:\n",
      "print(log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).shape[0])\n",
      "print(cutoff_7percent_top)\n",
      "27/344:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.size) * 0.1))\n",
      "cutoff_3percent_top = round((int(log_cols.size) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/345:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.1))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/346:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/347:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.1))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/348:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.05))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/349:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.05))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.05))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/350:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print(value_Longest5, value_shortest5)\n",
      "27/351:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length, above which to filter out the rows: ', str(value_Longest5))\n",
      "print('length, below which to filter out the rows: ', str(value_shortest5))\n",
      "27/352:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest5))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest5))\n",
      "27/353:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest5 = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest5 = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest5))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest5))\n",
      "\n",
      "log_cols[log_cols['textLength']<=cutoff_3percent_top].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=cutoff_7percent_top].to_csv('longest_7pct.csv')\n",
      "27/354:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_Longest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_shortest].to_csv('longest_7pct.csv')\n",
      "27/355:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "27/356: re.sub('.\\S{18,}\\s', \" \", str('MessageChannelItemWriter'))\n",
      "27/357: re.sub('\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/358: re.sub('\\S+?(?=\\\\)\\\\S*\\\\S*', \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/359: re.sub('\\S+?(?=\\)\\S*\\S*', \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/360: re.sub('\\S+?(?=\\\\)\\S*\\S*', \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/361: re.sub('\\S+?(?=\\\\)\\\\S*\\\\S*', \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/362: re.sub(\"\\S+?(?=\\\\)\\\\S*\\\\S*\", \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/363: re.sub(\"\\S+?(?=\\)\\S*\\S*\", \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/364: re.sub(\"\\\\\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*\", \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/365: re.sub(\"[\\]S+?(?=[\\])[\\]S*[\\]S*\", \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/366: re.sub(\"[\\\\]S+?(?=[\\\\])[\\\\]S*[\\\\]S*\", \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/367: re.sub(\"[\\\\\\]S+?(?=[\\\\\\])[\\\\\\]S*[\\\\\\]S*\", \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/368: str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\")\n",
      "27/369: re.sub(\"\\\\\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*\", \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/370: re.sub(\"\\[\\\\]S+?(?=\\[\\\\])\\[\\\\]S*\\[\\\\]S*\", \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/371: re.sub(\"\\[\\\\\\]S+?(?=\\[\\\\\\])\\[\\\\\\]S*\\[\\\\\\]S*\", \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/372: re.sub(\"[\\\\]S+?(?=[\\\\])[\\\\]S*[\\\\]S*\", \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/373: re.sub('[\\\\]S+?(?=[\\\\])[\\\\]S*[\\\\]S*', \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/374: re.sub('\\\\\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/375: re.sub(r'\\\\\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/376: re.sub(r'\\\\S+?(?=\\\\)\\\\S*\\\\S*', \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/377: re.sub(r'\\{1,}S+?(?=\\{1,})\\{1,}S*\\{1,}S*', \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/378: re.sub(r'\\\\{1,}S+?(?=\\\\{1,})\\\\{1,}S*\\\\{1,}S*', \" \", str(\"ed by E:\\Git\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/379: re.sub(r'\\\\{1,}S+?(?=\\\\{1,})\\\\{1,}S*\\\\{1,}S*', \" \", str(\"ed by E:\\\\Git\\\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/380: re.sub(r'\\\\S+?(?=\\\\)\\\\S*\\\\S*', \" \", str(\"ed by E:\\\\Git\\\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/381: re.sub(r'\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(\"ed by E:\\\\Git\\\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/382: re.sub(r'\\S+?(?=\\\\)\\\\S*\\\\S*', \" \", str(\"ed by E:\\\\Git\\\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/383: re.sub(r'\\S+?(?=\\\\)\\\\S*\\\\\\S*', \" \", str(\"ed by E:\\\\Git\\\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/384: re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(\"ed by E:\\\\Git\\\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/385: re.sub(r'\\S+?(?=\\\\\\)\\\\\\S*\\\\\\S*', \" \", str(\"ed by E:\\\\Git\\\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/386: re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(\"ed by E:\\\\Git\\\\DNN 291 60   Warning 99 is obsolete: 'This\"))\n",
      "27/387: text = re.sub(\"{(.+?)}\", '', \"   {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}    xd:>runtime m\")\n",
      "27/388: text = re.sub(\"{(.+?)}\", '', \"   {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}    xd:>runtime m\")\n",
      "27/389: text = re.sub(\"{(.+?)}\", '', \"   {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}   a xd:>runtime m\")\n",
      "27/390: text = re.sub(\"{(.+?)}\", '', \"as {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}   a xd:>runtime m\")\n",
      "27/391: text = re.sub(\"{(.+?)}\", '', \"as fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}   a xd:>runtime m\")\n",
      "27/392: text = re.sub(\"{(.+?)}\", '', \"as fixedDelay=1, format=yyyy-MM-dd HH:mm:ss  {count=1, sequence=1}   a xd:>runtime m\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/393: text = re.sub(\"{(.+?)}\", '', \"as fixedDelay=1, format=yyyy-MM-dd HH:mm:ss  count=1, sequence=1}   a xd:>runtime m\")\n",
      "27/394: text = re.sub(\"{(.+?)}\", '', \"as fixedDelay=1, format=yyyy-MM-dd HH:mm:ss  count=1, sequence=1   a xd:>runtime m\")\n",
      "27/395: re.sub(\"{(.+?)}\", '', \"   {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}    xd:>runtime m\")\n",
      "27/396:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}.', \" \", str(text))  \n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    return text\n",
      "27/397: re.sub(\"\\s{2,}\", '', \"      xd:>runtime m\")\n",
      "27/398: re.sub(\"\\s{2,}\", ' ', \"      xd:>runtime m\")\n",
      "27/399:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}.', \" \", str(text))  \n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/400:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "import seaborn as sns\n",
      "27/401:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "27/402: changelog.head(3)\n",
      "27/403: issues.head(3)\n",
      "27/404: sprints.head(3)\n",
      "27/405: users.head(3)\n",
      "27/406: changelog.head(3)\n",
      "27/407:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "27/408:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "27/409:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "27/410:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "27/411:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "27/412:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{18,}.', \" \", str(text))  \n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/413:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "27/414:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "27/415:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "27/416:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "27/417: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "27/418:\n",
      "#remove outlier\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 6974, 2507])\n",
      "27/419: log_cols.to_csv('log_cols.csv')\n",
      "27/420:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=20)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/421:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=10)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/422:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/423:\n",
      "print(log_cols[log_cols['textLength']<=749].size)\n",
      "print(log_cols.size)\n",
      "27/424:\n",
      "cut_val = 749  \n",
      "print(log_cols[log_cols['textLength']<=cut_val].size)\n",
      "print(log_cols.size)\n",
      "27/425:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=50)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/426:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=4)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/427:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/428:\n",
      "#detect the number of rows that fall in top 5 %\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "27/429:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in ALL dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/430:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/431:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/432:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "print(user_text_combined.size)\n",
      "user_text_combined.head(3)\n",
      "27/433: user_text_combined.shape\n",
      "27/434:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "27/435:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "27/436:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=perc_25_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/437:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=perc_75_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/438:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=perc_90_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/439:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=perc_95_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/440:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "perc_25_colour = 'gold'\n",
      "perc_50_colour = 'mediumaquamarine'\n",
      "perc_75_colour = 'deepskyblue'\n",
      "perc_95_colour = 'peachpuff'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=perc_50_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/441:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/442:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/443:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "27/444:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/445:\n",
      "text=\"The customer is using the core module, Member Directory, to display members of the group, NOT leaderboard. The member directory module is not functioning correctly in social 2.0.1. This is the call in 1.3.3: exec sp_executesql N';Exec @0, @1, @2, @3, @4, @5, @6, @7, @8, @9, @10, @11',N'@0 int,@1 int,@2 int,@3 int,@4 int,@5 int,@6 int,@7 int,@8 nvarchar(4000),@9 int,@10 Here it is in 2.0.1: exec sp_executesql N';Exec @0, @1, @2, @3, @4, @5, @6, @7, @8, @9, @10, @11',N'@0 int,@1 int,@2 int,@3 int,@4 int,@5 int,@6 int,@7 int,@8 nvarchar(4000),@9 int,@10 Note that parameter @3 is -1 in the second call, where it should be the group id from the query string (3056). The group ID is in the query string as expected in both versions. Module settings were not changed.\"\n",
      "re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "27/446:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "27/447:\n",
      "text=\"In the mean while, I would like to know if there is any workaround for this issue. I am using with Java 1.8.0_51 on Ubuntu 12.04.5 precise. I added the driver jar to xd/lib from here: stream create foo --definition \"jdbc --fixedDelay=30 --split=1 --query='SELECT Stack trace Test Class FROM fooTable limit 10\"; public static void main(String[] args) throws SQLException, IOException }\"\n",
      "re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "27/448:\n",
      "text=\"In the mean while, I would like to know if there is any workaround for this issue. I am using with Java 1.8.0_51 on Ubuntu 12.04.5 precise. I added the driver jar to xd/lib from here: stream create foo --definition \\\"jdbc --fixedDelay=30 --split=1 --query='SELECT Stack trace Test Class FROM fooTable limit 10\\\"; public static void main(String[] args) throws SQLException, IOException }\"\n",
      "re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "27/449:\n",
      "text = \"am create foo --definition \"jdbc --fixedDelay=30 --split=1 --query='SELECT Stack trace Test Class FROM fooTable limit 10\"; public static void main(String[] args) throws SQLException, IOException }\"\n",
      "text = re.sub(\"--\\S*\", '', str(text))\n",
      "27/450:\n",
      "text = \"am create foo --definition \\\"jdbc --fixedDelay=30 --split=1 --query='SELECT Stack trace Test Class FROM fooTable limit 10\\\"; public static void main(String[] args) throws SQLException, IOException }\"\n",
      "text = re.sub(\"--\\S*\", '', str(text))\n",
      "27/451:\n",
      "text = \"am create foo --definition \\\"jdbc --fixedDelay=30 --split=1 --query='SELECT Stack trace Test Class FROM fooTable limit 10\\\"; public static void main(String[] args) throws SQLException, IOException }\"\n",
      "re.sub(\"--\\S*\", '', str(text))\n",
      "27/452:\n",
      "text = \"CT Stack trace Test Class FROM fooTable limit 10\\\"; public s[tatic void main(Str[in[adas]da]rgs) thr]ow\"\n",
      "re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "27/453:\n",
      "text = \"is issue. I am using with Java 1.8.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "re.sub('\\S+\\.\\S+', '', str(text))\n",
      "27/454:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = text.replace('&nbsp;', ' ')\n",
      "    text = text.replace('sp_executesql', ' ')\n",
      "    text = text.replace('exec', ' ')\n",
      "    \n",
      "\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~\\S*\", '', str(text))\n",
      "    #remove websites and version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    \n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/455:\n",
      "text = \"is issue. I am using with Java 1.8.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/456:\n",
      "text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/457:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "\n",
      "\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/458:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "text\n",
      "\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/459:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "text.encode('utf-8')\n",
      "\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/460:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "\n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "OnlyAscii(text)\n",
      "\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/461:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "text2='asda'\n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "OnlyAscii(text, text2)\n",
      "\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/462:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "text2='asda'\n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "print(OnlyAscii(text), OnlyAscii(text2))\n",
      "\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/463:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "text2='a_-}{~sda'\n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "print(OnlyAscii(text), OnlyAscii(text2))\n",
      "\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/464:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "\n",
      " \n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "#print(OnlyAscii(text), OnlyAscii(text2))\n",
      "\n",
      "OnlyAscii(user_text_combined.text)\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/465:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "\n",
      " \n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "#print(OnlyAscii(text), OnlyAscii(text2))\n",
      "\n",
      "for txt in user_text_combined.text:\n",
      "    print txt\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/466:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "\n",
      " \n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "#print(OnlyAscii(text), OnlyAscii(text2))\n",
      "\n",
      "for txt in user_text_combined.text:\n",
      "    print (txt)\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/467:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "\n",
      " \n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "#print(OnlyAscii(text), OnlyAscii(text2))\n",
      "\n",
      "for txt in user_text_combined.text:\n",
      "    print (txt)a\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/468:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "\n",
      " \n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "#print(OnlyAscii(text), OnlyAscii(text2))\n",
      "i = 0\n",
      "for txt in user_text_combined.text:\n",
      "    if OnlyAscii(txt)==False:\n",
      "        print(i,(txt))\n",
      "    i = i + 1\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/469:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "\n",
      " \n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "#print(OnlyAscii(text), OnlyAscii(text2))\n",
      "i = 0\n",
      "for txt in user_text_combined.text:\n",
      "    if OnlyAscii(txt)==False:\n",
      "        print(i\n",
      "    i = i + 1\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/470:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "\n",
      " \n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "#print(OnlyAscii(text), OnlyAscii(text2))\n",
      "i = 0\n",
      "for txt in user_text_combined.text:\n",
      "    if OnlyAscii(txt)==False:\n",
      "        print(i)\n",
      "    i = i + 1\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/471:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "\n",
      " \n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "#print(OnlyAscii(text), OnlyAscii(text2))\n",
      "i = 0\n",
      "nr_of_nonunicode = 0\n",
      "for txt in user_text_combined.text:\n",
      "    if OnlyAscii(txt)==False:\n",
      "        nr_of_nonunicode = nr_of_nonunicode + 1\n",
      "        #print(i)\n",
      "    i = i + 1\n",
      "print (nr_of_nonunicode)\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/472:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "\n",
      " \n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "#print(OnlyAscii(text), OnlyAscii(text2))\n",
      "i = 0\n",
      "nr_of_nonunicode = 0\n",
      "for txt in user_text_combined.text:\n",
      "    if OnlyAscii(txt.encode('utf-8'))==False:\n",
      "        nr_of_nonunicode = nr_of_nonunicode + 1\n",
      "        #print(i)\n",
      "    i = i + 1\n",
      "print (nr_of_nonunicode)\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/473:\n",
      "#text = \"is issue. I am using with Java 18.0_51 on Ubun.tu 12.04.5 pr.ecise. I added the.\"\n",
      "#re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "\n",
      "text = 'ld be \"EspaÃƒÂ±ol (EspaÃƒÂ±a)\". Si'\n",
      "\n",
      " \n",
      "\n",
      "OnlyAscii = lambda s: re.match('^[\\x00-\\x7F]+$', s) != None\n",
      "#print(OnlyAscii(text), OnlyAscii(text2))\n",
      "i = 0\n",
      "nr_of_nonunicode = 0\n",
      "for txt in user_text_combined.text:\n",
      "    if OnlyAscii(txt)==False:\n",
      "        nr_of_nonunicode = nr_of_nonunicode + 1\n",
      "        #print(i)\n",
      "    i = i + 1\n",
      "print (nr_of_nonunicode)\n",
      "\n",
      "# this should be in the very end\n",
      "# (=>|\\||--|~|\\[|]|{|})\n",
      "27/474: 'asd'.encode('utf-8')\n",
      "27/475: ' help MickaÃƒÂ«l P.S: is thi'.encode('utf-8')\n",
      "27/476:\n",
      "text = ' help MickaÃƒÂ«l P.S: is thi'\n",
      "\n",
      "re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
      "27/477:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = text.replace('&nbsp;', ' ')\n",
      "    text = text.replace('sp_executesql', ' ')\n",
      "    text = text.replace('exec', ' ')\n",
      "    \n",
      "\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace(' at at ', '') \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/478:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "27/479:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "27/480: changelog.head(3)\n",
      "27/481: issues.head(3)\n",
      "27/482: sprints.head(3)\n",
      "27/483: users.head(3)\n",
      "27/484: changelog.head(3)\n",
      "27/485:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "27/486:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "27/487:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "27/488:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "27/489:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "27/490:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = text.replace('&nbsp;', ' ')\n",
      "    text = text.replace('sp_executesql', ' ')\n",
      "    text = text.replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace(' at at ', '') \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/491:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "27/492:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "27/493:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = text.str.replace('&nbsp;', ' ')\n",
      "    text = text.str.replace('sp_executesql', ' ')\n",
      "    text = text.str.replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.str.replace(' at at ', '') \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/494:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "27/495:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = text.replace('&nbsp;', ' ')\n",
      "    text = text.str.replace('sp_executesql', ' ')\n",
      "    text = text.str.replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.str.replace(' at at ', '') \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/496:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "27/497:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = text.replace('&nbsp;', ' ')\n",
      "    text = text.replace('sp_executesql', ' ')\n",
      "    text = text.replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace(' at at ', '') \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/498:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "27/499:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace(' at at ', '') \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/500:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "27/501:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "27/502:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "27/503:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "27/504: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "27/505:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2575, 2507])\n",
      "27/506: log_cols.to_csv('log_cols.csv')\n",
      "27/507:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/508:\n",
      "cut_val = 831  \n",
      "print(log_cols[log_cols['textLength']<=cut_val].size)\n",
      "print(log_cols.size)\n",
      "27/509:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/510:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "27/511:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "27/512:\n",
      "text = \"step ingest-logs in job commit failed; nested exception is commit failed at at at at at at at at at at at at at at at at at at at at at at at at Method) at at at at at at at at at at Source) at at Method) at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at Caused by: co\"\n",
      "text.replace(' at at ', '')\n",
      "27/513:\n",
      "text = \"step ingest-logs in job commit failed; nested exception is commit failed at at at at at at at at at at at at at at at at at at at at at at at at Method) at at at at at at at at at at Source) at at Method) at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at Caused by: co\"\n",
      "text.replace('at at', '')\n",
      "27/514:\n",
      "text = \"step ingest-logs in job commit failed; nested exception is commit failed at at at at at at at at at at at at at at at at at at at at at at at at Method) at at at at at at at at at at Source) at at Method) at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at Caused by: co\"\n",
      "text.replace(' at at', '')\n",
      "27/515:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', '') \n",
      "    \n",
      "    text = re.sub('(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/516:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "27/517:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "27/518: changelog.head(3)\n",
      "27/519: issues.head(3)\n",
      "27/520: sprints.head(3)\n",
      "27/521: users.head(3)\n",
      "27/522: changelog.head(3)\n",
      "27/523:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "27/524:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "27/525:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "27/526:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "27/527:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "27/528:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', '') \n",
      "    #remove the non-textual characters\n",
      "    text = re.sub('(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/529:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "27/530:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', '') \n",
      "    #remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/531:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "27/532:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "27/533:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "27/534:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "27/535: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "27/536:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2507])\n",
      "27/537: log_cols.to_csv('log_cols.csv')\n",
      "27/538:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/539:\n",
      "cut_val = 828\n",
      "print(log_cols[log_cols['textLength']<=cut_val].size)\n",
      "print(log_cols.size)\n",
      "27/540:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "27/541:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "27/542:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "27/543:\n",
      "text = \"e out how solve it. For some reason when we run a step that takes long time (4 hours or more), spring XD is unable to save the metadata and throws the following error. ERROR - Encountered an error uting step ingest-logs in job commit failed; nested exception is commit failed at at at at at at at at at at at at at at at at at at at at at at at at Method) at at at at at at at at at at Source) at at Method) at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at Caused by: commit failed at at at 97 common frames omitted Caused by: unable to commit against JDBC connection at at 99 common frames omitted Ca\"\n",
      "\n",
      "text.replace('at at ', ' ')\n",
      "27/544:\n",
      "text = \"e out how solve it. For some reason when we run a step that takes long time (4 hours or more), spring XD is unable to save the metadata and throws the following error. ERROR - Encountered an error uting step ingest-logs in job commit failed; nested exception is commit failed at at at at at at at at at at at at at at at at at at at at at at at at Method) at at at at at at at at at at Source) at at Method) at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at Caused by: commit failed at at at 97 common frames omitted Caused by: unable to commit against JDBC connection at at 99 common frames omitted Ca\"\n",
      "\n",
      "text.replace(' at at ', ' ')\n",
      "27/545:\n",
      "text = \"e out how solve it. For some reason when we run a step that takes long time (4 hours or more), spring XD is unable to save the metadata and throws the following error. ERROR - Encountered an error uting step ingest-logs in job commit failed; nested exception is commit failed at at at at at at at at at at at at at at at at at at at at at at at at Method) at at at at at at at at at at Source) at at Method) at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at Caused by: commit failed at at at 97 common frames omitted Caused by: unable to commit against JDBC connection at at 99 common frames omitted Ca\"\n",
      "\n",
      "text.replace('at at ', ' ')\n",
      "27/546:\n",
      "text = \"e out how solve it. For some reason when we run a step that takes long time (4 hours or more), spring XD is unable to save the metadata and throws the following error. ERROR - Encountered an error uting step ingest-logs in job commit failed; nested exception is commit failed at at at at at at at at at at at at at at at at at at at at at at at at Method) at at at at at at at at at at Source) at at Method) at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at at Caused by: commit failed at at at 97 common frames omitted Caused by: unable to commit against JDBC connection at at 99 common frames omitted Ca\"\n",
      "\n",
      "text.replace(' at at', ' ')\n",
      "27/547:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "27/548:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "27/549: changelog.head(3)\n",
      "27/550: issues.head(3)\n",
      "27/551: sprints.head(3)\n",
      "27/552: users.head(3)\n",
      "27/553: changelog.head(3)\n",
      "27/554:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "27/555:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "27/556:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "27/557:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "27/558:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "27/559:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace(' at at', ' ') \n",
      "    #remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "27/560:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "31/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "31/2:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "31/3: changelog.head(3)\n",
      "31/4: issues.head(3)\n",
      "31/5: sprints.head(3)\n",
      "31/6: users.head(3)\n",
      "31/7: changelog.head(3)\n",
      "31/8:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "31/9:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "31/10:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "31/11:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "31/12:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "31/13:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace(' at at', ' ') \n",
      "    #remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "31/14:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "31/15:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "31/16:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "31/17:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "31/18: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "31/19:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2507])\n",
      "31/20: log_cols.to_csv('log_cols.csv')\n",
      "31/21:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/22:\n",
      "cut_val = 690\n",
      "print(log_cols[log_cols['textLength']<=cut_val].size)\n",
      "print(log_cols.size)\n",
      "31/23:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/24:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "31/25:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/26:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "31/27:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "31/28:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "31/29:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "31/30:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "31/31:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace(' at at', ' ') \n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "31/32:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "31/33:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "31/34:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "31/35:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "31/36: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "31/37:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2507])\n",
      "31/38: log_cols.to_csv('log_cols.csv')\n",
      "31/39:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/40:\n",
      "cut_val = 690\n",
      "print(log_cols[log_cols['textLength']<=cut_val].size)\n",
      "print(log_cols.size)\n",
      "31/41:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/42:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "31/43:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/44:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "31/45:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "31/46: changelog.head(3)\n",
      "31/47: issues.head(3)\n",
      "31/48: sprints.head(3)\n",
      "31/49: users.head(3)\n",
      "31/50: changelog.head(3)\n",
      "31/51:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "31/52:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "31/53:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "31/54:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "31/55:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "31/56:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "31/57:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "31/58:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "31/59:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "31/60:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "31/61:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "31/62:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "31/63:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "31/64: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "31/65:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2507])\n",
      "31/66: log_cols.to_csv('log_cols.csv')\n",
      "31/67:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/68:\n",
      "cut_val = 690\n",
      "print(log_cols[log_cols['textLength']<=cut_val].size)\n",
      "print(log_cols.size)\n",
      "31/69:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/70:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "31/71:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''        \n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/72:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "31/73:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "31/74: changelog.head(3)\n",
      "31/75: issues.head(3)\n",
      "31/76: sprints.head(3)\n",
      "31/77: users.head(3)\n",
      "31/78: changelog.head(3)\n",
      "31/79:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "31/80:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "31/81:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "31/82:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "31/83:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "31/84:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/)([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/)[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "31/85:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "31/86:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "31/87:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "31/88:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "31/89: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "31/90:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2507])\n",
      "31/91: log_cols.to_csv('log_cols.csv')\n",
      "31/92:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/93:\n",
      "cut_val = 690\n",
      "print(log_cols[log_cols['textLength']<=cut_val].size)\n",
      "print(log_cols.size)\n",
      "31/94:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/95:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "31/96:\n",
      "if '__file__' in vars():\n",
      "    wk_dir = os.path.dirname(os.path.realpath('__file__'))\n",
      "else:\n",
      "    print('We are running the script interactively')\n",
      "\n",
      "with open(join(dirname('__file__'), 'personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "    print(json.dumps(profile, indent=2))\n",
      "31/97:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        \n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/98:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + row['text'].str.length()\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        \n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/99:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + row['text'].str.length()\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        \n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/100:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + row['text'].length()\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        \n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/101:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + row['text'].str.len()\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        \n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/102:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + row['text'].len()\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        \n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/103:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        \n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/104:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "for project in log_cols['project'].unique():\n",
      "    for dev_user in log_cols[log_cols['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        curr_df = log_cols[(log_cols['project']==project) & (log_cols['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        \n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/105: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "31/106:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "31/107:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "31/108: changelog.head(3)\n",
      "31/109: issues.head(3)\n",
      "31/110: sprints.head(3)\n",
      "31/111: users.head(3)\n",
      "31/112: changelog.head(3)\n",
      "31/113:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "31/114:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "31/115:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "31/116:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "31/117:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "31/118:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    #remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    #remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    #remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    #remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    \n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    #remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    #remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    #remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "31/119:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "31/120:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "31/121:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "31/122:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "31/123: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "31/124:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2507])\n",
      "31/125: log_cols.to_csv('log_cols.csv')\n",
      "31/126:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/127:\n",
      "cut_val = 690\n",
      "print(log_cols[log_cols['textLength']<=cut_val].size)\n",
      "print(log_cols.size)\n",
      "31/128:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/129:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "31/130: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "31/131:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        \n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/132:\n",
      "\n",
      "text = 'I am trying to use Presto JDBC driver with Spring-XD JDBC source but I as soon as I deploy the stream I get This is because so the driver seems to work fine. In the mean while, I would like to know if there is any workaround for this issue. I am using with Java on Ubuntu precise. I added the driver jar to xdlib from here: stream create foo ERROR - Could not open JDBC Connection for transaction; nested exception is Disabling auto-commit mode not supported at at at at at at at at Source) at at at at at at at at at at at at at at Caused by: Disabling auto-commit mode not supported at at Method) at at at at at at at Source) at 21 more public class Test. '\n",
      "re.sub('at\\sat\\s', ' ', str(text))\n",
      "31/133:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "31/134:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "31/135: changelog.head(3)\n",
      "31/136: issues.head(3)\n",
      "31/137: sprints.head(3)\n",
      "31/138: users.head(3)\n",
      "31/139: changelog.head(3)\n",
      "31/140:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "31/141:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "31/142:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "31/143:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "31/144:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "31/145:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "31/146:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "31/147:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "31/148:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "31/149:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "31/150: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "31/151:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2507])\n",
      "31/152: log_cols.to_csv('log_cols.csv')\n",
      "31/153:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/154:\n",
      "cut_val = 690\n",
      "print(log_cols[log_cols['textLength']<=cut_val].size)\n",
      "print(log_cols.size)\n",
      "31/155:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/156:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "31/157: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "31/158:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        \n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/159:\n",
      "log_cols['text']\n",
      "\n",
      "\n",
      "#re.sub('at\\sat\\s', ' ', str(text))\n",
      "31/160:\n",
      "log_cols[log_cols['text']=='tezra']\n",
      "\n",
      "\n",
      "#re.sub('at\\sat\\s', ' ', str(text))\n",
      "31/161:\n",
      "log_cols[log_cols['author']=='tezra']\n",
      "\n",
      "\n",
      "#re.sub('at\\sat\\s', ' ', str(text))\n",
      "31/162:\n",
      "log_cols[log_cols['author']=='tezra'].text\n",
      "\n",
      "\n",
      "#re.sub('at\\sat\\s', ' ', str(text))\n",
      "31/163:\n",
      "print(log_cols[log_cols['author']=='tezra'].text)\n",
      "\n",
      "\n",
      "#re.sub('at\\sat\\s', ' ', str(text))\n",
      "31/164:\n",
      "text = log_cols[log_cols['author']=='tezra'].text\n",
      "\n",
      "\n",
      "#re.sub('at\\sat\\s', ' ', str(text))\n",
      "31/165:\n",
      "text = log_cols[log_cols['author']=='tezra'].text\n",
      "\n",
      "\n",
      "re.sub('at\\sat\\s', ' ', str(text))\n",
      "31/166:\n",
      "text = log_cols[log_cols['author']=='tezra'].text\n",
      "\n",
      "\n",
      "print(re.sub('at\\sat\\s', ' ', str(text)))\n",
      "31/167:\n",
      "text = log_cols[log_cols['author']=='tezra'].text\n",
      "\n",
      "\n",
      "print(re.sub('at\\sat\\s', ' ', str(text)))\n",
      "\n",
      "print(x) for x in text\n",
      "31/168:\n",
      "text = log_cols[log_cols['author']=='tezra'].text\n",
      "\n",
      "\n",
      "print(re.sub('at\\sat\\s', ' ', str(text)))\n",
      "\n",
      "print(x for x in text)\n",
      "31/169:\n",
      "text = log_cols[log_cols['author']=='tezra'].text\n",
      "\n",
      "\n",
      "print(re.sub('at\\sat\\s', ' ', str(text)))\n",
      "\n",
      "[print(x) for x in text]\n",
      "31/170:\n",
      "text = log_cols[log_cols['author']=='tezra'].text\n",
      "\n",
      "\n",
      "text=re.sub('at\\sat\\s', ' ', str(text))\n",
      "[print(x) for x in text]\n",
      "31/171:\n",
      "text = log_cols[log_cols['author']=='tezra'].text\n",
      "\n",
      "\n",
      "#text=re.sub('at\\sat\\s', ' ', str(text))\n",
      "[print(x) for x in text]\n",
      "31/172:\n",
      "text = log_cols[log_cols['author']=='tezra'].text\n",
      "\n",
      "\n",
      "text=re.sub('at\\sat\\s', ' ', str(text))\n",
      "[print(x) for x in text]\n",
      "31/173:\n",
      "text = log_cols[log_cols['author']=='tezra'].text\n",
      "\n",
      "\n",
      "#text=re.sub('at\\sat\\s', ' ', str(text))\n",
      "[print(x) for x in text]\n",
      "31/174:\n",
      "\n",
      "\n",
      "\n",
      "#text=re.sub('at\\sat\\s', ' ', str(text))\n",
      "[print(x) for x in log_cols[log_cols['author']=='tezra'].text]\n",
      "31/175:\n",
      "\n",
      "#text=re.sub('at\\sat\\s', ' ', str(text))\n",
      "[print(x) for x in log_cols[log_cols['author']=='tezra'].text.apply(removeCodeSnippet)]\n",
      "31/176:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "31/177:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "31/178: changelog.head(3)\n",
      "31/179: issues.head(3)\n",
      "31/180: sprints.head(3)\n",
      "31/181: users.head(3)\n",
      "31/182: changelog.head(3)\n",
      "31/183:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "31/184:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "31/185:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "31/186:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "31/187:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "31/188:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "31/189:\n",
      "\n",
      "#text=re.sub('at\\sat\\s', ' ', str(text))\n",
      "[print(x) for x in log_cols[log_cols['author']=='tezra'].text.apply(removeCodeSnippet)]\n",
      "31/190:\n",
      "\n",
      "#text=re.sub('at\\sat\\s', ' ', str(text))\n",
      "#[print(x) for x in log_cols[log_cols['author']=='tezra'].text.apply(removeCodeSnippet)]\n",
      "[print(x) for x in log_cols[log_cols['author']=='tezra'].text]\n",
      "31/191:\n",
      "\n",
      "#text=re.sub('at\\sat\\s', ' ', str(text))\n",
      "[print(x) for x in log_cols[log_cols['author']=='tezra'].text.apply(removeCodeSnippet)]\n",
      "[print(x) for x in log_cols[log_cols['author']=='tezra'].text]\n",
      "31/192:\n",
      "\n",
      "#text=re.sub('at\\sat\\s', ' ', str(text))\n",
      "[print(x) for x in log_cols[log_cols['author']=='tezra'].text.apply(removeCodeSnippet).apply(removeCodeSnippet)]\n",
      "[print(x) for x in log_cols[log_cols['author']=='tezra'].text]\n",
      "31/193:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "31/194:\n",
      "\n",
      "#text=re.sub('at\\sat\\s', ' ', str(text))\n",
      "[print(x) for x in log_cols[log_cols['author']=='tezra'].text.apply(removeCodeSnippet)]\n",
      "#[print(x) for x in log_cols[log_cols['author']=='tezra'].text]\n",
      "31/195:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "31/196:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "31/197: changelog.head(3)\n",
      "31/198: issues.head(3)\n",
      "31/199: sprints.head(3)\n",
      "31/200: users.head(3)\n",
      "31/201: changelog.head(3)\n",
      "31/202:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "31/203:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "31/204:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "31/205:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "31/206:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "31/207:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    \n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\)', '', str(text))\n",
      "    \n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "31/208:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "31/209:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "31/210:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "31/211:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/212: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "31/213:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2507])\n",
      "31/214: log_cols.to_csv('log_cols.csv')\n",
      "31/215:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/216:\n",
      "cut_val = 690\n",
      "print(log_cols[log_cols['textLength']<=cut_val].size)\n",
      "print(log_cols.size)\n",
      "31/217:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/218:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "31/219: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "31/220:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        \n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        \n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "\n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "31/221:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.9))\n",
      "cut_val_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print(log_cols[log_cols['textLength']<=cut_val].size)\n",
      "print(log_cols.size)\n",
      "31/222:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.9))\n",
      "cut_val_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print(log_cols[log_cols['textLength']<=cut_val].shape[0])\n",
      "print(log_cols.shape[0])\n",
      "31/223:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.9))\n",
      "cut_val_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "print(cut_val)\n",
      "print(log_cols[log_cols['textLength']<=cut_val].shape[0])\n",
      "print(log_cols.shape[0])\n",
      "31/224:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.9))\n",
      "cut_val_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "print(cut_val_length)\n",
      "print(log_cols[log_cols['textLength']<=cut_val].shape[0])\n",
      "print(log_cols.shape[0])\n",
      "31/225:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.9))\n",
      "cut_val_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print(log_cols[log_cols['textLength']<=cut_val_length].shape[0])\n",
      "print(log_cols.shape[0])\n",
      "31/226:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val_length]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "\n",
      "plt.title('Distribution of text lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/227:\n",
      "cut_val_top = round((int(log_cols.shape[0]) * 0.9))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_top).iloc[cut_val_top-1].textLength\n",
      "\n",
      "cut_val_bottom = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_bottom).iloc[cut_val_bottom-1].textLength\n",
      "\n",
      "print(log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print(log_cols[log_cols['textLength']>=cut_val_top_length].shape[0])\n",
      "print(log_cols[log_cols['textLength']<=cut_val_bottom].shape[0])\n",
      "print(log_cols.shape[0])\n",
      "31/228:\n",
      "cut_val_top = round((int(log_cols.shape[0]) * 0.9))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_top).iloc[cut_val_top-1].textLength\n",
      "\n",
      "cut_val_bottom = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_bottom).iloc[cut_val_bottom-1].textLength\n",
      "\n",
      "print('', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print(log_cols[log_cols['textLength']>=cut_val_top_length].shape[0])\n",
      "print(log_cols[log_cols['textLength']<=cut_val_bottom].shape[0])\n",
      "print(log_cols.shape[0])\n",
      "31/229:\n",
      "cut_val_top = round((int(log_cols.shape[0]) * 0.9))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_top).iloc[cut_val_top-1].textLength\n",
      "\n",
      "cut_val_bottom = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_bottom).iloc[cut_val_bottom-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print(log_cols[log_cols['textLength']>=cut_val_top_length].shape[0])\n",
      "print(log_cols[log_cols['textLength']<=cut_val_bottom].shape[0])\n",
      "print(log_cols.shape[0])\n",
      "31/230:\n",
      "cut_val_top = round((int(log_cols.shape[0]) * 0.9))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_top).iloc[cut_val_top-1].textLength\n",
      "\n",
      "cut_val_bottom = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_bottom).iloc[cut_val_bottom-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0])\n",
      "print(log_cols[log_cols['textLength']<=cut_val_bottom].shape[0])\n",
      "print(log_cols.shape[0])\n",
      "31/231:\n",
      "cut_val_top = round((int(log_cols.shape[0]) * 0.9))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_top).iloc[cut_val_top-1].textLength\n",
      "\n",
      "cut_val_bottom = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_bottom).iloc[cut_val_bottom-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0])\n",
      "print('nr of rows below 10% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom].shape[0])\n",
      "print(log_cols.shape[0])\n",
      "31/232:\n",
      "cut_val_top = round((int(log_cols.shape[0]) * 0.9))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_top).iloc[cut_val_top-1].textLength\n",
      "\n",
      "cut_val_bottom = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_bottom).iloc[cut_val_bottom-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0])\n",
      "print('nr of rows below 10% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "31/233:\n",
      "cut_val_top = round((int(log_cols.shape[0]) * 0.9))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_top).iloc[cut_val_top-1].textLength\n",
      "\n",
      "cut_val_bottom = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_bottom).iloc[cut_val_bottom-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 10% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "31/234:\n",
      "cut_val_top = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val_top).iloc[cut_val_top-1].textLength\n",
      "cut_val_bottom = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val_bottom).iloc[cut_val_bottom-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 10% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "31/235:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 10% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "31/236:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 10% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "31/237:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the longest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/238:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the longest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/239:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the longest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "31/240:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "31/241:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove multiple whitespaces (needed for removing 'at at' texts, regex is the next command below)\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\|#)', ' ', str(text))\n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "31/242:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "31/243:\n",
      "from __future__ import print_function\n",
      "import json\n",
      "from os.path import join, dirname\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import csv\n",
      "import os\n",
      "31/244:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "31/245:\n",
      "if '__file__' in vars():\n",
      "    wk_dir = os.path.dirname(os.path.realpath('__file__'))\n",
      "else:\n",
      "    print('We are running the script interactively')\n",
      "\n",
      "with open(join(dirname('__file__'), 'personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "    print(json.dumps(profile, indent=2))\n",
      "31/246:\n",
      "with open(join(dirname('__file__'), 'personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "    print(json.dumps(profile, indent=2))\n",
      "31/247:\n",
      "with open(join(dirname('__file__'), 'personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "    print(json.dumps(profile, indent=2))\n",
      "31/248:\n",
      "with open(join(dirname('__file__'), 'personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "    print(json.dumps(profile, indent=2))\n",
      "32/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "32/2:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "32/3: changelog.head(3)\n",
      "32/4: issues.head(3)\n",
      "32/5: sprints.head(3)\n",
      "32/6: users.head(3)\n",
      "32/7: changelog.head(3)\n",
      "32/8:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "32/9:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "32/10:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "32/11:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "32/12:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "32/13:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove multiple whitespaces (needed for removing 'at at' texts, regex is the next command below)\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\|#)', ' ', str(text))\n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "32/14:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "32/15:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "32/16:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "32/17:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "32/18: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "32/19:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2507])\n",
      "32/20: log_cols.to_csv('log_cols.csv')\n",
      "32/21:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/22:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 10% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "32/23:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 10% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "32/24:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the longest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/25:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the shortest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/26:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the texts between 10%-90% percentile length in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/27:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "32/28: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "32/29:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "\n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "32/30:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "32/31:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "            words_in_text = len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "32/32:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(user_text_combined['words_in_text'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the shortest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/33:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(user_text_combined['words_in_text'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the shortest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/34:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(re.findall(r'\\w+', row['text']))\n",
      "            words_in_text = len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "32/35:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(user_text_combined['words_in_text'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the shortest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/36:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(user_text_combined['words_in_text'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the shortest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/37: txt=\"Custom conversion is broken. If the custom does not match any of those in then the channel configuration fails with The code needs to consult custom converters to see what (s) they support - and the output Java type. See. Rabbit Bus: Expose on. Related to XD-2567 which fixed this problem, but only in the bus. WARN - skipping header since it is not of expected type , it is. PubSub consumers can support concurrency since the threads are competing consumers on the queue.. For some reason, the Integration is not preventing the standard context from exporting the . This should be suppressed (when an IMBE is present) because it's annotated . Causes . Workaround in the stack overflow answer. Could be an SI issue, but investigation needed. However, we should probably include the stream job name in all MBeans for the stream (as is done for the integration exporter).. 2 Exchanges left behind.. When the is defined as a , the output channel must be set on the handler, not in the annotation. The handler should detect a . See. Also Spring Framework Spring AMQP Also Batch. This deprecated in SI Use in instead. Not that this can safely be done in - the preferred mechanism is available in SI too.. Expose Retryable Exceptions in the Retry Configuration. The uses the Integration Scheduler to run batch jobs Launching batch jobs with the uses the Integration default which only has 10 threads by default. Use the instead to free up the scheduler threads. However, we need to consider whether a separate configuration is needed to limit the number of batch jobs - it's possible the existing bus utor configuration is enough.. LocalMessageBus Will Only Run 10 Jobs by Default. This error surfaced recently as a result of a fix to a bug in which disabled this test in all environments. Now the test has been reactivated it is failing on the branch. The test runs OK on master.. This should probably be changed to: Documentation:. STS Gradle Import Missing Dependencies without Enabling Scala. Thanks to Gary I found this little gem of documentation to be able to use xpath expression in XD. Only hiccup is that I had to also add the to the classpath (otherwise it is missing XPathException class).. Add the Dependencies Required to Use xpath in Streams. Since the message-driven adapter uses a , the default behavior is to lose messages on exceptions (with the DMLC, the message is ack'd before the listener is invoked). In order to provide recovery of such situations, the source needs to expose so it can be set to . Or, perhaps, given that we don't expose complex configuration, the source should use a instead (where the ack is sent after the listener is successfully invoked).. Currently, you cannot suppress (or change) the prefix on user-defined headers. See Add $ property.. See With set with back-whacks, it fails on with set with whacks, it fails with . The StackOverflow failure is similar. works fine.. An additional commit ( for XD-2230 was applied only to master; it needs to be backported to. I believe it is being cause by the following PR: XD-2381: Split MessageBus and Analytics dependencies from DIRT PR: 1307 SHA:. Here is the full exception: Dispatcher has no subscribers for channel nested exception is Dispatcher has no subscribers and here is that stream: filter router Gary Russell: This looks like another (not fixed by the previous fix) timing problem with taps when using singlenode. The tap is started before the tap stream is deployed. But it's not clear to me how the filter module could be deployed bound as a consumer before the Gary Russell: I see the problem: binds the consumer before the producer - this is the wrong order for a passive component such as the filter. cc. When establishing the tap, we create the tap channel and add the WireTap before the tap channel has been bound to the bus. ERROR - Dispatcher has no subscribers for channel nested exception is Dispatcher has no subscribers at. Upgrade to Spring SI SA. Add a Retry Dead Letter Interceptor to the RabbitMQ Source. Provide for retry and or dead-lettering for the rabbit source (similar to the rabbit message bus).. It is documented here But maybe it should also be at the top of the appendix?. The JMS Source Sink has a pluggable provider (default ) but the URL property implies activeMQ - the property name should be generic (found while testing XD-1149).. Add Support for addresses Property on RabbitMQ Source. Documentation for data partitioning, and all Rabbit Bus properties. Remove from tcp Source; Add Binary Support to the http Source. The TCP source converts to String. This prevents binary transfers. Remove the transformer; if the user wants a String; (s)he can use (assuming the byte stream has valid UTF-8 encoding). Another option would be to add a option, but since conversion can already handle it, it's probably better to use that. On the other hand, a option would enable backwards compatibility. The http source also converts to String.. Allows looking at message headers without turning on debugging.. XD-1019 added simple (stateless) retry to the message bus. Use stateful retry and an enabling failed messages to be requeued on the broker until successful (perhaps because another instance can handle the message); also provides a mechanism to route failed messages to a dead-letter exchange. Requires setting the message id header in bus-generated messages. Also add profiles and properties for common retry backoff policies.. If the rabbit source receives a message it can't convert, a is thrown and the message is rejected (and requeued), causing an endless loop. Add an to the inbound adapter to detect and convert to . Also consider adding a retry interceptor to do the same for exceptions in modules (when using local transport).. Update to Spring AMQP. tx-size, concurrency etc.. For some reason, the partitioned batch jobs are storing s in the DB with a null ID. This causes to fail because the HATEOS code asserts not For some reason, the query for objects with the also returned these two objects with null keys. Blowing away my data directory fixes the problem (until I run another partitioned batch job). I need to figure out why spring batch is creating these bad records, but we should probably add some defensive code to protect against null IDs. You can reproduce by building against my XD-1146 branch.. Migrate to SI Redis Queue and Topic Adapters. When INT-3133 is resolved, SpEL s are inherited from parent contexts. Instead of adding the to each module's context, add it to the parent instead.. Move SpEL to Module Parent Context. and . references and is referenced by (and stopped).. Taps are currently source modules. They could be refactored to simply bridge the tapped module's tap pub sub topic directly (with conversion) to the first tap module's input channel. Note - ensure destroy works. Currently the tap is destroyed by the simple fact it is a module; if it's no longer a module we'll need special handling to stop remove the tap adapter.. Factor out common Redis Rabbit code. Also, factor out common inbound tap code (very similar). Change transport nternals to Use instead of and .. Revert XD-624 When SI is Available. When importing Spring-XD as a gradle project, in STS, while building the model, we get Root exception: Project location doesn't exist: . gradlew eclipse creates these directories, but the plugin needs them before running that task The problem seems to be that these \"projects\" are not really projects. Perhaps a quick fix would be to commit these directories (with a dummy file) ??. Epic For Distributed Monitoring Stories. results in the following stack trace in the DEBUG log. It's apparently benign, but. Parameter parsing does not work if an argument contains ' For example: Also, I was surprised that this but this I think we need to tokenize the argument (with ' if contains spaces) and remove any surrounding from the result. This means if someone wants a SpEL literal they would have to use something like resulting in a SpEL literal 'Hello, world!'. The calls before the context has had its attached. This can cause issues with s with placeholders in constructor args because the unresolved placeholder is used when the is to determine the type of object it will serve up.. Add JUnit so Tests Fail Fast with Clear Messaging if Redis Not Available. \"\n",
      "32/38: txt='Custom conversion is broken. If the custom does not match any of those in then the channel configuration fails with The code needs to consult custom converters to see what (s) they support - and the output Java type. See. Rabbit Bus: Expose on. Related to XD-2567 which fixed this problem, but only in the bus. WARN - skipping header since it is not of expected type , it is. PubSub consumers can support concurrency since the threads are competing consumers on the queue.. For some reason, the Integration is not preventing the standard context from exporting the . This should be suppressed (when an IMBE is present) because it's annotated . Causes . Workaround in the stack overflow answer. Could be an SI issue, but investigation needed. However, we should probably include the stream job name in all MBeans for the stream (as is done for the integration exporter).. 2 Exchanges left behind.. When the is defined as a , the output channel must be set on the handler, not in the annotation. The handler should detect a . See. Also Spring Framework Spring AMQP Also Batch. This deprecated in SI Use in instead. Not that this can safely be done in - the preferred mechanism is available in SI too.. Expose Retryable Exceptions in the Retry Configuration. The uses the Integration Scheduler to run batch jobs Launching batch jobs with the uses the Integration default which only has 10 threads by default. Use the instead to free up the scheduler threads. However, we need to consider whether a separate configuration is needed to limit the number of batch jobs - it's possible the existing bus utor configuration is enough.. LocalMessageBus Will Only Run 10 Jobs by Default. This error surfaced recently as a result of a fix to a bug in which disabled this test in all environments. Now the test has been reactivated it is failing on the branch. The test runs OK on master.. This should probably be changed to: Documentation:. STS Gradle Import Missing Dependencies without Enabling Scala. Thanks to Gary I found this little gem of documentation to be able to use xpath expression in XD. Only hiccup is that I had to also add the to the classpath (otherwise it is missing XPathException class).. Add the Dependencies Required to Use xpath in Streams. Since the message-driven adapter uses a , the default behavior is to lose messages on exceptions (with the DMLC, the message is ack'd before the listener is invoked). In order to provide recovery of such situations, the source needs to expose so it can be set to . Or, perhaps, given that we don't expose complex configuration, the source should use a instead (where the ack is sent after the listener is successfully invoked).. Currently, you cannot suppress (or change) the prefix on user-defined headers. See Add $ property.. See With set with back-whacks, it fails on with set with whacks, it fails with . The StackOverflow failure is similar. works fine.. An additional commit ( for XD-2230 was applied only to master; it needs to be backported to. I believe it is being cause by the following PR: XD-2381: Split MessageBus and Analytics dependencies from DIRT PR: 1307 SHA:. Here is the full exception: Dispatcher has no subscribers for channel nested exception is Dispatcher has no subscribers and here is that stream: filter router Gary Russell: This looks like another (not fixed by the previous fix) timing problem with taps when using singlenode. The tap is started before the tap stream is deployed. But it's not clear to me how the filter module could be deployed bound as a consumer before the Gary Russell: I see the problem: binds the consumer before the producer - this is the wrong order for a passive component such as the filter. cc. When establishing the tap, we create the tap channel and add the WireTap before the tap channel has been bound to the bus. ERROR - Dispatcher has no subscribers for channel nested exception is Dispatcher has no subscribers at. Upgrade to Spring SI SA. Add a Retry Dead Letter Interceptor to the RabbitMQ Source. Provide for retry and or dead-lettering for the rabbit source (similar to the rabbit message bus).. It is documented here But maybe it should also be at the top of the appendix?. The JMS Source Sink has a pluggable provider (default ) but the URL property implies activeMQ - the property name should be generic (found while testing XD-1149).. Add Support for addresses Property on RabbitMQ Source. Documentation for data partitioning, and all Rabbit Bus properties. Remove from tcp Source; Add Binary Support to the http Source. The TCP source converts to String. This prevents binary transfers. Remove the transformer; if the user wants a String; (s)he can use (assuming the byte stream has valid UTF-8 encoding). Another option would be to add a option, but since conversion can already handle it, it's probably better to use that. On the other hand, a option would enable backwards compatibility. The http source also converts to String.. Allows looking at message headers without turning on debugging.. XD-1019 added simple (stateless) retry to the message bus. Use stateful retry and an enabling failed messages to be requeued on the broker until successful (perhaps because another instance can handle the message); also provides a mechanism to route failed messages to a dead-letter exchange. Requires setting the message id header in bus-generated messages. Also add profiles and properties for common retry backoff policies.. If the rabbit source receives a message it can't convert, a is thrown and the message is rejected (and requeued), causing an endless loop. Add an to the inbound adapter to detect and convert to . Also consider adding a retry interceptor to do the same for exceptions in modules (when using local transport).. Update to Spring AMQP. tx-size, concurrency etc.. For some reason, the partitioned batch jobs are storing s in the DB with a null ID. This causes to fail because the HATEOS code asserts not For some reason, the query for objects with the also returned these two objects with null keys. Blowing away my data directory fixes the problem (until I run another partitioned batch job). I need to figure out why spring batch is creating these bad records, but we should probably add some defensive code to protect against null IDs. You can reproduce by building against my XD-1146 branch.. Migrate to SI Redis Queue and Topic Adapters. When INT-3133 is resolved, SpEL s are inherited from parent contexts. Instead of adding the to each module's context, add it to the parent instead.. Move SpEL to Module Parent Context. and . references and is referenced by (and stopped).. Taps are currently source modules. They could be refactored to simply bridge the tapped module's tap pub sub topic directly (with conversion) to the first tap module's input channel. Note - ensure destroy works. Currently the tap is destroyed by the simple fact it is a module; if it's no longer a module we'll need special handling to stop remove the tap adapter.. Factor out common Redis Rabbit code. Also, factor out common inbound tap code (very similar). Change transport nternals to Use instead of and .. Revert XD-624 When SI is Available. When importing Spring-XD as a gradle project, in STS, while building the model, we get Root exception: Project location doesn't exist: . gradlew eclipse creates these directories, but the plugin needs them before running that task The problem seems to be that these \"projects\" are not really projects. Perhaps a quick fix would be to commit these directories (with a dummy file) ??. Epic For Distributed Monitoring Stories. results in the following stack trace in the DEBUG log. It's apparently benign, but. Parameter parsing does not work if an argument contains ' For example: Also, I was surprised that this but this I think we need to tokenize the argument (with ' if contains spaces) and remove any surrounding from the result. This means if someone wants a SpEL literal they would have to use something like resulting in a SpEL literal 'Hello, world!'. The calls before the context has had its attached. This can cause issues with s with placeholders in constructor args because the unresolved placeholder is used when the is to determine the type of object it will serve up.. Add JUnit so Tests Fail Fast with Clear Messaging if Redis Not Available. '\n",
      "32/39: txt='Add SSL and attachments to Mail sink module. see XD-2076 & XD-2498.. An error message occurs about the. '\n",
      "32/40: len(re.findall(r'\\w+', txt)\n",
      "32/41: txt\n",
      "32/42: len(re.findall(r'\\w+', txt)\n",
      "32/43: len(re.findall(r'\\w+', txt))\n",
      "32/44:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(re.findall(r'\\w+', row['text']))\n",
      "            words_in_text = words_in_text + len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "32/45:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(re.findall(r'\\w+', row['text']))\n",
      "            words_in_text = words_in_text + len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "32/46:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(user_text_combined['words_in_text'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the shortest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/47:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'purple'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the shortest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/48:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'liteblue'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the shortest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/49:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'lightblue'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the shortest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/50:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'pink'\n",
      "counts, bins, patches = ax.hist(log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the texts between 10%-90% percentile length in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/51:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "32/52:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "32/53: changelog.head(3)\n",
      "32/54: issues.head(3)\n",
      "32/55: sprints.head(3)\n",
      "32/56: users.head(3)\n",
      "32/57: changelog.head(3)\n",
      "32/58:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "32/59:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "32/60:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "32/61:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "32/62:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "32/63:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove multiple whitespaces (needed for removing 'at at' texts, regex is the next command below)\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\|#)', ' ', str(text))\n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "32/64:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "32/65:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "32/66:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "32/67:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "32/68: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "32/69:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2507])\n",
      "32/70: log_cols.to_csv('log_cols.csv')\n",
      "32/71:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/72:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 10% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "32/73:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the longest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/74:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'lightblue'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the shortest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/75:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'pink'\n",
      "counts, bins, patches = ax.hist(log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the texts between 10%-90% percentile length in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/76:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "32/77: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "32/78:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(re.findall(r'\\w+', row['text']))\n",
      "            words_in_text = words_in_text + len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "32/79:\n",
      "from __future__ import print_function\n",
      "import json\n",
      "from os.path import join, dirname\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import csv\n",
      "import os\n",
      "32/80:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "\n",
      "'''\n",
      "another account credentials:\n",
      "BFsOsCFLvQgZHLicoAj_ywnJ_92h0ry0gTgKudmuDmjm\n",
      "https://gateway-lon.watsonplatform.net/personality-insights/api\n",
      "'''\n",
      "32/81:\n",
      "with open(join(dirname('__file__'), 'personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "#print(json.dumps(profile, indent=2))\n",
      "32/82:\n",
      "'''\n",
      "# filter changelog with the textual fields\n",
      "time_log_filtered = changelog[changelog['field'].isin(['timeestimate', 'timespent', 'Time Spent', 'timeoriginalestimate'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'to', 'toString']\n",
      "time_log_cols = time_log_filtered[cols]\n",
      "\n",
      "time_log_cols.field.unique()\n",
      "\n",
      "\n",
      "\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "time_log_grouped = time_log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "time_log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "time_latest_logs_from_issues = pd.merge(time_log_cols, time_log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(time_log_cols.shape[0], time_log_grouped.shape[0], time_latest_logs_from_issues.shape[0]) \n",
      "time_latest_logs_from_issues.head(6)\n",
      "time_log_cols = time_latest_logs_from_issues\n",
      "\n",
      "#Let's find out, how many of the issues have both, estimated time and actual time spent\n",
      "time_log_cols = time_log_cols[(time_log_cols['toString'] != '0') & (pd.isnull(time_log_cols['toString']) == False) ].sort_values('key')\n",
      "time_log_cols_gr = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "ime_log_cols_gr_ds = time_log_cols_gr.reset_index(level= [0, 1, 2], inplace=False)\n",
      "ime_log_cols_gr_ds[ime_log_cols_gr_ds['created']>1].shape[0]\n",
      "\n",
      "# TO-DO: \n",
      "# 1) Calculate actual completion time by: Dataset Issues, column 'created', column 'resolutiondate' when 'resolution.name'=='Complete'\n",
      "# 2) Calculate the story points for the issues calculated above.\n",
      "# 3) \n",
      "'''\n",
      "32/83:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "            words_in_text = words_in_text + len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "32/84:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "counts, bins, patches = ax.hist(log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)]['textLength'], \n",
      "                                 edgecolor='gray', bins=30)\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('word count in text', fontsize=15)\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "plt.show()\n",
      "32/85:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "counts, bins, patches = ax.hist(user_text_combined['words_in_text'], \n",
      "                                 edgecolor='gray', bins=30)\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('word count in text', fontsize=15)\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "plt.show()\n",
      "32/86:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "counts, bins, patches = ax.hist(user_text_combined['words_in_text'], color = 'yellow',edgecolor='gray', bins=30)\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('word count in text', fontsize=15)\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "plt.show()\n",
      "32/87:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "counts, bins, patches = ax.hist(user_text_combined['words_in_text'],color='gold',edgecolor='gray', bins=30)\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('word count in text', fontsize=15)\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "plt.show()\n",
      "32/88:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "32/89:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "32/90: changelog.head(3)\n",
      "32/91: issues.head(3)\n",
      "32/92: sprints.head(3)\n",
      "32/93: users.head(3)\n",
      "32/94: changelog.head(3)\n",
      "32/95:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "32/96:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "32/97:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "32/98:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "32/99:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "32/100:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove multiple whitespaces (needed for removing 'at at' texts, regex is the next command below)\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\|#)', ' ', str(text))\n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "32/101:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "32/102:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "32/103:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "32/104:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "32/105: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "32/106:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2507])\n",
      "32/107: log_cols.to_csv('log_cols.csv')\n",
      "32/108:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'gold'\n",
      "counts, bins, patches = ax.hist(log_cols['textLength'], facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of text lengths in the whole dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/109:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 10% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "32/110:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'mediumaquamarine'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the longest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/111:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'lightblue'\n",
      "counts, bins, patches = ax.hist(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the shortest 10%  texts lengths in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/112:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "_colour = 'pink'\n",
      "counts, bins, patches = ax.hist(log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)]['textLength'], \n",
      "                                facecolor=_colour, edgecolor='gray', bins=30)\n",
      "# Set the ticks to be at the edges of the bins.\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.title('Distribution of the texts between 10%-90% percentile length in the dataset', fontsize=20)\n",
      "plt.ylabel('Count', fontsize=15)\n",
      "plt.xlabel('Text Length', fontsize=15)\n",
      "# Calculate bar centre to display the count of data points and %\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "# Display the the count of data points and % for each bar in histogram\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "# Display the graph\n",
      "plt.show()\n",
      "32/113:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.07))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 7% of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 3% of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "32/114: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "32/115:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "            words_in_text = words_in_text + len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "32/116:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "counts, bins, patches = ax.hist(user_text_combined['words_in_text'],color='gold',edgecolor='gray', bins=30)\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.ylabel('Count of such occurences', fontsize=15)\n",
      "plt.xlabel('word count in text', fontsize=15)\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "plt.show()\n",
      "32/117:\n",
      "from __future__ import print_function\n",
      "import json\n",
      "from os.path import join, dirname\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import csv\n",
      "import os\n",
      "32/118:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "\n",
      "'''\n",
      "another account credentials:\n",
      "BFsOsCFLvQgZHLicoAj_ywnJ_92h0ry0gTgKudmuDmjm\n",
      "https://gateway-lon.watsonplatform.net/personality-insights/api\n",
      "'''\n",
      "32/119:\n",
      "with open(join(dirname('__file__'), 'personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "#print(json.dumps(profile, indent=2))\n",
      "32/120:\n",
      "'''\n",
      "# filter changelog with the textual fields\n",
      "time_log_filtered = changelog[changelog['field'].isin(['timeestimate', 'timespent', 'Time Spent', 'timeoriginalestimate'])]\n",
      "\n",
      "# take only necessary columns \n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'to', 'toString']\n",
      "time_log_cols = time_log_filtered[cols]\n",
      "\n",
      "time_log_cols.field.unique()\n",
      "\n",
      "\n",
      "\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "        ###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "time_log_grouped = time_log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "time_log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "time_latest_logs_from_issues = pd.merge(time_log_cols, time_log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(time_log_cols.shape[0], time_log_grouped.shape[0], time_latest_logs_from_issues.shape[0]) \n",
      "time_latest_logs_from_issues.head(6)\n",
      "time_log_cols = time_latest_logs_from_issues\n",
      "\n",
      "#Let's find out, how many of the issues have both, estimated time and actual time spent\n",
      "time_log_cols = time_log_cols[(time_log_cols['toString'] != '0') & (pd.isnull(time_log_cols['toString']) == False) ].sort_values('key')\n",
      "time_log_cols_gr = time_log_cols.groupby(['key', 'project', 'author']).agg({'created': 'count'})\n",
      "ime_log_cols_gr_ds = time_log_cols_gr.reset_index(level= [0, 1, 2], inplace=False)\n",
      "ime_log_cols_gr_ds[ime_log_cols_gr_ds['created']>1].shape[0]\n",
      "\n",
      "# TO-DO: \n",
      "# 1) Calculate actual completion time by: Dataset Issues, column 'created', column 'resolutiondate' when 'resolution.name'=='Complete'\n",
      "# 2) Calculate the story points for the issues calculated above.\n",
      "# 3) \n",
      "'''\n",
      "32/121:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "32/122: hist_with_perc(log_cols['textLength'],30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "32/123:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "32/124: hist_with_perc(log_cols['textLength'],30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "32/125: Plot the distribution of text lengths.\n",
      "32/126:\n",
      "hist_with_perc(log_cols['textLength'],\n",
      "               30,'Distribution of the longest 10%  texts lengths in the dataset','text length','Count','mediumaquamarine')\n",
      "32/127:\n",
      "hist_with_perc(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],\n",
      "               30,'Distribution of the longest 10%  texts lengths in the dataset','text length','Count','mediumaquamarine')\n",
      "32/128:\n",
      "hist_with_perc(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],\n",
      "               30,'Distribution of the shortest 10%  texts lengths in the dataset','text length','Count','mediumaquamarine')\n",
      "32/129:\n",
      "hist_with_perc(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],\n",
      "               30,'Distribution of the texts between 10%-90% percentile length in the dataset','text length','Count','mediumaquamarine')\n",
      "32/130:\n",
      "\n",
      "hist_with_perc(log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)]['textLength'],\n",
      "               30,'Distribution of the texts between 10%-90% percentile length in the dataset','text length','Count','mediumaquamarine')\n",
      "32/131:\n",
      "\n",
      "hist_with_perc(log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,\n",
      "               30,'Distribution of the texts between 10%-90% percentile length in the dataset',\n",
      "               'text length','Count','mediumaquamarine')\n",
      "32/132:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "counts, bins, patches = ax.hist(user_text_combined['words_in_text'],color='gold',edgecolor='gray', bins=30)\n",
      "ax.set_xticks(bins.round(0))\n",
      "plt.xticks(rotation=45)\n",
      "plt.ylabel('Count of such occurences', fontsize=15)\n",
      "plt.xlabel('word count in text', fontsize=15)\n",
      "bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "for i in range(len(bins)-1):\n",
      "    bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "plt.show()\n",
      "\n",
      "\n",
      "hist_with_perc(user_text_combined['words_in_text'],\n",
      "               30,'','word count in text','Count of such occurences','mediumaquamarine')\n",
      "32/133:\n",
      "hist_with_perc(user_text_combined['words_in_text'],\n",
      "               30,'','word count in text','Count of such occurences','red')\n",
      "32/134:\n",
      "hist_with_perc(user_text_combined['words_in_text'],\n",
      "               30,'','word count in text','Count of such occurences','yellow')\n",
      "32/135:\n",
      "hist_with_perc(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],\n",
      "               30,'Distribution of the longest 10%  texts lengths in the dataset','text length','Count','littleblue')\n",
      "32/136:\n",
      "hist_with_perc(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],\n",
      "               30,'Distribution of the longest 10%  texts lengths in the dataset','text length','Count','lightblue')\n",
      "32/137:\n",
      "hist_with_perc(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],\n",
      "               30,'Distribution of the shortest 10%  texts lengths in the dataset','text length','Count','aquamarine')\n",
      "32/138: users.head()\n",
      "32/139: user_text_combined.head()\n",
      "32/140:\n",
      "pd.merge(user_text_combined, users, how = 'inner',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])\n",
      "32/141: user_text_combined.shape[0]\n",
      "32/142: user_text_combined['project','user'].unique()\n",
      "32/143: user_text_combined.groupby(['project','user'])\n",
      "32/144: user_text_combined.groupby(['project','user']).count\n",
      "32/145: user_text_combined.groupby(['project','user']).count()\n",
      "32/146: users.groupby(['project','name']).count()\n",
      "32/147:\n",
      "users.groupby(['project','name']).count()\n",
      "\n",
      "users.sort_values(['project', 'name'], ascending=True)\n",
      "32/148: users.head\n",
      "32/149: users.head()\n",
      "32/150: users['displayName','emailAddress','name','project'].drop_duplicates()\n",
      "32/151: users['displayName','emailAddress','name','project'].dropduplicates()\n",
      "32/152: users['displayName','emailAddress','name','project']\n",
      "32/153: users[['displayName','emailAddress','name','project']]\n",
      "32/154: users[['displayName','emailAddress','name','project']].drop_suplicates()\n",
      "32/155: users[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "32/156:\n",
      "pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'inner',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])\n",
      "32/157: user_text_combined.shape\n",
      "32/158:\n",
      "pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])\n",
      "32/159:\n",
      "pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name']).sort_values(['project','user'])\n",
      "32/160:\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])['project','user','text','count_of_texts','words_in_text','text_length','emailAddress']\n",
      "32/161:\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','text_length','emailAddress']]\n",
      "32/162:\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/163: user_texts_emails\n",
      "32/164: user_texts_emails[user_texts_emails['emailAddress'] == None]\n",
      "32/165: user_texts_emails[user_texts_emails['emailAddress'] == np.nan]\n",
      "32/166: user_texts_emails.sort_values('emailAddress', ascending=True)\n",
      "32/167: users[users.project=='nexus']\n",
      "32/168: users[users.project=='nexus'].sort_values('name', ascending=True)\n",
      "32/169:\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName',combine_first('emailAddress', ' '),'name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/170:\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName',pd.combine_first('emailAddress', ' '),'name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/171:\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName','emailAddress'.combine_first(' '),'name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/172:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailAddress'] =users_df['emailAddress'].combine_first(' ') \n",
      "'''\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "'''\n",
      "32/173:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailAddress'] =users_df['emailAddress'].combine_first([]) \n",
      "\n",
      "'''\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "'''\n",
      "32/174:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailAddress'] =users_df['emailAddress'].combine_first(['']) \n",
      "\n",
      "'''\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "'''\n",
      "32/175:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailAddress'] =users_df['emailAddress'].combine_first(users_df['name']) \n",
      "\n",
      "'''\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "'''\n",
      "32/176:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailAddress'].combine_first(users_df['name']) \n",
      "\n",
      "'''\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "'''\n",
      "32/177:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailAddress'].combine_first(users_df['name'])\n",
      "32/178:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailAddress'].combine_first(users_df['name']).sort_values('name')\n",
      "32/179:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailAddress'].combine_first(users_df['name']).sort_values('emailAddress')\n",
      "32/180:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailOrName'] = users_df['emailAddress'].combine_first(users_df['name']).sort_values('name')\n",
      "users_df[['displayName','emailOrName','name','project']].drop_duplicates()\n",
      "\n",
      "'''\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "'''\n",
      "32/181:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailOrName'] = users_df['emailAddress'].combine_first(users_df['name'])\n",
      "users_df[['displayName','emailOrName','name','project']].drop_duplicates()\n",
      "\n",
      "'''\n",
      "user_texts_emails = pd.merge(user_text_combined, users[['displayName','emailAddress','name','project']].drop_duplicates(), how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "'''\n",
      "32/182:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailOrName'] = users_df['emailAddress'].combine_first(users_df['name'])\n",
      "users_df[['displayName','emailOrName','name','project']].drop_duplicates()\n",
      "32/183:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailOrName'] = users_df['emailAddress'].combine_first(users_df['name'])\n",
      "users_df = users_df[['displayName','emailOrName','name','project']].drop_duplicates()\n",
      "\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/184:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailOrName'] = users_df['emailAddress'].combine_first(users_df['name'])\n",
      "users_df = users_df[['displayName','emailOrName','name','project']].drop_duplicates()\n",
      "\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailOrName']]\n",
      "32/185: user_texts_emails.sort_values('emailAddress', ascending=True)\n",
      "32/186: user_texts_emails.sort_values('emailOrName', ascending=True)\n",
      "32/187:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/188: user_texts_emails.sort_values('emailAddress', ascending=True)\n",
      "32/189:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df['emailOrName'] = users_df['emailAddress'].combine_first(users_df['name'])\n",
      "users_df = users_df[['displayName','emailOrName','name','project']].drop_duplicates()\n",
      "\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'inner',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailOrName']]\n",
      "32/190: user_texts_emails.sort_values('emailAddress', ascending=True)\n",
      "32/191: user_texts_emails.sort_values('emailOrName', ascending=True)\n",
      "32/192: users_df\n",
      "32/193: users_df[users_df.emailAddress=Nan]\n",
      "32/194: users_df[users_df.emailAddress==Nan]\n",
      "32/195: users_df[users_df.emailAddress=='Nan']\n",
      "32/196: users_df[users_df.emailAddress==np.Nan]\n",
      "32/197: users_df[users_df.emailAddress==np.nan]\n",
      "32/198: users_df[users_df.emailOrName==np.nan]\n",
      "32/199: users_df[pd.isnull(users_df.emailOrName)==True]\n",
      "32/200: users_df[pd.isnull(users_df.emailOrName)==False]\n",
      "32/201: users_df[pd.isnull(users_df.emailOrName)==True]\n",
      "32/202: users_df[pd.isnull(users_df.name)==True]\n",
      "32/203: users_df[pd.isnull(users_df.displayName)==True]\n",
      "32/204:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df[(pd.isnull(user_df.name)==False)(pd.isnull(user_df.emailAddress)==True)].emailAddress = '-'\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'inner',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/205:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df[(pd.isnull(user_df.name)==False)&(pd.isnull(user_df.emailAddress)==True)].emailAddress = '-'\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'inner',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/206:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df[(pd.isnull(user_df.name)==False & pd.isnull(user_df.emailAddress)==True)].emailAddress = '-'\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'inner',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/207:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df[(pd.isnull(user_df['name'])==False)&(pd.isnull(user_df['emailAddress'])==True)]['emailAddress'] = '-'\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "32/208: users_df[pd.isnull(users_df.displayName)==True]\n",
      "32/209:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df[(pd.isnull(user_df['name'])==False)&(pd.isnull(users_df['emailAddress'])==True)]['emailAddress'] = '-'\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'inner',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/210:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df[(pd.isnull(users_df['name'])==False)&(pd.isnull(users_df['emailAddress'])==True)]['emailAddress'] = '-'\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'inner',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/211: user_texts_emails.sort_values('emailAddress', ascending=True)\n",
      "32/212: users_df[pd.isnull(users_df.displayName)==True]\n",
      "32/213: users_df[pd.isnull(users_df.emailAddress)==True]\n",
      "32/214: users_df[pd.isnull(users_df.emailAddress)==True]['emailAddress']='-'\n",
      "32/215: users_df[pd.isnull(users_df.emailAddress)==True]\n",
      "32/216: users_df.loc(users_df[pd.isnull(users_df.emailAddress)==True], ['emailAddress'])\n",
      "32/217: users_df(users_df[pd.isnull(users_df.emailAddress)==True]['emailAddress']='-'\n",
      "32/218: users_df[pd.isnull(users_df.emailAddress)==True]['emailAddress']='-'\n",
      "32/219: users_df.loc(users_df[pd.isnull(users_df.emailAddress)==True], ['emailAddress'])\n",
      "32/220: users_df.loc(pd.isnull(users_df.emailAddress)==True, ['emailAddress'])\n",
      "32/221: users_df.loc(pd.isnull(users_df.emailAddress)==True, 'emailAddress')\n",
      "32/222: users_df.loc(users_df[pd.isnull(users_df.emailAddress)==True], 'emailAddress')\n",
      "32/223: users_df.loc(users_df[pd.isnull(users_df.emailAddress)==True], users_df['emailAddress'])\n",
      "32/224: users_df.loc[users_df[pd.isnull(users_df.emailAddress)==True], users_df['emailAddress']]\n",
      "32/225: users_df.loc[users_df[pd.isnull(users_df.emailAddress)==True], 'emailAddress']\n",
      "32/226: users_df.loc[pd.isnull(users_df.emailAddress)==True, 'emailAddress']\n",
      "32/227: users_df.loc[pd.isnull(users_df.emailAddress)==True, 'emailAddress']='-'\n",
      "32/228: users_df[pd.isnull(users_df.emailAddress)==True]\n",
      "32/229:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df.loc[pd.isnull(pd.isnull(users_df['name'])==False)&(pd.isnull(users_df['emailAddress'])==True), 'emailAddress']='-'\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'inner',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/230: user_texts_emails.sort_values('emailAddress', ascending=True)\n",
      "32/231: users_df.loc[pd.isnull(pd.isnull(users_df['name'])==False)&(pd.isnull(users_df['emailAddress'])==True), 'emailAddress']='-'\n",
      "32/232: users_df[pd.isnull(users_df.emailAddress)==True]\n",
      "32/233: users_df.loc[pd.isnull(pd.isnull(users_df['name'])==False)&(pd.isnull(users_df['emailAddress'])==True), 'emailAddress']\n",
      "32/234: users_df.loc[pd.isnull(pd.isnull(users_df['name'])==False)&(pd.isnull(users_df['emailAddress'])==True), 'emailAddress']\n",
      "32/235: users_df.loc[pd.isnull(pd.isnull(users_df['name'])==False)&pd.isnull(users_df['emailAddress'])==True, 'emailAddress']\n",
      "32/236: users_df.loc[pd.isnull(users_df['emailAddress'])==True, 'emailAddress']\n",
      "32/237:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df.loc[pd.isnull(users_df['emailAddress'])==True, 'emailAddress']='-'\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'inner',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "32/238: user_texts_emails.sort_values('emailAddress', ascending=True)\n",
      "32/239: users_df.loc[pd.isnull(users_df['emailAddress'])==True, 'emailAddress']='-'\n",
      "32/240: users_df[pd.isnull(users_df.emailAddress)==True]\n",
      "32/241:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df.loc[pd.isnull(users_df['emailAddress'])==True, 'emailAddress']='-'\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "\n",
      "users_df[pd.isnull(users_df.emailAddress)==True]\n",
      "32/242:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df.loc[pd.isnull(users_df['emailAddress'])==True, 'emailAddress']='-'\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "\n",
      "users_df[users_df['name']=='alin']\n",
      "32/243:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'inner',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails.sort_values('emailAddress', ascending=True)\n",
      "32/244:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails.sort_values('emailAddress', ascending=True)\n",
      "32/245:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails.sort_values('emailAddress', ascending=True)\n",
      "user_texts_emails[pd.isnull(user_texts_emails['emailAddress'])==True]\n",
      "32/246:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails.sort_values('emailAddress', ascending=True)\n",
      "user_texts_emails[pd.isnull(user_texts_emails['emailAddress'])==True].shape[0]\n",
      "32/247:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails.sort_values('emailAddress', ascending=True)\n",
      "user_texts_emails[pd.isnull(user_texts_emails['emailAddress'])==True]\n",
      "32/248:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "32/249:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails[pd.isnull(user_texts_emails['emailAddress'])==True]\n",
      "32/250:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails[pd.isnull(user_texts_emails['emailAddress'])==False]\n",
      "32/251:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails[pd.isnull(user_texts_emails['emailAddress'])==False].group_by('emailAddress').count()\n",
      "32/252:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails[pd.isnull(user_texts_emails['emailAddress'])==False].groupby('emailAddress').count()\n",
      "32/253:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails.groupby('emailAddress').count()\n",
      "32/254:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails.groupby(['name','emailAddress']).count()\n",
      "32/255:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails.groupby('name','emailAddress').count()\n",
      "32/256:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails.groupby('user','emailAddress').count()\n",
      "32/257:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_texts_emails.groupby(['user','emailAddress']).count()\n",
      "32/258:\n",
      "text = 'I am the word, number one. This is the other word, number second. here comes the third one, third word'\n",
      "\n",
      "len(row['text'].split())\n",
      "32/259:\n",
      "text = 'I am the word, number one. This is the other word, number second. here comes the third one, third word'\n",
      "\n",
      "len(text.split())\n",
      "32/260:\n",
      "text = 'I am the word, number one. This is the other word, number second. here comes the third one, third word'\n",
      "\n",
      "text.split()\n",
      "32/261:\n",
      "text = 'I am the word, number one. This is the other word, number second. here comes the third one, third word'\n",
      "\n",
      "\n",
      "for word in text.split():\n",
      "    print word\n",
      "32/262:\n",
      "text = 'I am the word, number one. This is the other word, number second. here comes the third one, third word'\n",
      "\n",
      "\n",
      "for word in text.split():\n",
      "    print (word)\n",
      "32/263:\n",
      "text = 'I am the word, number one. This is the other word, number second. here comes the third one, third word'\n",
      "\n",
      "i = 0\n",
      "for word in text.split():\n",
      "    if word=='word':\n",
      "        i= i+1\n",
      "print(i)\n",
      "32/264:\n",
      "text = 'I am the word, number one. This is the other word, number second. here comes the third one, third word'\n",
      "\n",
      "i = 0\n",
      "for word in text.split():\n",
      "    if word=='word':\n",
      "        i= i+1\n",
      "        print(word)\n",
      "print(i)\n",
      "32/265:\n",
      "text = 'I am the word, number one. This is the other word, number second. here comes the third one, third word'\n",
      "\n",
      "i = 0\n",
      "for word in text.split():\n",
      "    if word=='word':\n",
      "        i= i+1\n",
      "        print(word)\n",
      "    print(word)\n",
      "print(i)\n",
      "33/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "driver_activity = pd.to_csv('Hourly_DriverActivity_1.csv')\n",
      "overview_search = pd.to_csv('Hourly_OverviewSearch_1.csv')\n",
      "33/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv')\n",
      "33/3: driver_activity.head\n",
      "33/4:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv')\n",
      "33/5: driver_activity.head\n",
      "33/6: driver_activity.head(5)\n",
      "33/7:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv')\n",
      "33/8: driver_activity.head(5)\n",
      "33/9:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=';')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv')\n",
      "33/10: driver_activity.head(5)\n",
      "33/11:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "33/12: driver_activity.head(5)\n",
      "33/13: driver_activity.head(3)\n",
      "33/14: driver_activity.head(10)\n",
      "33/15: driver_activity.head(3)\n",
      "33/16: overview_search.head(3)\n",
      "33/17: driver_activity.Date\n",
      "33/18:\n",
      "#driver_activity['Day']=\n",
      "driver_activity.str[:5]\n",
      "33/19:\n",
      "#driver_activity['Day']=\n",
      "driver_activity.Date.str[:5]\n",
      "33/20:\n",
      "#driver_activity['Day']=\n",
      "driver_activity.Date.str[:10]\n",
      "33/21:\n",
      "#driver_activity['Date_Day']=driver_activity.Date.str[:10]\n",
      "#driver_activity['Date_hour']=driver_activity.Date.str[11:13]\n",
      "driver_activity.Date.str[11:13]\n",
      "33/22:\n",
      "#driver_activity['Date_Day']=driver_activity.Date.str[:10]\n",
      "#driver_activity['Date_hour']=driver_activity.Date.str[11:13]\n",
      "#df['week_number_of_year'] = df['date_given'].dt.week\n",
      "driver_activity.Date.str[:10].dt.week\n",
      "33/23:\n",
      "#driver_activity['Date_Day']=driver_activity.Date.str[:10]\n",
      "#driver_activity['Date_hour']=driver_activity.Date.str[11:13]\n",
      "#df['week_number_of_year'] = df['date_given'].dt.week\n",
      "#atetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
      "datetime.strptime(driver_activity.Date.str[:10]).dt.week\n",
      "33/24:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "from datetime import datetime\n",
      "\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "33/25:\n",
      "#driver_activity['Date_Day']=driver_activity.Date.str[:10]\n",
      "#driver_activity['Date_hour']=driver_activity.Date.str[11:13]\n",
      "#df['week_number_of_year'] = df['date_given'].dt.week\n",
      "#atetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
      "datetime.strptime(driver_activity.Date.str[:10]).dt.week\n",
      "33/26:\n",
      "#driver_activity['Date_Day']=driver_activity.Date.str[:10]\n",
      "#driver_activity['Date_hour']=driver_activity.Date.str[11:13]\n",
      "#df['week_number_of_year'] = df['date_given'].dt.week\n",
      "#atetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
      "datetime.strptime(driver_activity.Date.str[:10], '%Y-%M-%d').dt.week\n",
      "33/27:\n",
      "#driver_activity['Date_Day']=driver_activity.Date.str[:10]\n",
      "#driver_activity['Date_hour']=driver_activity.Date.str[11:13]\n",
      "#df['week_number_of_year'] = df['date_given'].dt.week\n",
      "#atetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
      "#datetime.strptime(driver_activity.Date.str[:10], '%Y-%M-%d').dt.week\n",
      "pd.todatetime(driver_activity.Date.str[:10])\n",
      "33/28:\n",
      "#driver_activity['Date_Day']=driver_activity.Date.str[:10]\n",
      "#driver_activity['Date_hour']=driver_activity.Date.str[11:13]\n",
      "#df['week_number_of_year'] = df['date_given'].dt.week\n",
      "#atetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
      "#datetime.strptime(driver_activity.Date.str[:10], '%Y-%M-%d').dt.week\n",
      "pd.to_datetime(driver_activity.Date.str[:10])\n",
      "33/29:\n",
      "#driver_activity['Date_Day']=driver_activity.Date.str[:10]\n",
      "#driver_activity['Date_hour']=driver_activity.Date.str[11:13]\n",
      "#df['week_number_of_year'] = df['date_given'].dt.week\n",
      "#atetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
      "#datetime.strptime(driver_activity.Date.str[:10], '%Y-%M-%d').dt.week\n",
      "pd.to_datetime(driver_activity.Date.str[:10], format='%Y-%M-%d')\n",
      "33/30:\n",
      "#driver_activity['Date_Day']=driver_activity.Date.str[:10]\n",
      "#driver_activity['Date_hour']=driver_activity.Date.str[11:13]\n",
      "#df['week_number_of_year'] = df['date_given'].dt.week\n",
      "#atetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
      "#datetime.strptime(driver_activity.Date.str[:10], '%Y-%M-%d').dt.week\n",
      "pd.to_datetime(driver_activity.Date.str[:10])\n",
      "33/31:\n",
      "#driver_activity['Date_Day']=driver_activity.Date.str[:10]\n",
      "#driver_activity['Date_hour']=driver_activity.Date.str[11:13]\n",
      "#df['week_number_of_year'] = df['date_given'].dt.week\n",
      "#atetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
      "#datetime.strptime(driver_activity.Date.str[:10], '%Y-%M-%d').dt.week\n",
      "pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "33/32:\n",
      "driver_activity['Date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['Date_hour']=driver_activity.Date.str[11:13]\n",
      "driver_activity['Date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "33/33: driver_activity\n",
      "33/34: driver_activity.head(3)\n",
      "33/35: driver_activity['Date_week'].unique()\n",
      "33/36:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "from datetime import datetime\n",
      "\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "33/37:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13]\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "33/38: driver_activity['date_week'].unique()\n",
      "33/39:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13]\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "driver_activity.head(3)\n",
      "33/40:\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    driver_activity[driver_activity['date_week']==week].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "33/41:\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    driver_activity[driver_activity['date_week']==_week].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "33/42:\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    print(driver_activity[driver_activity['date_week']==_week].sort_values('Rides per online hour', ascending=False).head(36))\n",
      "33/43:\n",
      "#for _week in driver_activity['date_week'].unique():\n",
      "#    driver_activity[driver_activity['date_week']==_week].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "\n",
      "driver_activity[driver_activity['date_week']==46].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "33/44:\n",
      "#driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "#driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).strftime()\n",
      "#driver_activity['date_hour']=driver_activity.Date.str[11:13]\n",
      "#driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "\n",
      "#df['weekday'] = df[['datetime']].apply(lambda x: dt.datetime.strftime(x['datetime'], '%A'), axis=1)\n",
      "\n",
      "datetime.strftime(pd.to_datetime(driver_activity.Date.str[:10]), '%A')\n",
      "\n",
      "#print(driver_activity['date_week'].unique())\n",
      "#driver_activity.head(3)\n",
      "33/45:\n",
      "#driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "#driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).strftime()\n",
      "#driver_activity['date_hour']=driver_activity.Date.str[11:13]\n",
      "#driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "\n",
      "#df['weekday'] = df[['datetime']].apply(lambda x: dt.datetime.strftime(x['datetime'], '%A'), axis=1)\n",
      "\n",
      "pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: dt.datetime.strftime(x, '%A'))\n",
      "\n",
      "#print(driver_activity['date_week'].unique())\n",
      "#driver_activity.head(3)\n",
      "33/46:\n",
      "#driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "#driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).strftime()\n",
      "#driver_activity['date_hour']=driver_activity.Date.str[11:13]\n",
      "#driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "\n",
      "#df['weekday'] = df[['datetime']].apply(lambda x: dt.datetime.strftime(x['datetime'], '%A'), axis=1)\n",
      "\n",
      "pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "\n",
      "#print(driver_activity['date_week'].unique())\n",
      "#driver_activity.head(3)\n",
      "33/47: driver_activity.head(3)\n",
      "33/48:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "from datetime import datetime\n",
      "\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "33/49: driver_activity.head(3)\n",
      "33/50:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13]\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "driver_activity.head(3)\n",
      "33/51:\n",
      "#for _week in driver_activity['date_week'].unique():\n",
      "#    driver_activity[driver_activity['date_week']==_week].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "\n",
      "driver_activity[driver_activity['date_week']==46].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "33/52:\n",
      "#for _week in driver_activity['date_week'].unique():\n",
      "#    driver_activity[driver_activity['date_week']==_week].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "\n",
      "driver_activity[driver_activity['date_week']==46].sort_values('Rides per online hour', ascending=False).head(36).to_csv('exp1.csv')\n",
      "33/53:\n",
      "#for _week in driver_activity['date_week'].unique():\n",
      "#    driver_activity[driver_activity['date_week']==_week].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "\n",
      "driver_activity[driver_activity['date_week']==46].sort_values(('Online (h)'-'Has booking (h)'/'Online (h)', ascending=False).head(36).to_csv('exp1.csv')\n",
      "33/54:\n",
      "#for _week in driver_activity['date_week'].unique():\n",
      "#    driver_activity[driver_activity['date_week']==_week].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "\n",
      "driver_activity[driver_activity['date_week']==46].sort_values(('Online (h)'-'Has booking (h)'/'Online (h)'), ascending=False).head(36).to_csv('exp1.csv')\n",
      "33/55:\n",
      "#for _week in driver_activity['date_week'].unique():\n",
      "#    driver_activity[driver_activity['date_week']==_week].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "\n",
      "driver_activity[driver_activity['date_week']==46].sort_values((int('Online (h)')-int('Has booking (h)')/int('Online (h)')), ascending=False).head(36).to_csv('exp1.csv')\n",
      "33/56:\n",
      "#for _week in driver_activity['date_week'].unique():\n",
      "#    driver_activity[driver_activity['date_week']==_week].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "driver_activity['Online (h)'] - driver_activity['Has booking (h)']\n",
      "#driver_activity[driver_activity['date_week']==46].sort_values((int('Online (h)')-int('Has booking (h)')/int('Online (h)')), ascending=False).head(36).to_csv('exp1.csv')\n",
      "33/57:\n",
      "#for _week in driver_activity['date_week'].unique():\n",
      "#    driver_activity[driver_activity['date_week']==_week].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "(driver_activity['Online (h)'] - driver_activity['Has booking (h)'])/driver_activity['Online (h)']\n",
      "#driver_activity[driver_activity['date_week']==46].sort_values((int('Online (h)')-int('Has booking (h)')/int('Online (h)')), ascending=False).head(36).to_csv('exp1.csv')\n",
      "33/58:\n",
      "#for _week in driver_activity['date_week'].unique():\n",
      "#    driver_activity[driver_activity['date_week']==_week].sort_values('Rides per online hour', ascending=False).head(36)\n",
      "driver_activity['undersupply']=(driver_activity['Online (h)'] - driver_activity['Has booking (h)'])/driver_activity['Online (h)']\n",
      "#driver_activity[driver_activity['date_week']==46].sort_values((int('Online (h)')-int('Has booking (h)')/int('Online (h)')), ascending=False).head(36).to_csv('exp1.csv')\n",
      "33/59: driver_activity.sort_values('undersupply', ascending=False)\n",
      "33/60: driver_activity.sort_values('undersupply', ascending=True)\n",
      "33/61:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13]\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "33/62: driver_activity.sort_values('undersupply', ascending=True).head(5)\n",
      "33/63: driver_activity.sort_values(['undersupply', 'date_week'], ascending=True).head(5)\n",
      "33/64: driver_activity.sort_values(['undersupply'], ascending=True).head(5)\n",
      "33/65:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "from datetime import datetime\n",
      "\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "33/66: driver_activity.head(3)\n",
      "33/67:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13]\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "33/68: driver_activity['undersupply']=driver_activity['Has booking (h)']/driver_activity['Online (h)']\n",
      "33/69: driver_activity.sort_values(['undersupply'], ascending=False).head(5)\n",
      "33/70:\n",
      "\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']=_week].sort_values(['undersupply'],ascending=False)[['date_week','Date', 'date_weekday', 'undersupply']].head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.Concat([weekly_undersupply, iter_subset])\n",
      "    i = i+1\n",
      "33/71:\n",
      "\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']=_week][['date_week','Date', 'date_weekday', 'undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.Concat([weekly_undersupply, iter_subset])\n",
      "    i = i+1\n",
      "33/72:\n",
      "\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_week','Date', 'date_weekday', 'undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.Concat([weekly_undersupply, iter_subset])\n",
      "    i = i+1\n",
      "33/73:\n",
      "\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_week','Date', 'date_weekday', 'undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.concat([weekly_undersupply, iter_subset])\n",
      "    i = i+1\n",
      "33/74: weekly_undersupply\n",
      "33/75: weekly_undersupply.shape[0]\n",
      "33/76: weekly_undersupply\n",
      "33/77:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_week','Date', 'date_weekday', 'undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.concat([weekly_undersupply, iter_subset], axis=1, sort=False)\n",
      "    i = i+1\n",
      "33/78: weekly_undersupply.sort_values()\n",
      "33/79: weekly_undersupply\n",
      "33/80:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.concat([weekly_undersupply, iter_subset], axis=1, sort=False)\n",
      "    i = i+1\n",
      "33/81: weekly_undersupply\n",
      "33/82:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "from datetime import datetime\n",
      "\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "33/83: driver_activity.head(3)\n",
      "33/84:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13]\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "33/85: driver_activity['undersupply']=driver_activity['Has booking (h)']/driver_activity['Online (h)']\n",
      "33/86: driver_activity.sort_values(['undersupply'], ascending=False).head(5)\n",
      "33/87:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.concat([weekly_undersupply, iter_subset], axis=1, sort=False)\n",
      "    i = i+1\n",
      "33/88: weekly_undersupply\n",
      "33/89:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.concat([weekly_undersupply, iter_subset], axis=1, ignore_index=True)\n",
      "    i = i+1\n",
      "33/90:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.concat([weekly_undersupply, iter_subset], axis=1, ignore_index=True)\n",
      "    i = i+1\n",
      "33/91: weekly_undersupply\n",
      "33/92:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.concat([weekly_undersupply, iter_subset], axis=1, ignore_index=False)\n",
      "    i = i+1\n",
      "33/93: weekly_undersupply\n",
      "33/94:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.concat([weekly_undersupply, iter_subset], axis=1, ignore_index=True)\n",
      "    i = i+1\n",
      "33/95: weekly_undersupply\n",
      "33/96:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.concat([weekly_undersupply, iter_subset], axis=1)\n",
      "    i = i+1\n",
      "33/97: weekly_undersupply\n",
      "33/98:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.concat([weekly_undersupply, iter_subset], axis=1, sort=True)\n",
      "    i = i+1\n",
      "33/99: weekly_undersupply\n",
      "33/100:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = pd.concat([weekly_undersupply, iter_subset], axis=1, sort=False)\n",
      "    i = i+1\n",
      "33/101: weekly_undersupply\n",
      "33/102:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "33/103: weekly_undersupply\n",
      "33/104:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_week', 'date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "33/105: weekly_undersupply\n",
      "33/106: weekly_undersupply.groupby(date_week)\n",
      "33/107: weekly_undersupply.groupby('date_week')\n",
      "33/108: weekly_undersupply.groupby('date_week').count\n",
      "33/109: weekly_undersupply.groupby('date_week').count()\n",
      "33/110: weekly_undersupply.groupby('date_week').()\n",
      "33/111: weekly_undersupply.groupby('date_week')()\n",
      "33/112: weekly_undersupply.groupby('date_week')\n",
      "33/113: weekly_undersupply.groupby('date_week').head(180)\n",
      "33/114: weekly_undersupply.groupby('date_week').size\n",
      "33/115: weekly_undersupply.groupby('date_week').size()\n",
      "33/116: weekly_undersupply.groupby('date_week').agg()\n",
      "33/117: weekly_undersupply.groupby('date_week').agg(max)\n",
      "33/118: weekly_undersupply.groupby('date_week')\n",
      "33/119: weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=True)\n",
      "33/120: weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=False)\n",
      "33/121:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_week', 'date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "eekly_undersupply = weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=False)\n",
      "33/122: weekly_undersupply\n",
      "33/123:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_week', 'date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "weekly_undersupply = weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=False)\n",
      "33/124: weekly_undersupply\n",
      "33/125:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['date_week', 'date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "weekly_undersupply = weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=False)\n",
      "weekly_undersupply.to_csv('weekly_undersupply.csv')\n",
      "33/126: weekly_undersupply\n",
      "33/127: weekly_undersupply.head(3)\n",
      "33/128:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['Date','date_week', 'date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "weekly_undersupply = weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=False)\n",
      "weekly_undersupply.to_csv('weekly_undersupply.csv')\n",
      "33/129: weekly_undersupply.head(3)\n",
      "33/130: weekly_undersupply.head()\n",
      "33/131: driver_activity\n",
      "33/132: driver_activity.head()\n",
      "33/133:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(date_weekday, date_hour).agg({'undersupply':'mean'})\n",
      "33/134:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby('date_weekday', date_hour).agg({'undersupply':'mean'})\n",
      "33/135:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby('date_weekday', 'date_hour').agg({'undersupply':'mean'})\n",
      "33/136:\n",
      "driver_activity.head()\n",
      "#driver_activity.groupby('date_weekday', 'date_hour').agg({'undersupply':'mean'})\n",
      "33/137:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'})\n",
      "33/138:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).head(36)\n",
      "33/139:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values(['undersupply'])head(36)\n",
      "33/140:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values(['undersupply']).head(36)\n",
      "33/141:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "33/142:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['Date','date_week', 'date_weekday', 'date_hour','undersupply']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "33/143:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['Date','date_week', 'date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "33/144:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_week', 'date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "33/145:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_week', 'date_weekday', 'date_hour']).agg({'undersupply':'max'}).sort_values('undersupply', ascending=False).head(36)\n",
      "33/146:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_week', 'date_weekday', 'date_hour']).agg({'undersupply':'min'}).sort_values('undersupply', ascending=False).head(36)\n",
      "33/147:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_week', 'date_weekday', 'date_hour']).agg({'undersupply':'max'}).sort_values(['date_week', ascending=False).head(36)\n",
      "33/148:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_week', 'date_weekday', 'date_hour']).agg({'undersupply':'max'}).sort_values(['date_week'], ascending=False).head(36)\n",
      "33/149: weekly_undersupply\n",
      "33/150:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "33/151: <h4> Understanding #1: 36 hours of a week that are on average most undersupplied. </h4>\n",
      "33/152: driver_activity.head(5)\n",
      "33/153:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime\n",
      "from datetime import datetime\n",
      "import seaborn as sns\n",
      "33/154:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "33/155: plt.plot(driver_activity['online (h)'], driver_activity['Has booking (h)'])\n",
      "33/156: plt.plot(driver_activity['Online (h)'], driver_activity['Has booking (h)'])\n",
      "33/157: plt.plot(driver_activity['Online (h)'], driver_activity['Has booking (h)'])\n",
      "33/158:\n",
      "plt.plot(driver_activity['Online (h)'].groupby('date_hour').agg({'hour':'mean'}), \n",
      "         driver_activity['Has booking (h)'].groupby('date_hour').agg({'hour':'mean'}))\n",
      "33/159:\n",
      "plt.plot(driver_activity.groupby('Online (h)').agg({'date_hour':'mean'}), \n",
      "         driver_activity.groupby('Has booking (h)').agg({'date_hour':'mean'}))\n",
      "33/160: driver_activity.groupby('Online (h)').agg({'date_hour':'mean'})\n",
      "33/161: driver_activity.groupby('Online (h)').agg({'date_hour':'max'})\n",
      "33/162:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=int(driver_activity.Date.str[11:13])\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "33/163:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13].astype(int)\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "33/164:\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "33/165: driver_activity.head(3)\n",
      "33/166:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13].astype(int)\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "33/167: driver_activity['undersupply']=driver_activity['Has booking (h)']/driver_activity['Online (h)']\n",
      "33/168:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "33/169:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['Date','date_week', 'date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "weekly_undersupply = weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=False)\n",
      "weekly_undersupply.to_csv('weekly_undersupply.csv')\n",
      "33/170: weekly_undersupply\n",
      "33/171: driver_activity.head(5)\n",
      "33/172: driver_activity.groupby('Online (h)').agg({'date_hour':'max'})\n",
      "33/173: driver_activity.groupby('Online (h)').agg({'date_hour':'mean'})\n",
      "33/174:\n",
      "plt.plot(driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}))\n",
      "33/175: driver_activity.groupby('date_hour').agg({'Online (h)':'mean'})\n",
      "33/176: driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'})\n",
      "33/177:\n",
      "plt.plot(driver_activity['Online (h)'], driver_activity['Has booking (h)'])\n",
      "#plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')\n",
      "33/178: #plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')\n",
      "33/179: plt.plot(x=driver_activity[date_hour].unique())\n",
      "33/180: plt.plot(x=driver_activity['date_hour'].unique())\n",
      "33/181: plt.plot(driver_activity['date_hour'].unique())\n",
      "33/182: plt.plot(x=driver_activity['date_hour'].unique(), y=driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}))\n",
      "33/183: plt.plot(driver_activity['date_hour'].unique(), driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}))\n",
      "33/184:\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "33/185:\n",
      "sns.lineplot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "33/186:\n",
      "sns.barplot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "33/187:\n",
      "sns.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "33/188:\n",
      "sns.scatterplot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "33/189:\n",
      "sns.scatter(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "33/190:\n",
      "sns.lmplot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "33/191:\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "33/192:\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "ax.set_xticks()\n",
      "33/193:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks()\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.show()\n",
      "33/194:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.show()\n",
      "33/195:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.show()\n",
      "33/196:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.show()\n",
      "33/197:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), 'r'\n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.show()\n",
      "33/198:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), 'r',\n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.show()\n",
      "33/199:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), 'r--',\n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.show()\n",
      "33/200:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.show()\n",
      "33/201:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),'r--', \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.show()\n",
      "33/202:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.legend()\n",
      "plt.show()\n",
      "33/203:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.legend('a', 'b')\n",
      "plt.show()\n",
      "33/204:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.legend('a', 'b', 'c')\n",
      "plt.show()\n",
      "33/205:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.legend('a', 'b', 'c')\n",
      "plt.show()\n",
      "33/206:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "plt.show()\n",
      "33/207:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         columns=['A', 'B', 'C'])\n",
      "plt.show()\n",
      "33/208:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         columns=['A', 'B'])\n",
      "plt.show()\n",
      "33/209:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "plt.show()\n",
      "33/210:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), label='A'\n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "plt.show()\n",
      "33/211:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), label='A',\n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "plt.show()\n",
      "33/212:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "plt.show()\n",
      "33/213:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), label='A'\n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "plt.show()\n",
      "33/214:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), label='A',\n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "plt.show()\n",
      "33/215:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), labels='A',\n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "plt.show()\n",
      "33/216:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         labels='A',\n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "plt.show()\n",
      "33/217:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "plt.show()\n",
      "33/218:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "plt.labels=['A', 'B'],\n",
      "plt.show()\n",
      "33/219:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "plt.labels=['A', 'B'],\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "plt.show()\n",
      "33/220:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "plt.show()\n",
      "33/221:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "# Create a legend for the first line.\n",
      "first_legend = plt.legend(handles=[line1], loc='upper right')\n",
      "\n",
      "# Add the legend manually to the current Axes.\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "\n",
      "# Create another legend for the second line.\n",
      "plt.legend(handles=[line2], loc='lower right')\n",
      "\n",
      "plt.show()\n",
      "33/222:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "# Create a legend for the first line.\n",
      "first_legend = plt.legend(handles=['A'], loc='upper right')\n",
      "\n",
      "# Add the legend manually to the current Axes.\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "\n",
      "# Create another legend for the second line.\n",
      "plt.legend(handles=['B'], loc='lower right')\n",
      "\n",
      "plt.show()\n",
      "33/223:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "# Create a legend for the first line.\n",
      "first_legend = plt.legend(handles=driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), loc='upper right')\n",
      "\n",
      "# Add the legend manually to the current Axes.\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "\n",
      "# Create another legend for the second line.\n",
      "plt.legend(handles=driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), loc='lower right')\n",
      "\n",
      "plt.show()\n",
      "33/224:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "# Create a legend for the first line.\n",
      "first_legend = plt.legend(handles='A', loc='upper right')\n",
      "\n",
      "# Add the legend manually to the current Axes.\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "\n",
      "# Create another legend for the second line.\n",
      "plt.legend(handles='B', loc='lower right')\n",
      "\n",
      "plt.show()\n",
      "33/225:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "# Create a legend for the first line.\n",
      "first_legend = plt.legend('A', loc='upper right')\n",
      "\n",
      "# Add the legend manually to the current Axes.\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "\n",
      "# Create another legend for the second line.\n",
      "plt.legend('B', loc='lower right')\n",
      "\n",
      "plt.show()\n",
      "33/226:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "# Create a legend for the first line.\n",
      "first_legend = plt.legend('demand', loc='upper right')\n",
      "\n",
      "# Add the legend manually to the current Axes.\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "\n",
      "# Create another legend for the second line.\n",
      "plt.legend('supply', loc='lower right')\n",
      "\n",
      "plt.show()\n",
      "33/227:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "# Create a legend for the first line.\n",
      "first_legend = plt.legend('demand'], loc='upper right')\n",
      "\n",
      "# Add the legend manually to the current Axes.\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "\n",
      "# Create another legend for the second line.\n",
      "plt.legend('supply', loc='lower right')\n",
      "\n",
      "plt.show()\n",
      "33/228:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "# Create a legend for the first line.\n",
      "first_legend = plt.legend(['demand'], loc='upper right')\n",
      "\n",
      "# Add the legend manually to the current Axes.\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "\n",
      "# Create another legend for the second line.\n",
      "plt.legend('supply', loc='lower right')\n",
      "\n",
      "plt.show()\n",
      "33/229:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "# Create a legend for the first line.\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "\n",
      "# Add the legend manually to the current Axes.\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "\n",
      "# Create another legend for the second line.\n",
      "plt.legend('supply', loc='lower right')\n",
      "\n",
      "plt.show()\n",
      "33/230:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "# Create a legend for the first line.\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "\n",
      "# Add the legend manually to the current Axes.\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/231: driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'})\n",
      "33/232:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour')\n",
      "\n",
      "plt.show()\n",
      "33/233:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour',textsize=10)\n",
      "\n",
      "plt.show()\n",
      "33/234:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour',size=10)\n",
      "\n",
      "plt.show()\n",
      "33/235:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour',size=30)\n",
      "\n",
      "plt.show()\n",
      "33/236:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour',size=10)\n",
      "\n",
      "plt.show()\n",
      "33/237:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour',size=20)\n",
      "\n",
      "plt.show()\n",
      "33/238:\n",
      "fig, ax = plt.subplots(figsize=(16,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour')\n",
      "plt.ylabel('hours')\n",
      "\n",
      "plt.show()\n",
      "33/239:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour')\n",
      "plt.ylabel('hours')\n",
      "\n",
      "plt.show()\n",
      "33/240:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('time of the day')\n",
      "plt.ylabel('hours')\n",
      "\n",
      "plt.show()\n",
      "33/241:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('hours')\n",
      "\n",
      "plt.show()\n",
      "33/242:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hrs')\n",
      "\n",
      "plt.show()\n",
      "33/243:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hr')\n",
      "\n",
      "plt.show()\n",
      "33/244:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/245:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_yticks([0,100])\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/246:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_yticks([0,5,100])\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/247:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_yticks(np.random(0,40))\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/248:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_yticks()\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/249:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/250:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.barplot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/251:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.bar(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/252:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/253:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "#ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "y = [4, 9, 2]\n",
      "z = [1, 2, 3]\n",
      "k = [11, 12, 13]\n",
      "\n",
      "ax = plt.subplot(111)\n",
      "ax.bar(x-0.2, y, width=0.2, color='b', align='center')\n",
      "ax.bar(x, z, width=0.2, color='g', align='center')\n",
      "ax.bar(x+0.2, k, width=0.2, color='r', align='center')\n",
      "ax.xaxis_date()\n",
      "\n",
      "plt.show()\n",
      "ax.bar(driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}))\n",
      "ax.bar(driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/254:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "#ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "ax = plt.subplot(111)\n",
      "\n",
      "plt.show()\n",
      "ax.bar(driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}))\n",
      "ax.bar(driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/255:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "ax = plt.subplot(111)\n",
      "\n",
      "plt.show()\n",
      "ax.bar(driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}))\n",
      "ax.bar(driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}))\n",
      "\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/256:\n",
      "\n",
      "sns.barplot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "33/257: <h2> 3) Visualisation of hours where we lack supply during a weekly period. This one we can send to drivers to show when to online for extra hours. </h3>\n",
      "33/258:\n",
      "tab = pd.crosstab(driver_activity['date_hour']\n",
      "                    , driver_activity['date_weekday']\n",
      "                    , values = driver_activity['Online (h)']- driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(10,8)})\n",
      "sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/259:\n",
      "tab = pd.crosstab(driver_activity['date_hour']\n",
      "                    , driver_activity['date_weekday']\n",
      "                    , values = driver_activity['Online (h)']- driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,8)})\n",
      "sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/260:\n",
      "tab = pd.crosstab(driver_activity['date_hour']\n",
      "                    , driver_activity['date_weekday']\n",
      "                    , values = driver_activity['Online (h)']- driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,10)})\n",
      "sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/261:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']- driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,10)})\n",
      "sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/262:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']- driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,8)})\n",
      "sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/263:\n",
      "fig, ax = plt.subplots(figsize=(12,10))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/264:\n",
      "fig, ax = plt.subplots(figsize=(12,8))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/265: driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'})\n",
      "33/266: driver_activity.groupby('date_hour').agg({'Online (h)':'mean'})\n",
      "33/267:\n",
      "fig, ax = plt.subplots(figsize=(12,8))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_xtics([0,5,10,50])\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/268:\n",
      "fig, ax = plt.subplots(figsize=(12,8))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_ytics([0,5,10,50])\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/269:\n",
      "fig, ax = plt.subplots(figsize=(12,8))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/270:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/271:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/272:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_yticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/273:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_yticks([0,100])\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/274:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_yticks([0,100])\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/275:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_yticks([0,40])\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "plt.show()\n",
      "33/276:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_yticks([0,40])\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}), \n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/277:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_yticks([0,40])\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand'\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend(['demand','supply'], loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/278:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_yticks([0,40])\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand'\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend()\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/279:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "ax.set_yticks([0,40])\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand'\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),label='supply'\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/280:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand'\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),label='supply'\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/281:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand',linewidth=2\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),label='supply'\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/282:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'}),label='supply'\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/283:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right')\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/284:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right', size=20)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/285:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right', fontsize=20)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/286:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right', fontsize=10)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/287:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day')\n",
      "plt.ylabel('average hour')\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/288:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/289:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,8)})\n",
      "plt.yticks(rotation=45)\n",
      "sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/290:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,8)})\n",
      "sns.yticks(rotation=45)\n",
      "sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/291:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,8)})\n",
      "sns.set_xticklabels(rotation=30)\n",
      "sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/292:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,8)})\n",
      "set_xticklabels(rotation=30)\n",
      "sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/293:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,8)})\n",
      "\n",
      "sns.heatmap(tab, annot=True, cmap='Reds', fmt='g', set_xticklabels(rotation=30))\n",
      "33/294:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,8)})\n",
      "\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/295:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,8)})\n",
      "\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "hm.set_xticklabels(rotation=30)\n",
      "33/296:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,8)})\n",
      "\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "hm.set_yticklabels(rotation=30)\n",
      "33/297:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,8)})\n",
      "\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "hm.set_yticklabels(rotation=30, labels='')\n",
      "33/298:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(16,8)})\n",
      "\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "hm.set_yticklabels(rotation=30, labels=driver_activity['date_weekday'].unique())\n",
      "33/299:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(16,8)})\n",
      "hm.set_yticklabels(rotation=30, labels=driver_activity['date_weekday'].unique())\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/300:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(16,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "hm.set_yticklabels(rotation=30, labels=driver_activity['date_weekday'].unique())\n",
      "33/301:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(16,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "hm.set_yticklabels(rotation=30, labels=driver_activity['date_weekday'])\n",
      "33/302:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(16,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "hm.set_yticklabels(rotation=30)\n",
      "33/303:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(16,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/304:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/305:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "34/1:\n",
      "# This cell shows average clicks on each final esult for each courses. code presentation is not distincted.\n",
      "\n",
      "tab = pd.crosstab(results_aged.code_module\n",
      "                    , results_aged.final_result\n",
      "                    , values =  results_aged.sum_click\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "sns.set(rc={'figure.figsize':(10,8)})\n",
      "sns.heatmap(tab, annot=True, cmap='RdYl', fmt='g')\n",
      "\n",
      "# It is shown here that higher number of clicks on average, results in more positive results.\n",
      "33/306:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='RdBu', fmt='g')\n",
      "33/307:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='RdWh', fmt='g')\n",
      "33/308:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='RdWt', fmt='g')\n",
      "33/309:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='RdWe', fmt='g')\n",
      "33/310:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='RdWi', fmt='g')\n",
      "33/311:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='GnRd', fmt='g')\n",
      "33/312:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='GrRd', fmt='g')\n",
      "33/313:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Reds', fmt='g')\n",
      "33/314:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='r', fmt='g')\n",
      "33/315:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='Rd', fmt='g')\n",
      "33/316:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='RdYl', fmt='g')\n",
      "33/317:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='RdGn', fmt='g')\n",
      "33/318:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='RdBl', fmt='g')\n",
      "33/319:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='RdBu', fmt='g')\n",
      "33/320:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='RdGr', fmt='g')\n",
      "33/321:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='RdGe', fmt='g')\n",
      "33/322:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='RdGn', fmt='g')\n",
      "33/323:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='YlGnRd', fmt='g')\n",
      "33/324:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='YlGn', fmt='g')\n",
      "33/325:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='GnYl', fmt='g')\n",
      "33/326:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='YlGn', fmt='g')\n",
      "33/327:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='YlGn', fmt='r')\n",
      "33/328:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='YlGn', fmt='g')\n",
      "33/329:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='YlGnBu', fmt='g')\n",
      "33/330:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, fmt='g')\n",
      "33/331:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='puBl', fmt='g')\n",
      "33/332:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='PuBl', fmt='g')\n",
      "33/333:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap='PuRd', fmt='g')\n",
      "33/334:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Online (h)']-driver_activity['Has booking (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, fmt='g')\n",
      "33/335:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = (driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, fmt='g')\n",
      "33/336:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, fmt='g')\n",
      "33/337:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)', 0]\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, fmt='g')\n",
      "33/338:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)'], 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, fmt='g')\n",
      "33/339:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)'] 0),\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, fmt='g')\n",
      "33/340:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)'],0),\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, fmt='g')\n",
      "33/341:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round((driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)'],0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, fmt='g')\n",
      "33/342:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round((driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)'],1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, fmt='g')\n",
      "33/343:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = 1-round((driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)'],1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, fmt='g')\n",
      "33/344:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = 1-round((driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)'],1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"reds\",fmt='g')\n",
      "33/345:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = 1-round((driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)'],1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "33/346:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = 1-round((driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)'],3)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "33/347:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = 1-round((driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)'],2)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "33/348:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round((driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)'],2)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "33/349:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = 1-round((driver_activity['Online (h)']-driver_activity['Has booking (h)'])/driver_activity['Online (h)'],2)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "33/350:\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "33/351:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13].astype(int)\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "driver_activity.to_csv('exp1.csv')\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "33/352: driver_activity['undersupply']=driver_activity['Has booking (h)']/driver_activity['Online (h)']\n",
      "33/353:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "33/354:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['Date','date_week', 'date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "weekly_undersupply = weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=False)\n",
      "weekly_undersupply.to_csv('weekly_undersupply.csv')\n",
      "33/355: weekly_undersupply\n",
      "33/356:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/357:\n",
      "Overview_search['date_Day']=pd.to_datetime(Overview_search.Date.str[:10])\n",
      "Overview_search['date_weekday'] = pd.to_datetime(Overview_search.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "Overview_search['date_hour']=Overview_search.Date.str[11:13].astype(int)\n",
      "Overview_search['date_week'] = pd.to_datetime(Overview_search.Date.str[:10]).dt.week\n",
      "Overview_search.to_csv('exp2.csv')\n",
      "\n",
      "print(Overview_search['date_week'].unique())\n",
      "33/358:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "33/359:\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "33/360: driver_activity.head(3)\n",
      "33/361:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13].astype(int)\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "driver_activity.to_csv('exp1.csv')\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "33/362:\n",
      "Overview_search['date_Day']=pd.to_datetime(Overview_search.Date.str[:10])\n",
      "Overview_search['date_weekday'] = pd.to_datetime(Overview_search.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "Overview_search['date_hour']=Overview_search.Date.str[11:13].astype(int)\n",
      "Overview_search['date_week'] = pd.to_datetime(Overview_search.Date.str[:10]).dt.week\n",
      "Overview_search.to_csv('exp2.csv')\n",
      "\n",
      "print(Overview_search['date_week'].unique())\n",
      "33/363:\n",
      "overview_search['date_Day']=pd.to_datetime(overview_search.Date.str[:10])\n",
      "overview_search['date_weekday'] = pd.to_datetime(overview_search.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "overview_search['date_hour']=overview_search.Date.str[11:13].astype(int)\n",
      "overview_search['date_week'] = pd.to_datetime(overview_search.Date.str[:10]).dt.week\n",
      "overview_search.to_csv('exp2.csv')\n",
      "\n",
      "print(overview_search['date_week'].unique())\n",
      "33/364: overview_search.head(5)\n",
      "33/365: driver_activity['undersupply']=driver_activity['Has booking (h)']/driver_activity['Online (h)']\n",
      "33/366:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "33/367: driver_activity\n",
      "33/368: driver_activity.head(4)\n",
      "33/369:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Finished Rides']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "33/370:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Finished Rides']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"GnYl\",fmt='g')\n",
      "33/371:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Finished Rides']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Greens\",fmt='g')\n",
      "33/372:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Finished Rides']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/373:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Finished Rides']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/374: tab\n",
      "33/375:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity[driver_activity['Finished Rides']>10]\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/376:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Finished Rides']\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/377:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity[driver_activity['Finished Rides']/max(driver_activity['Finished Rides'])\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/378:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = driver_activity['Finished Rides']/max(driver_activity['Finished Rides'])\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/379:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/380:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 2)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/381:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/382:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/383: max(driver_activity['Finished Rides'])\n",
      "33/384: driver_activity['Finished Rides']\n",
      "33/385: driver_activity.sort_values('Finished Rides', ascending=False)\n",
      "33/386:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides'], 1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/387:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/388:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1).apply(lambda x: 1 if(x>=0.1) else 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/389:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/390:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1).apply(lambda x: 1 if(x>=0.2) else 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/391:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1).apply(lambda x: 1 if(x>=0.2) 0 if (x<0.2))\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/392:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1).apply(lambda x: 1 if(x>=0.2) 0 if (x<0.2) else 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/393:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1).apply(lambda x: 1 if(x>=0.2) 0 else)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/394:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1).apply(lambda x: 1 if(x>=0.2) else 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/395:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1).apply(lambda x: 0 if(x<0.2) else 1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/396:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1).apply(lambda x: x>0.2)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/397:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1).apply(lambda x: 1 if x>0.2)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/398:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1).apply(lambda x: 1 if (x>0.2))\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/399:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1).apply(lambda x: x>0.2)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/400:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']/max(driver_activity['Finished Rides']), 1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/401:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides']), 1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/402:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides'], 1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "33/403:\n",
      "full_date = pd.merge(driver_activity, overview_search, how = 'left',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "33/404: full_data.shape[0]\n",
      "33/405:\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'left',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "33/406: full_data.shape[0]\n",
      "33/407:\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'inner',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "33/408: print(full_data.shape[0], driver_activity.shape[0], overview_search.shape[0])\n",
      "33/409: full_date.head(3)\n",
      "33/410:\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "33/411: driver_activity.head(3)\n",
      "33/412:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13].astype(int)\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "driver_activity.to_csv('exp1.csv')\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "33/413: driver_activity['undersupply']=driver_activity['Has booking (h)']/driver_activity['Online (h)']\n",
      "33/414:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "33/415:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['Date','date_week', 'date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "weekly_undersupply = weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=False)\n",
      "weekly_undersupply.to_csv('weekly_undersupply.csv')\n",
      "33/416: weekly_undersupply\n",
      "33/417:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(driver_activity['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(driver_activity['date_hour'].unique(), \n",
      "         driver_activity.groupby('date_hour').agg({'Online (h)':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "33/418:\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'inner',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "33/419: print(full_data.shape[0], driver_activity.shape[0], overview_search.shape[0])\n",
      "33/420: it means we lost only one hourly data during the joining the tables, which is fine.\n",
      "33/421: full_date.head(3)\n",
      "33/422:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "33/423:\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "33/424: driver_activity.head(3)\n",
      "33/425:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13].astype(int)\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "driver_activity.to_csv('exp1.csv')\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "33/426: driver_activity['undersupply']=driver_activity['Has booking (h)']/driver_activity['Online (h)']\n",
      "33/427:\n",
      "driver_activity.head()\n",
      "driver_activity.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "33/428:\n",
      "i = 0\n",
      "for _week in driver_activity['date_week'].unique():\n",
      "    iter_subset = driver_activity[driver_activity['date_week']==_week][['Date','date_week', 'date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "weekly_undersupply = weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=False)\n",
      "weekly_undersupply.to_csv('weekly_undersupply.csv')\n",
      "33/429:\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'inner',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "33/430: print(full_data.shape[0], driver_activity.shape[0], overview_search.shape[0])\n",
      "33/431: full_date.head(3)\n",
      "33/432: overview_search\n",
      "37/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "37/2:\n",
      "\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "37/3: driver_activity.head(3)\n",
      "37/4:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13].astype(int)\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "driver_activity.to_csv('exp1.csv')\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "37/5: driver_activity['undersupply']=driver_activity['Has booking (h)']/driver_activity['Online (h)']\n",
      "37/6: full_date.head(3)\n",
      "37/7:\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'inner',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "37/8: print(full_data.shape[0], driver_activity.shape[0], overview_search.shape[0])\n",
      "37/9: full_data.head(3)\n",
      "37/10: full_data[full_data['Taxify: Coverage Ratio (unique)']>=90]\n",
      "37/11: full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "37/12: full_data[full_data['Taxify: Coverage Ratio (unique)']>=95].sort_values('Online'/'Taxify: People saw +1 cars (unique)')\n",
      "37/13: full_data[full_data['Taxify: Coverage Ratio (unique)']>=95].sort_values(['Online']/['Taxify: People saw +1 cars (unique)'])\n",
      "37/14: full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]['Online']/['Taxify: People saw +1 cars (unique)']\n",
      "37/15: full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]['Online'/'Taxify: People saw +1 cars (unique)']\n",
      "37/16: full_data[full_data['Taxify: Coverage Ratio (unique)']>=95][float('Online')/float('Taxify: People saw +1 cars (unique)')]\n",
      "37/17: full_data[full_data['Taxify: Coverage Ratio (unique)']>=95][full_data['Online']/full_data['Taxify: People saw +1 cars (unique)']\n",
      "37/18: full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]full_data['Online']/full_data['Taxify: People saw +1 cars (unique)']\n",
      "37/19: full_data['Online']/full_data['Taxify: People saw +1 cars (unique)']\n",
      "37/20: full_data['online']/full_data['Taxify: People saw +1 cars (unique)']\n",
      "37/21: full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "37/22:\n",
      "#full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "full_data['Online (h)']/full_data['Taxify: People saw +1 cars (unique)']\n",
      "37/23:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "mean(hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)'])\n",
      "37/24:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "avg(hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)'])\n",
      "37/25:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.mean(hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)'])\n",
      "37/26:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "print(np.mean(hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)']),\n",
      "np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)']))\n",
      "37/27:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'])\n",
      "37/28:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)']\n",
      "37/29:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'])\n",
      "37/30:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype=np.float)\n",
      "37/31:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype=np.float)\n",
      "37/32:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "math.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype=np.float)\n",
      "37/33:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "pd.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype=np.float)\n",
      "37/34:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype=np.float)\n",
      "37/35:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.nanmean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'])\n",
      "37/36:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.nanmean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype=float64)\n",
      "37/37:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.nanmean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype=float32)\n",
      "37/38:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype=float32)\n",
      "37/39:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype='float32')\n",
      "37/40:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype='float64')\n",
      "37/41:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.nanmean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype='float64')\n",
      "37/42:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "np.nansum(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype='float64')\n",
      "37/43:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "sum(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], dtype='float64')\n",
      "37/44:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "sum(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'])\n",
      "37/45:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']<95]\n",
      "\n",
      "low_cov.to_csv('lo.csv')\n",
      "\n",
      "sum(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'])\n",
      "37/46:\n",
      "hi_cov = full_data[full_data['Taxify: Coverage Ratio (unique)']>=95]\n",
      "low_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']<95) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "\n",
      "sum(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'])\n",
      "37/47:\n",
      "hi_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']>=95) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "low_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']<95) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "\n",
      "sum(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], \n",
      "   hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)'])\n",
      "37/48:\n",
      "hi_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']>=95) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "low_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']<95) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "\n",
      "print(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)'], \n",
      "   hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)'])\n",
      "37/49:\n",
      "hi_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']>=95) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "low_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']<95) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "\n",
      "print(np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)']), \n",
      "   np.mean(hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)']))\n",
      "37/50:\n",
      "hi_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']>=90) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "low_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']<90) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "\n",
      "print(np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)']), \n",
      "   np.mean(hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)']))\n",
      "37/51:\n",
      "hi_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']>=97) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "low_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']<97) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "\n",
      "print(np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)']), \n",
      "   np.mean(hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)']))\n",
      "37/52:\n",
      "hi_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']>=98) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "low_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']<98) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "\n",
      "print(np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)']), \n",
      "   np.mean(hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)']))\n",
      "37/53:\n",
      "hi_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']>=96) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "low_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']<96) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "\n",
      "print(np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)']), \n",
      "   np.mean(hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)']))\n",
      "37/54:\n",
      "hi_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']>95) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "low_cov = full_data[(full_data['Taxify: Coverage Ratio (unique)']<=95) & full_data['Taxify: People saw +1 cars (unique)']>0]\n",
      "\n",
      "print(np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)']), \n",
      "   np.mean(hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)']))\n",
      "37/55:\n",
      "\n",
      "print(np.mean(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)']), \n",
      "   np.mean(hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)']))\n",
      "print(np.median(low_cov['Online (h)']/low_cov['Taxify: People saw +1 cars (unique)']), \n",
      "   np.median(hi_cov['Online (h)']/hi_cov['Taxify: People saw +1 cars (unique)']))\n",
      "37/56:\n",
      "tab = pd.crosstab(driver_activity['date_weekday']\n",
      "                    , driver_activity['date_hour']\n",
      "                    , values = round(driver_activity['Finished Rides'], 1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/57:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides'], 1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/58:\n",
      "print(np.mean(low_cov['Rides per online hour']), \n",
      "   np.mean(hi_cov['Rides per online hour']))\n",
      "print(np.median(low_cov['Rides per online hour']), \n",
      "   np.median(hi_cov['Rides per online hour']))\n",
      "37/59:\n",
      "print('mean for low and high: ', np.mean(low_cov['Rides per online hour']), \n",
      "   np.mean(hi_cov['Rides per online hour']))\n",
      "print('median for low and high: 'np.median(low_cov['Rides per online hour']), \n",
      "   np.median(hi_cov['Rides per online hour']))\n",
      "37/60:\n",
      "print('mean for low and high: ', np.mean(low_cov['Rides per online hour']), \n",
      "   np.mean(hi_cov['Rides per online hour']))\n",
      "print('median for low and high: ', np.median(low_cov['Rides per online hour']), \n",
      "   np.median(hi_cov['Rides per online hour']))\n",
      "37/61: full_data[is_hi_cov] = full_data['Taxify: Coverage Ratio (unique)'].apply(lambda x: 1 if (x>95) else 0)\n",
      "37/62: full_data['is_hi_cov'] = full_data['Taxify: Coverage Ratio (unique)'].apply(lambda x: 1 if (x>95) else 0)\n",
      "37/63: boxpl(hi_cov, 'Rides per online hour', 'is_hi_cov')\n",
      "37/64:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 5 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            n = n + 1\n",
      "37/65: boxpl(hi_cov, 'Rides per online hour', 'is_hi_cov')\n",
      "37/66: sns.boxplot(hi_cov, 'Rides per online hour', 'is_hi_cov')\n",
      "37/67: sns.boxplot(full_data, 'Rides per online hour', 'is_hi_cov')\n",
      "37/68: sns.boxplot(full_data['Rides per online hour'], y=full_data['is_hi_cov'])\n",
      "37/69: full_data['is_hi_cov']\n",
      "37/70: full_data['is_hi_cov'].unique()\n",
      "37/71: sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/72: sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/73: full_data['is_hi_cov'] = full_data['Taxify: Coverage Ratio (unique)'].apply(lambda x: 1 if (x>98) else 0)\n",
      "37/74: full_data['is_hi_cov'].unique()\n",
      "37/75: sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/76:\n",
      "sns.figsize(10, 4)\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/77:\n",
      "figsize(10, 4)\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/78:\n",
      "plt.figsize(10, 4)\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/79:\n",
      "plt.figsize=(10, 4)\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/80:\n",
      "plt.figsize=(6, 4)\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/81:\n",
      "figure = plt.figure(figsize=(8,4))\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/82:\n",
      "figure = plt.figure(figsize=(10,6))\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/83:\n",
      "figure = plt.figure(figsize=(9,7))\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/84:\n",
      "figure = plt.figure(figsize=(7,7))\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/85:\n",
      "figure = plt.figure(figsize=(5,7))\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/86:\n",
      "figure = plt.figure(figsize=(5,6))\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/87:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides']/0.3, 1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/88:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides']/0.3, 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/89:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides'], 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(14,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/90:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides'], 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(14,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/91:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides'], 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(12,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/92:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides'], 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(12,8)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/93:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides'], 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(12,4)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/94:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides'], 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(10,4)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/95:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides'], 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(12,4)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/96: rph_threshhold = 0.3\n",
      "37/97:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides']/rph_threshhold, 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(12,4)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/98:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides']/rph_threshhold, 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(14,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/99:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides']/rph_threshhold, 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(16,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/100:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides']/rph_threshhold, 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/101: This heatmap shows 1) darker colors when it is peak hour and 2) number of online hours needed to fully cover the demand.\n",
      "37/102:\n",
      "figure = plt.figure(figsize=(5,6))\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "37/103:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides'], 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(14,4)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "37/104:\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'inner',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "\n",
      "print(full_data.shape[0], driver_activity.shape[0], overview_search.shape[0])\n",
      "37/105:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "37/106:\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "37/107: driver_activity.head(3)\n",
      "37/108:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13].astype(int)\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "driver_activity.to_csv('exp1.csv')\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "37/109:\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'inner',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "\n",
      "print(full_data.shape[0], driver_activity.shape[0], overview_search.shape[0])\n",
      "37/110: full_data['undersupply']=full_data['Has booking (h)']/full_data['Online (h)']\n",
      "37/111:\n",
      "full_data.head()\n",
      "full_data.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "37/112:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(full_data['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Online (h)':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "\n",
      "\n",
      "#plt.plot( 'x', 'y1', data=df, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
      "#plt.plot( 'x', 'y2', data=df, marker='', color='olive', linewidth=2)\n",
      "#plt.plot( 'x', 'y3', data=df, marker='', color='olive', linewidth=2, linestyle='dashed', label=\"toto\")\n",
      "\n",
      "\n",
      "plt.show()\n",
      "37/113:\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(full_data['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Has booking (h)':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Online (h)':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "\n",
      "plt.show()\n",
      "37/114:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = 1-round((full_data['Online (h)']-full_data['Has booking (h)'])/full_data['Online (h)'],2)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "37/115:\n",
      "full_data['total_demand'] = full_data['Taxify: People saw +1 cars (unique)']+full_data['Taxify: People saw 0 cars (unique)']\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(full_data['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'total_demand':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Active drivers':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "\n",
      "plt.show()\n",
      "37/116:\n",
      "\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(full_data['date_hour'].unique())\n",
      "\n",
      "\n",
      "\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'total_demand':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Active drivers':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "\n",
      "plt.show()\n",
      "37/117:\n",
      "#define plots and hours on the horizontal ticks\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(full_data['date_hour'].unique())\n",
      "# plot 1: demand, hourly average \n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'total_demand':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "# plot 2: supply, hourly average\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Active drivers':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "#let's add legend on the top right corner\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "#show the drawn plot\n",
      "plt.show()\n",
      "37/118:\n",
      "#define plots and hours on the horizontal ticks\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(full_data['date_hour'].unique())\n",
      "# plot 1: demand, hourly average \n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'total_demand':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "# plot 2: supply, hourly average\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Active drivers':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "#let's add legend on the top right corner\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "#show the drawn plot\n",
      "plt.show()\n",
      "37/119:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = 1-round(full_data['total_demand']/full_data['Active drivers'],2)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "37/120:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = 1-round(full_data['total_demand']/full_data['Active drivers'],0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "37/121:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = 1-round(full_data['total_demand']/full_data['Active drivers'],1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,10)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "37/122:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = 1-round(full_data['total_demand']/full_data['Active drivers'],1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "37/123:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = 1-round(full_data['total_demand']/full_data['Active drivers'],1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(16,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "37/124:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['total_demand']/full_data['Active drivers'],1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(16,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "37/125: This plot shows how much more is demand, than supply each weekday and hour on average. Values more than 1 mean, that demand is more than the supply. values less than 1 mean, that demand is less, than supply.\n",
      "37/126:\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'inner',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "\n",
      "print(full_data.shape[0], driver_activity.shape[0], overview_search.shape[0])\n",
      "37/127: full_data.head(3)\n",
      "37/128: full_data['total_demand']\n",
      "37/129:\n",
      "#define plots and hours on the horizontal ticks\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(full_data['date_hour'].unique())\n",
      "# plot 1: demand, hourly average \n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'total_demand':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "# plot 2: supply, hourly average\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Active drivers':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "#let's add legend on the top right corner\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "#show the drawn plot\n",
      "plt.show()\n",
      "37/130: full_data['total_demand'] = full_data['Taxify: People saw +1 cars (unique)']+full_data['Taxify: People saw 0 cars (unique)']\n",
      "37/131:\n",
      "#define plots and hours on the horizontal ticks\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(full_data['date_hour'].unique())\n",
      "# plot 1: demand, hourly average \n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'total_demand':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "# plot 2: supply, hourly average\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Active drivers':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "#let's add legend on the top right corner\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "#show the drawn plot\n",
      "plt.show()\n",
      "37/132: full_data['total_demand']\n",
      "37/133: full_data.sort_values('total_demand', ascending=False)\n",
      "37/134: full_data.sort_values('total_demand', ascending=False).head(36)\n",
      "37/135: top_demand_data = full_data.sort_values('total_demand', ascending=False).head(36)\n",
      "37/136: top_demand_data\n",
      "37/137: top_demand_data.head(3)\n",
      "37/138: top_demand_data.head(3)\n",
      "37/139: top_demand_data.head(1)\n",
      "37/140: top_demand_data.head(2)\n",
      "37/141: top_demand_data['Rides per online hour']*10*0.8\n",
      "37/142: mean(top_demand_data['Rides per online hour']*10*0.8)\n",
      "37/143: np.mean(top_demand_data['Rides per online hour']*10*0.8)\n",
      "37/144: top_demand_data[['Rides per online hour'*10*0.8]]\n",
      "37/145: top_demand_data[['Rides per online hour']*10*0.8]\n",
      "37/146: top_demand_data[['Date'],['Rides per online hour']*10*0.8]\n",
      "37/147: top_demand_data[['Date','Rides per online hour']*10*0.8]\n",
      "37/148: top_demand_data[['Date','Rides per online hour']]\n",
      "37/149: top_demand_data[['Date','Rides per online hour'*10*0.8]]\n",
      "37/150: top_demand_data[['Date','Rides per online hour']]\n",
      "37/151:\n",
      "top_demand_data['hourly_income_after_tax'] = top_demand_data['Rides per online hour']*10*0.8\n",
      "top_demand_data[['Date','hourly_income_after_tax']]\n",
      "37/152: top_demand_data['hourly_income_after_tax'] = top_demand_data['Rides per online hour']*10*0.8\n",
      "37/153:\n",
      "top_demand_data['extra_hours_needed'] = (top_demand_data['Online (h)'] - top_demand_data['Taxify: Coverage Ratio (unique)']) * 100\n",
      "top_demand_data[['Date','hourly_income_after_tax', 'extra_hours_needed']]\n",
      "37/154:\n",
      "top_demand_data['extra_hours_needed'] = (top_demand_data['Online (h)']/top_demand_data['Taxify: Coverage Ratio (unique)']) * 100\n",
      "top_demand_data[['Date','hourly_income_after_tax', 'extra_hours_needed']]\n",
      "37/155:\n",
      "top_demand_data['extra_hours_needed'] = ((top_demand_data['Online (h)']/top_demand_data['Taxify: Coverage Ratio (unique)']) * 100) - top_demand_data['Online (h)'] \n",
      "top_demand_data[['Date','hourly_income_after_tax', 'extra_hours_needed']]\n",
      "38/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "38/2:\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "38/3: driver_activity.head(3)\n",
      "38/4:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13].astype(int)\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "driver_activity.to_csv('exp1.csv')\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "38/5:\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'inner',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "\n",
      "print(full_data.shape[0], driver_activity.shape[0], overview_search.shape[0])\n",
      "38/6: full_data['undersupply']=full_data['Has booking (h)']/full_data['Online (h)']\n",
      "38/7:\n",
      "full_data.head()\n",
      "full_data.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "38/8:\n",
      "i = 0\n",
      "for _week in full_data['date_week'].unique():\n",
      "    iter_subset = full_data[full_data['date_week']==_week][['Date','date_week', 'date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "weekly_undersupply = weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=False)\n",
      "weekly_undersupply.to_csv('weekly_undersupply.csv')\n",
      "38/9: weekly_undersupply\n",
      "38/10: full_data['total_demand'] = full_data['Taxify: People saw +1 cars (unique)']+full_data['Taxify: People saw 0 cars (unique)']\n",
      "38/11:\n",
      "#define plots and hours on the horizontal ticks\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(full_data['date_hour'].unique())\n",
      "# plot 1: demand, hourly average \n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'total_demand':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "# plot 2: supply, hourly average\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Active drivers':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "#let's add legend on the top right corner\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "#show the drawn plot\n",
      "plt.show()\n",
      "38/12:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['total_demand']/full_data['Active drivers'],1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(16,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "38/13:\n",
      "First, merge these two datasets, joining them by the date. <br/>\n",
      "\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'inner',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "39/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "39/2:\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "39/3: driver_activity.head(3)\n",
      "39/4:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13].astype(int)\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "driver_activity.to_csv('exp1.csv')\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "39/5:\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'inner',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "\n",
      "print(full_data.shape[0], driver_activity.shape[0], overview_search.shape[0])\n",
      "39/6: full_data['undersupply']=full_data['Has booking (h)']/full_data['Online (h)']\n",
      "39/7:\n",
      "full_data.head()\n",
      "full_data.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "39/8:\n",
      "i = 0\n",
      "for _week in full_data['date_week'].unique():\n",
      "    iter_subset = full_data[full_data['date_week']==_week][['Date','date_week', 'date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "weekly_undersupply = weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=False)\n",
      "weekly_undersupply.to_csv('weekly_undersupply.csv')\n",
      "39/9: weekly_undersupply\n",
      "39/10: full_data['total_demand'] = full_data['Taxify: People saw +1 cars (unique)']+full_data['Taxify: People saw 0 cars (unique)']\n",
      "39/11:\n",
      "#define plots and hours on the horizontal ticks\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(full_data['date_hour'].unique())\n",
      "# plot 1: demand, hourly average \n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'total_demand':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "# plot 2: supply, hourly average\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Active drivers':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "#let's add legend on the top right corner\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "#show the drawn plot\n",
      "plt.show()\n",
      "39/12:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['total_demand']/full_data['Active drivers'],1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(16,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "39/13:\n",
      "First, merge these two datasets, joining them by the date. <br/>\n",
      "\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'inner',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "39/14:\n",
      "top_demand_data['extra_hours_needed'] = ((top_demand_data['Online (h)']/top_demand_data['Taxify: Coverage Ratio (unique)']) * 100) - top_demand_data['Online (h)'] \n",
      "top_demand_data[['Date','hourly_income_after_tax', 'extra_hours_needed']]\n",
      "40/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "40/2:\n",
      "driver_activity = pd.read_csv('Hourly_DriverActivity_1.csv', sep=',')\n",
      "overview_search = pd.read_csv('Hourly_OverviewSearch_1.csv', sep=',')\n",
      "40/3: driver_activity.head(3)\n",
      "40/4:\n",
      "driver_activity['date_Day']=pd.to_datetime(driver_activity.Date.str[:10])\n",
      "driver_activity['date_weekday'] = pd.to_datetime(driver_activity.Date.str[:10]).apply(lambda x: datetime.strftime(x, '%A'))\n",
      "driver_activity['date_hour']=driver_activity.Date.str[11:13].astype(int)\n",
      "driver_activity['date_week'] = pd.to_datetime(driver_activity.Date.str[:10]).dt.week\n",
      "driver_activity.to_csv('exp1.csv')\n",
      "\n",
      "print(driver_activity['date_week'].unique())\n",
      "40/5:\n",
      "full_data = pd.merge(driver_activity, overview_search, how = 'inner',\n",
      "                                   left_on = ['Date'],\n",
      "                                   right_on = ['Date'])\n",
      "\n",
      "print(full_data.shape[0], driver_activity.shape[0], overview_search.shape[0])\n",
      "40/6: full_data['undersupply']=full_data['Has booking (h)']/full_data['Online (h)']\n",
      "40/7:\n",
      "full_data.head()\n",
      "full_data.groupby(['date_weekday', 'date_hour']).agg({'undersupply':'mean'}).sort_values('undersupply', ascending=False).head(36)\n",
      "40/8:\n",
      "i = 0\n",
      "for _week in full_data['date_week'].unique():\n",
      "    iter_subset = full_data[full_data['date_week']==_week][['Date','date_week', 'date_weekday', 'date_hour','undersupply']].sort_values('undersupply',ascending=False).head(36)\n",
      "    if i==0:\n",
      "        weekly_undersupply = iter_subset \n",
      "    else:\n",
      "        weekly_undersupply = weekly_undersupply.append(iter_subset)\n",
      "    i = i+1\n",
      "weekly_undersupply = weekly_undersupply.sort_values(['date_week', 'undersupply'], ascending=False)\n",
      "weekly_undersupply.to_csv('weekly_undersupply.csv')\n",
      "40/9: weekly_undersupply\n",
      "40/10: full_data['total_demand'] = full_data['Taxify: People saw +1 cars (unique)']+full_data['Taxify: People saw 0 cars (unique)']\n",
      "40/11:\n",
      "#define plots and hours on the horizontal ticks\n",
      "fig, ax = plt.subplots(figsize=(12,6))\n",
      "ax.set_xticks(full_data['date_hour'].unique())\n",
      "# plot 1: demand, hourly average \n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'total_demand':'mean'}),\n",
      "         label='demand',linewidth=4\n",
      "         )\n",
      "# plot 2: supply, hourly average\n",
      "plt.plot(full_data['date_hour'].unique(), \n",
      "         full_data.groupby('date_hour').agg({'Active drivers':'mean'})\n",
      "         ,label='supply', linewidth=4\n",
      "         )\n",
      "#let's add legend on the top right corner\n",
      "first_legend = plt.legend( loc='upper right', fontsize=15)\n",
      "ax = plt.gca().add_artist(first_legend)\n",
      "plt.xlabel('hour of the day', fontsize=15)\n",
      "plt.ylabel('average hour', fontsize=15)\n",
      "#show the drawn plot\n",
      "plt.show()\n",
      "40/12:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['total_demand']/full_data['Active drivers'],1)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(16,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Reds\",fmt='g')\n",
      "40/13: full_data['is_hi_cov'] = full_data['Taxify: Coverage Ratio (unique)'].apply(lambda x: 1 if (x>98) else 0)\n",
      "40/14: full_data['is_hi_cov'].unique()\n",
      "40/15:\n",
      "figure = plt.figure(figsize=(5,6))\n",
      "sns.boxplot(y=full_data['Rides per online hour'], x=full_data['is_hi_cov'])\n",
      "40/16: rph_threshhold = 0.3\n",
      "40/17:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides'], 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(14,4)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "40/18:\n",
      "tab = pd.crosstab(full_data['date_weekday']\n",
      "                    , full_data['date_hour']\n",
      "                    , values = round(full_data['Finished Rides']/rph_threshhold, 0)\n",
      "                    , aggfunc = 'mean')\n",
      "\n",
      "hm=sns.set(rc={'figure.figsize':(18,6)})\n",
      "hm = sns.heatmap(tab, annot=True, cmap=\"Blues\",fmt='g')\n",
      "40/19: top_demand_data = full_data.sort_values('total_demand', ascending=False).head(36)\n",
      "40/20: top_demand_data.head(2)\n",
      "40/21: np.mean(top_demand_data['Rides per online hour']*10*0.8)\n",
      "40/22: top_demand_data['hourly_income_after_tax'] = top_demand_data['Rides per online hour']*10*0.8\n",
      "40/23:\n",
      "top_demand_data['extra_hours_needed'] = ((top_demand_data['Online (h)']/top_demand_data['Taxify: Coverage Ratio (unique)']) * 100) - top_demand_data['Online (h)'] \n",
      "top_demand_data[['Date','hourly_income_after_tax', 'extra_hours_needed']]\n",
      "41/1:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "log_filtered.to_csv('log_filtered.csv')\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "41/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "41/3:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "41/4: changelog.head(3)\n",
      "41/5: issues.head(3)\n",
      "41/6: sprints.head(3)\n",
      "41/7: users.head(3)\n",
      "41/8: changelog.head(3)\n",
      "41/9:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "41/10:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "log_filtered.to_csv('log_filtered.csv')\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "41/11:\n",
      "changelog[changelog['field'].isin(['Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])].to_csv('cols_to_check.csv')\n",
      "41/12:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "41/13:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "41/14: changelog.head(3)\n",
      "41/15: issues.head(3)\n",
      "41/16: sprints.head(3)\n",
      "41/17: users.head(3)\n",
      "41/18: changelog.head(3)\n",
      "41/19:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "41/20:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "log_filtered.to_csv('log_filtered.csv')\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "41/21:\n",
      "changelog[changelog['field'].isin(['Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])].to_csv('cols_to_check.csv')\n",
      "41/22:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "41/23:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "41/24:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "41/25:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove multiple whitespaces (needed for removing 'at at' texts, regex is the next command below)\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\|#)', ' ', str(text))\n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "41/26:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "41/27:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "41/28:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment',\n",
      "                                            'Acceptance Criteria', 'Migration Impact', 'QA Test Plan', 'Out of Scope'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "41/29:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "41/30: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "41/31:\n",
      "#remove outliers\n",
      "log_cols = log_cols.drop([7085, 3635, 3521, 2575, 1740, 2507])\n",
      "41/32: log_cols.to_csv('log_cols.csv')\n",
      "41/33:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "41/34: hist_with_perc(log_cols['textLength'],30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "41/35:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 10% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "41/36:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.05))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 10% to 90%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 90% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 10% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "41/37:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.05))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 5% to 95%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 95% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 5% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "41/38:\n",
      "hist_with_perc(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],\n",
      "               30,'Distribution of the longest 10%  texts lengths in the dataset','text length','Count','lightblue')\n",
      "41/39:\n",
      "hist_with_perc(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],\n",
      "               30,'Distribution of the longest 5%  texts lengths in the dataset','text length','Count','lightblue')\n",
      "41/40:\n",
      "hist_with_perc(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],\n",
      "               30,'Distribution of the shortest 5%  texts lengths in the dataset','text length','Count','aquamarine')\n",
      "41/41:\n",
      "\n",
      "hist_with_perc(log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,\n",
      "               30,'Distribution of the texts between 5%-95% percentile length in the dataset',\n",
      "               'text length','Count','mediumaquamarine')\n",
      "41/42:\n",
      "#detect the number of rows that fall in top 7% and top 3%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.025))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.025))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/43: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "41/44:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "            words_in_text = words_in_text + len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "41/45:\n",
      "hist_with_perc(user_text_combined['words_in_text'],\n",
      "               30,'','word count in text','Count of such occurences','yellow')\n",
      "41/46: user_text_combined[user_text_combined['words_in_text']>600].head()\n",
      "41/47: user_text_combined[user_text_combined['words_in_text']>600].shape[0]\n",
      "41/48: log_filtered[log_filtered['field']=='Comment'].to_csv('orig_comments.csv')\n",
      "41/49:\n",
      "changelog[changelog['field']=='Comment']\n",
      "\n",
      "#log_filtered[log_filtered['field']=='Comment'].to_csv('orig_comments.csv')\n",
      "41/50:\n",
      "comments_orig = changelog[changelog['field']=='Comment']\n",
      "\n",
      "comments_orig['fromString'] = comments_orig['fromString'].apply(removeCodeSnippet)\n",
      "comments_orig['fromString'] = comments_orig['fromString'].apply(lambda x: str.strip(x))\n",
      "\n",
      "comments_orig.to_csv('orig_comments.csv')\n",
      "41/51: comments_orig.head()\n",
      "41/52:\n",
      "comments_orig = changelog[changelog['field']=='Comment']\n",
      "\n",
      "comments_orig['from'] = comments_orig['from'].apply(removeCodeSnippet)\n",
      "comments_orig['from'] = comments_orig['from'].apply(lambda x: str.strip(x))\n",
      "\n",
      "comments_orig.to_csv('orig_comments.csv')\n",
      "41/53: comments_orig.head()\n",
      "41/54:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "\n",
      "\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/55:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "comments_orig.head()\n",
      "\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/56:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "comments_orig.shape[0]\n",
      "\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/57:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "comments_orig.groupby('project', 'key', 'author').count\n",
      "\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/58:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "#comments_orig.groupby('project', 'key', 'author').count\n",
      "\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/59:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "comments_orig.groupby(['project', 'key', 'author']).count\n",
      "\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/60:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "comments_orig.groupby(['project', 'key', 'author']).count()\n",
      "\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/61:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "comments_orig.groupby(['project', 'key', 'author']).count().shape[0]\n",
      "\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/62:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "Print('Unique comments: ', comments_orig.groupby(['project', 'key', 'author']).count().shape[0])\n",
      "\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/63:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "print('Unique comments: ', comments_orig.groupby(['project', 'key', 'author']).count().shape[0])\n",
      "\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/64:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "print('Unique comments: ', comments_orig.groupby(['project', 'key', 'author']).count().shape[0])\n",
      "\n",
      "comments_orig[['project', 'key', 'author']]\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/65:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "print('Unique comments: ', comments_orig.groupby(['project', 'key', 'author']).count().shape[0])\n",
      "\n",
      "comments_orig[['project', 'key', 'author']].drop_duplicates()\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/66:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "print('Unique comments: ', comments_orig.groupby(['project', 'key', 'author']).count().shape[0])\n",
      "\n",
      "comments_orig[['project', 'key', 'author', 'field']].drop_duplicates()\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/67:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "print('Unique comments: ', comments_orig.groupby(['project', 'key', 'author']).count().shape[0])\n",
      "\n",
      "comments_orig[['project', 'key', 'author', 'field', 'from']].drop_duplicates()\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/68:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "print('Unique comments: ', comments_orig.groupby(['project', 'key', 'author']).count().shape[0])\n",
      "\n",
      "comments_orig[['project', 'key', 'author', 'field', 'from', 'created']].drop_duplicates()\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/69:\n",
      "comments_orig = changelog[changelog['field']=='Comment'][cols]\n",
      "\n",
      "print('Unique comments: ', comments_orig.groupby(['project', 'key', 'author']).count().shape[0])\n",
      "\n",
      "comments_orig[['project', 'key', 'author', 'field', 'from']].drop_duplicates()\n",
      "#comments_orig.to_csv('orig_comments.csv')\n",
      "41/70:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "41/71:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "41/72: changelog.head(3)\n",
      "41/73: issues.head(3)\n",
      "41/74: sprints.head(3)\n",
      "41/75: users.head(3)\n",
      "41/76: changelog.head(3)\n",
      "41/77:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "41/78:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "log_filtered.to_csv('log_filtered.csv')\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "41/79:\n",
      "changelog[changelog['field'].isin(['Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])].to_csv('cols_to_check.csv')\n",
      "41/80:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field', 'author'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "41/81:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "41/82:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "41/83:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove multiple whitespaces (needed for removing 'at at' texts, regex is the next command below)\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\|#)', ' ', str(text))\n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "41/84:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "41/85:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "41/86:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment',\n",
      "                                            'Acceptance Criteria', 'Migration Impact', 'QA Test Plan', 'Out of Scope'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "41/87:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment',\n",
      "                                            'Acceptance Criteria', 'Migration Impact', 'QA Test Plan', 'Out of Scope'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "41/88: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "41/89:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "41/90: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "41/91: log_cols.sort_values('textLength', ascending=False).head(10).to_csv('long_txt')\n",
      "41/92: log_cols.sort_values('textLength', ascending=False).head(10).to_csv('long_txt.csv')\n",
      "41/93: log_cols.sort_values('textLength', ascending=False).head(10)\n",
      "41/94: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "41/95:\n",
      "#remove outliers\n",
      "# I have checked them manually, \n",
      "#these are the ones that the text cleaning functions could not properly clean and contain mostly the code snippet or logs\n",
      "log_cols = log_cols.drop([3923, 1861, 3801, 2763, 7794, 2157])\n",
      "41/96: log_cols.to_csv('log_cols.csv')\n",
      "41/97:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "41/98: hist_with_perc(log_cols['textLength'],30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "41/99:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.05))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 5% to 95%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 95% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 5% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/100:\n",
      "hist_with_perc(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],\n",
      "               30,'Distribution of the longest 5%  texts lengths in the dataset','text length','Count','lightblue')\n",
      "41/101:\n",
      "hist_with_perc(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],\n",
      "               30,'Distribution of the shortest 5%  texts lengths in the dataset','text length','Count','aquamarine')\n",
      "41/102:\n",
      "\n",
      "hist_with_perc(log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,\n",
      "               30,'Distribution of the texts between 5%-95% percentile length in the dataset',\n",
      "               'text length','Count','mediumaquamarine')\n",
      "41/103:\n",
      "#detect the number of rows that fall in top 3% and bottom 2%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.02))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/104:\n",
      "#detect the number of rows that fall in top 3% and bottom 2%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.03))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/105:\n",
      "#detect the number of rows that fall in top 3% and bottom 2%\n",
      "cutoff_7percent_top = round((int(log_cols.shape[0]) * 0.025))\n",
      "cutoff_3percent_top = round((int(log_cols.shape[0]) * 0.025))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_7percent_top))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_3percent_top))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_7percent_top).iloc[cutoff_7percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_3percent_top).iloc[cutoff_3percent_top-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/106:\n",
      "#detect the number of rows that fall in top 2.5% and bottom 2.5%\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * 0.025))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * 0.025))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/107: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "41/108:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "            words_in_text = words_in_text + len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "41/109:\n",
      "hist_with_perc(user_text_combined['words_in_text'],\n",
      "               30,'','word count in text','Count of such occurences','yellow')\n",
      "41/110: user_text_combined[user_text_combined['words_in_text']>600].shape[0]\n",
      "41/111:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "41/112:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "41/113: changelog.head(3)\n",
      "41/114: issues.head(3)\n",
      "41/115: sprints.head(3)\n",
      "41/116: users.head(3)\n",
      "41/117: changelog.head(3)\n",
      "41/118:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "41/119:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "log_filtered.to_csv('log_filtered.csv')\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "41/120:\n",
      "changelog[changelog['field'].isin(['Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])].to_csv('cols_to_check.csv')\n",
      "41/121:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field', 'author'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "41/122:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "41/123:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "41/124:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove multiple whitespaces (needed for removing 'at at' texts, regex is the next command below)\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\|#)', ' ', str(text))\n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "41/125:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "41/126:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "41/127:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment',\n",
      "                                            'Acceptance Criteria', 'Migration Impact', 'QA Test Plan', 'Out of Scope'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "41/128:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "41/129: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "41/130: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "41/131:\n",
      "#remove outliers\n",
      "# I have checked them manually, \n",
      "#these are the ones that the text cleaning functions could not properly clean and contain mostly the code snippet or logs\n",
      "#log_cols = log_cols.drop([3923, 1861, 3801, 2763, 7794, 2157])\n",
      "41/132: log_cols.to_csv('log_cols.csv')\n",
      "41/133:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "41/134: hist_with_perc(log_cols['textLength'],30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "41/135:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.05))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 5% to 95%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 95% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 5% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "41/136:\n",
      "hist_with_perc(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],\n",
      "               30,'Distribution of the longest 5%  texts lengths in the dataset','text length','Count','lightblue')\n",
      "41/137:\n",
      "hist_with_perc(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],\n",
      "               30,'Distribution of the shortest 5%  texts lengths in the dataset','text length','Count','aquamarine')\n",
      "41/138:\n",
      "\n",
      "hist_with_perc(log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,\n",
      "               30,'Distribution of the texts between 5%-95% percentile length in the dataset',\n",
      "               'text length','Count','mediumaquamarine')\n",
      "41/139:\n",
      "#detect the number of rows that fall in top 2.5% and bottom 2.5%\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * 0.01))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * 0.01))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/140:\n",
      "#detect the number of rows that fall in top 2.5% and bottom 2.5%\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * 0.01))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * 0.02))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of 2.5 % of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/141: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "41/142:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "            words_in_text = words_in_text + len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "41/143:\n",
      "hist_with_perc(user_text_combined['words_in_text'],\n",
      "               30,'','word count in text','Count of such occurences','yellow')\n",
      "41/144: user_text_combined[user_text_combined['words_in_text']>600].shape[0]\n",
      "41/145:\n",
      "#detect the number of rows that fall in top % and bottom %\n",
      "top= 0.01\n",
      "bottom = 0.02\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of '+str(top)+'% of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of '+str(bottom)+'% of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/146:\n",
      "#detect the number of rows that fall in top % and bottom %\n",
      "top= 0.01\n",
      "bottom = 0.02\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of top '+str(top)+'% of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of bottom '+str(bottom)+'% of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/147:\n",
      "#detect the number of rows that fall in top % and bottom %\n",
      "top= 0.01\n",
      "bottom = 0.02\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of top '+str(top*100)+'% of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of bottom '+str(bottom*100)+'% of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/148:\n",
      "#detect the number of rows that fall in top % and bottom %\n",
      "top= 0.01\n",
      "bottom = 0.02\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of top '+str(round(top*100, 0)+'% of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of bottom '+str(round(bottom*100, 0)+'% of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/149:\n",
      "#detect the number of rows that fall in top % and bottom %\n",
      "top= 0.01\n",
      "bottom = 0.02\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of top '+str(round(top*100, 0))+'% of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of bottom '+str(round(bottom*100, 0))+'% of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/150:\n",
      "#detect the number of rows that fall in top % and bottom %\n",
      "top= 0.01\n",
      "bottom = 0.02\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of top '+str(round(top*100))+'% of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of bottom '+str(round(bottom*100))+'% of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/151:\n",
      "#detect the number of rows that fall in top % and bottom %\n",
      "top= 0.01\n",
      "bottom = 0.02\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of top '+str(round(top*100))+' % of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of bottom '+str(round(bottom*100))+' % of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/152:\n",
      "#detect the number of rows that fall in top % and bottom %\n",
      "top= 0.01\n",
      "bottom = 0.02\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of top '+str(round(top*100))+'% of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of bottom '+str(round(bottom*100))+'% of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "41/153: texts =['Email: test subject here', 'Email RE: Test Subject here', 'Email Re[4]:Test Subject here']\n",
      "41/154: texts\n",
      "41/155: re.search(\"Email.*\", texts)\n",
      "41/156: re.search('Email.*', texts)\n",
      "41/157: texts.apply(re.search('Email.*', texts))\n",
      "41/158: texts.apply(re.search('Email.*'))\n",
      "41/159: texts\n",
      "41/160:\n",
      "for txt in texts:\n",
      "    print(re.search('Email.*', txt))\n",
      "41/161:\n",
      "for txt in texts:\n",
      "    print(re.search('Email*', txt))\n",
      "41/162:\n",
      "for txt in texts:\n",
      "    print(re.search('Email', txt))\n",
      "41/163:\n",
      "for txt in texts:\n",
      "    print(re.search('asdas', txt))\n",
      "41/164:\n",
      "for txt in texts:\n",
      "    print(re.search('Email*', txt))\n",
      "41/165:\n",
      "for txt in texts:\n",
      "    re_parts = re.search('(r|R)(e|E)|\\[[0-9]\\]', txt)\n",
      "    print(re_parts)\n",
      "41/166:\n",
      "for txt in texts:\n",
      "    re_parts = re.search('((email|Email)\\s(r|R)(e|E))(:|\\[[0-9]\\]:)', txt)\n",
      "    print(re_parts)\n",
      "41/167:\n",
      "for txt in texts:\n",
      "    re_parts = re.search('(email|Email)\\s(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    print(re_parts)\n",
      "41/168:\n",
      "for txt in texts:\n",
      "    re_parts = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    print(re_parts)\n",
      "41/169:\n",
      "for txt in texts:\n",
      "    re_parts = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    re_parts = re.replace('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt, '')\n",
      "    print(re_parts)\n",
      "41/170:\n",
      "for txt in texts:\n",
      "    re_parts = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    re_parts = re.sub('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt, '')\n",
      "    print(re_parts)\n",
      "41/171:\n",
      "for txt in texts:\n",
      "    re_parts = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    re_parts = re.sub('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt, '')\n",
      "    print(re_parts)\n",
      "41/172: re_parts\n",
      "41/173:\n",
      "for txt in texts:\n",
      "    re_parts = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    #re_parts = re.sub('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt, '')\n",
      "    print(re_parts)\n",
      "41/174:\n",
      "for txt in texts:\n",
      "    re_parts = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    re_parts = re.sub('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', '', txt)\n",
      "    print(re_parts)\n",
      "41/175:\n",
      "for txt in texts:\n",
      "    re_parts = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    re_parts = re.sub('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', '', txt)\n",
      "    print(re_parts)\n",
      "41/176:\n",
      "for txt in texts:\n",
      "    #re_parts = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    re_parts = re.sub('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', '', txt)\n",
      "    re_parts = re_parts.str.strip()\n",
      "    print(re_parts)\n",
      "41/177:\n",
      "for txt in texts:\n",
      "    #re_parts = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    re_parts = re.sub('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', '', txt)\n",
      "    re_parts = re_parts.strip()\n",
      "    print(re_parts)\n",
      "41/178:\n",
      "# Detect the subejct by removing 'email', 're' and 're[0]' parts of the texts \n",
      "for txt in texts:\n",
      "    re_parts = re.sub('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', '', txt)\n",
      "    re_parts = re_parts.strip()\n",
      "    print(re_parts)\n",
      "41/179:\n",
      "for txt in texts:\n",
      "    re_parts = re.sub('(r|R)(e|E)|(\\[[0-9]\\])', '', txt)\n",
      "    re_parts = re_parts.strip()\n",
      "    print(re_parts)\n",
      "41/180:\n",
      "for txt in texts:\n",
      "    re_parts = re.group('(r|R)(e|E)|(\\[[0-9]\\])', txt)\n",
      "    re_parts = re_parts.strip()\n",
      "    print(re_parts)\n",
      "41/181:\n",
      "for txt in texts:\n",
      "    re_parts = re.match('(r|R)(e|E)|(\\[[0-9]\\])', txt)\n",
      "    print(re_parts)\n",
      "41/182:\n",
      "for txt in texts:\n",
      "    re_parts = txt.match('(r|R)(e|E)|(\\[[0-9]\\])')\n",
      "    print(re_parts)\n",
      "41/183:\n",
      "for txt in texts:\n",
      "    re_parts = txt.re.match('(r|R)(e|E)|(\\[[0-9]\\])')\n",
      "    print(re_parts)\n",
      "41/184:\n",
      "for txt in texts:\n",
      "    re_parts = re.match('(r|R)(e|E)|(\\[[0-9]\\])', txt)\n",
      "    print(re_parts)\n",
      "41/185:\n",
      "for txt in texts:\n",
      "    re_parts = re.match('(r|R)(e|E)|(\\[[0-9]\\])', txt, flags=0)\n",
      "    print(re_parts)\n",
      "41/186:\n",
      "for txt in texts:\n",
      "    p = re.compile('(r|R)(e|E)|(\\[[0-9]\\])')\n",
      "    m = p.match(txt)\n",
      "    if m:\n",
      "        print('Match found: ', m.group())\n",
      "    else:\n",
      "        print('No match')\n",
      "41/187:\n",
      "for txt in texts:\n",
      "    p = re.compile('(r|R)(e|E)')\n",
      "    m = p.match(txt)\n",
      "    if m:\n",
      "        print('Match found: ', m.group())\n",
      "    else:\n",
      "        print('No match')\n",
      "41/188: texts\n",
      "41/189:\n",
      "for txt in texts:\n",
      "    p = re.compile('(r|R)(e|E)|(\\[[0-9]\\])')\n",
      "    m = p.match(txt)\n",
      "    if m:\n",
      "        print('Match found: ', m.group())\n",
      "    else:\n",
      "        print('No match')\n",
      "41/190:\n",
      "for txt in texts:\n",
      "    p = re.compile('(r|R)(e|E)|(\\[[0-9]\\])')\n",
      "    m = p.match(txt)\n",
      "    if m:\n",
      "        print('Match found: ', m.group())\n",
      "    else:\n",
      "        print('No match')\n",
      "41/191:\n",
      "for txt in texts:\n",
      "    p = re.compile('(r|R)(e|E)')\n",
      "    m = p.match(txt)\n",
      "    if m:\n",
      "        print('Match found: ', m.group())\n",
      "    else:\n",
      "        print('No match')\n",
      "41/192:\n",
      "for txt in texts:\n",
      "    p = re.compile('re')\n",
      "    m = p.match(txt)\n",
      "    if m:\n",
      "        print('Match found: ', m.group())\n",
      "    else:\n",
      "        print('No match')\n",
      "41/193:\n",
      "for txt in texts:\n",
      "    p = re.compile('re\\S+')\n",
      "    m = p.match(txt)\n",
      "    if m:\n",
      "        print('Match found: ', m.group())\n",
      "    else:\n",
      "        print('No match')\n",
      "41/194:\n",
      "for txt in texts:\n",
      "    p = re.compile(r'(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)')\n",
      "    p.findall(txt)\n",
      "41/195:\n",
      "for txt in texts:\n",
      "    p = re.compile(r'(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)')\n",
      "    p.findall(txt)\n",
      "41/196:\n",
      "for txt in texts:\n",
      "    p = re.compile('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)')\n",
      "    p.findall(txt)\n",
      "41/197:\n",
      "for txt in texts:\n",
      "    p = re.compile('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)')\n",
      "    p.match(txt)\n",
      "41/198:\n",
      "for txt in texts:\n",
      "    p = re.compile('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)')\n",
      "    p.group(txt)\n",
      "41/199:\n",
      "for txt in texts:\n",
      "    p = re.findall('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "41/200:\n",
      "for txt in texts:\n",
      "    p = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "41/201:\n",
      "for txt in texts:\n",
      "    p = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    print(p)\n",
      "41/202:\n",
      "for txt in texts:\n",
      "    p = re.match('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    print(p)\n",
      "41/203:\n",
      "for txt in texts:\n",
      "    p = re.findall('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    print(p)\n",
      "41/204:\n",
      "for txt in texts:\n",
      "    p = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    print(p)\n",
      "41/205:\n",
      "for txt in texts:\n",
      "    p = re.search('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', txt)\n",
      "    print(p.group())\n",
      "41/206:\n",
      "for txt in texts:\n",
      "    p = re.search('(r|R)(e|E)|(\\[[0-9]\\])', txt)\n",
      "    print(p.group())\n",
      "41/207:\n",
      "for txt in texts:\n",
      "    re_with_numb = re.search('\\s(r|R)(e|E)((\\[[0-9]\\])|)', txt)\n",
      "    print(p.group())\n",
      "41/208: texts\n",
      "41/209:\n",
      "for txt in texts:\n",
      "    re_with_numb = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    print(p.group())\n",
      "41/210:\n",
      "# Detect the subejct by removing 'email', 're' and 're[0]' parts of the texts \n",
      "for txt in texts:\n",
      "    re_parts = re.sub('(r|R)(e|E)(\\[[0-9]\\])', '', txt)\n",
      "    re_parts = re_parts.strip()\n",
      "    print(re_parts)\n",
      "41/211:\n",
      "for txt in texts:\n",
      "    re_with_numb = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    print(re_with_numb.group())\n",
      "41/212:\n",
      "for txt in texts:\n",
      "    re_with_numb = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    print(re_with_numb)\n",
      "41/213:\n",
      "for txt in texts:\n",
      "    re_with_numb = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    print(re_with_numb.group())\n",
      "41/214:\n",
      "for txt in texts:\n",
      "    re_with_numb = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if re_with_numb==None:\n",
      "        'it is empty'\n",
      "    else:\n",
      "        print(re_with_numb.group())\n",
      "41/215:\n",
      "for txt in texts:\n",
      "    re_with_numb = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if re_with_numb.isNone:\n",
      "        'it is empty'\n",
      "    else:\n",
      "        print(re_with_numb.group())\n",
      "41/216:\n",
      "for txt in texts:\n",
      "    re_with_numb = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if re_with_numb:\n",
      "        'it is empty'\n",
      "    else:\n",
      "        print(re_with_numb.group())\n",
      "41/217:\n",
      "for txt in texts:\n",
      "    re_with_numb = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if re_with_numb is None:\n",
      "        'it is empty'\n",
      "    else:\n",
      "        print(re_with_numb.group())\n",
      "41/218:\n",
      "for txt in texts:\n",
      "    re_with_numb = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if re_with_numb\n",
      "        'it is empty'\n",
      "    else:\n",
      "        print(re_with_numb.group())\n",
      "41/219:\n",
      "for txt in texts:\n",
      "    re_with_numb = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if re_with_numb:\n",
      "        'it is empty'\n",
      "    else:\n",
      "        print(re_with_numb.group())\n",
      "41/220:\n",
      "for txt in texts:\n",
      "    re_with_numb = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_numb is not None):\n",
      "        print(re_with_numb.group())\n",
      "    else:\n",
      "        print('None')\n",
      "41/221:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        print(re_with_num.group())\n",
      "    else:\n",
      "41/222:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        print(re_with_num.group())\n",
      "    else:\n",
      "        ''\n",
      "41/223:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        print(re_with_num.group())\n",
      "    else:\n",
      "        re_with_num = re.search('(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            print(re_with_num.group())\n",
      "        else:\n",
      "            print ('0')\n",
      "41/224:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        print(re_with_num.group())\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            print(re_with_num.group())\n",
      "        else:\n",
      "            print ('0')\n",
      "41/225:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        print(re_with_num.group())\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            print(re_with_num.group())\n",
      "        else:\n",
      "            print ('0')\n",
      "41/226:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        nr_of_emails_txt = re.search('[0-9]', re_with_num)\n",
      "        re_nr = nr_of_emails_txt.group()\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            re_nr = 1\n",
      "        else:\n",
      "            re_nr = 2\n",
      "41/227:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        re_nr = re.search('[0-9]', nr_of_emails_txt.group())\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            re_nr = 1\n",
      "        else:\n",
      "            re_nr = 2\n",
      "41/228:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        re_nr = re.search('[0-9]', re_with_num.group())\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            re_nr = 1\n",
      "        else:\n",
      "            re_nr = 2\n",
      "    print(re_nr)\n",
      "41/229:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        re_nr = re.search('[0-9]', re_with_num.group()).group()\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            re_nr = 1\n",
      "        else:\n",
      "            re_nr = 2\n",
      "    print(re_nr)\n",
      "41/230:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        re_nr = re.search('[0-9]', re_with_num.group()).group()\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            re_nr = 1\n",
      "        else:\n",
      "            re_nr = 2\n",
      "    print(txt, re_nr)\n",
      "41/231:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        re_nr = re.search('[0-9]', re_with_num.group()).group()\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            re_nr = 1\n",
      "        else:\n",
      "            re_nr = 0\n",
      "    print(txt, re_nr)\n",
      "41/232: texts =['Email: test subject here', 'Email RE: Test Subject here', 'Email Re[4]:Test Subject here']\n",
      "41/233: texts\n",
      "41/234:\n",
      "# Detect the subejct by removing 'email', 're' and 're[0]' parts of the texts \n",
      "for txt in texts:\n",
      "    re_parts = re.sub('(email|Email)(:|\\s)(((|(r|R)(e|E))(:|\\[[0-9]\\]:))|)', '', txt)\n",
      "    re_parts = re_parts.strip()\n",
      "    print(re_parts)\n",
      "41/235:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        re_nr = re.search('[0-9]', re_with_num.group()).group()\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            re_nr = 1\n",
      "        else:\n",
      "            re_nr = 0\n",
      "    print(txt, re_nr)\n",
      "41/236:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        re_nr = re.search('[0-9]', re_with_num.group()).group() + 1\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            re_nr = 2\n",
      "        else:\n",
      "            re_nr = 1\n",
      "    print(txt, re_nr)\n",
      "41/237:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        re_nr = re.search('[0-9]', re_with_num.group()).group()\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            re_nr = 1\n",
      "        else:\n",
      "            re_nr = 0\n",
      "    print(txt, re_nr+1)\n",
      "41/238:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        re_nr = re.search('[0-9]', re_with_num.group()).group()\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            re_nr = 1\n",
      "        else:\n",
      "            re_nr = 0\n",
      "    print(txt, int(re_nr)+1)\n",
      "41/239:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        re_nr = re.search('[0-9]', re_with_num.group()).group()\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            re_nr = 1\n",
      "        else:\n",
      "            re_nr = 0\n",
      "    print(txt, int(re_nr)+1)\n",
      "41/240:\n",
      "\n",
      "\n",
      "len(texts.split())\n",
      "41/241:\n",
      "for txt in texts:\n",
      "    len(txt.split())\n",
      "41/242:\n",
      "for txt in texts:\n",
      "    words_cnt = len(txt.split())\n",
      "    print(words_cnt)\n",
      "41/243: 'first second thierd fourth'.split()\n",
      "41/244:\n",
      "myword = 'Email'\n",
      "nr = 0\n",
      "for txt in texts:\n",
      "    words = txt.split()\n",
      "    for word in words:\n",
      "        if word==myword:\n",
      "            nr = nr+1\n",
      "        else:\n",
      "            nr=nr    \n",
      "    \n",
      "print(myword, nr)\n",
      "41/245:\n",
      "myword = 'Email'\n",
      "nr = 0\n",
      "for txt in texts:\n",
      "    words = txt.split()\n",
      "    for word in words:\n",
      "        print(word)\n",
      "        if word==myword:\n",
      "            nr = nr+1\n",
      "        else:\n",
      "            nr=nr    \n",
      "    \n",
      "print(myword, nr)\n",
      "41/246: re.search('(r|R)(e|E)(\\[[0-9]\\])',  'Email Re[4]:Test Subject here')\n",
      "41/247:\n",
      "srch = re.search('(r|R)(e|E)(\\[[0-9]\\])',  'Email Re[4]:Test Subject here')\n",
      "srch.group()\n",
      "41/248: srch = re.search('(r|R)(e|E)(\\[[0-9]\\])',  'Email Re[4]:Test Subject here')\n",
      "41/249: re.search('(r|R)(e|E)(\\[[0-9]\\])',  'Email Re[4]:Test Subject here')\n",
      "41/250: re.search('(r|R)(e|E)(\\[[0-9]\\])',  'Email R]:Test Subject here')\n",
      "41/251: re.search('(r|R)(e|E)(\\[[0-9]\\])',  'Email R]:Test Subject here')\n",
      "41/252: re.search('(r|R)(e|E)(\\[[0-9]\\])',  'Email R]:Test Subject here')\n",
      "41/253: re.search('(r|R)(e|E)(\\[[0-9]\\])',  'Email Re[4]:Test Subject here')\n",
      "41/254:\n",
      "for txt in texts:\n",
      "    re_with_num = re.search('(r|R)(e|E)(\\[[0-9]\\])', txt)\n",
      "    if (re_with_num is not None):\n",
      "        re_nr = re.search('[0-9]', re_with_num.group()).group()\n",
      "    else:\n",
      "        re_with_num = re.search('(email|Email)\\s(r|R)(e|E)', txt)\n",
      "        if (re_with_num is not None):\n",
      "            re_nr = 1\n",
      "        else:\n",
      "            re_nr = 0\n",
      "    print(txt, int(re_nr)+1)\n",
      "42/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "42/2:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "42/3: changelog.head(3)\n",
      "42/4: issues.head(3)\n",
      "42/5: sprints.head(3)\n",
      "42/6: users.head(3)\n",
      "42/7: changelog.head(3)\n",
      "42/8:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "42/9:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "log_filtered.to_csv('log_filtered.csv')\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "42/10:\n",
      "changelog[changelog['field'].isin(['Acceptance Criteria', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])].to_csv('cols_to_check.csv')\n",
      "42/11:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field', 'author'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "42/12:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "42/13:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "42/14:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove multiple whitespaces (needed for removing 'at at' texts, regex is the next command below)\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\|#)', ' ', str(text))\n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "42/15:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "42/16:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "42/17:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment',\n",
      "                                            'Acceptance Criteria', 'Migration Impact', 'QA Test Plan', 'Out of Scope'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "\n",
      "descriptions.to_csv('descriptions.csv')\n",
      "summaries.to_csv('summaries.csv')\n",
      "comments.to_csv('comments.csv')\n",
      "#descriptions.loc[47].toString\n",
      "42/18:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "42/19: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "42/20:\n",
      "#remove outliers\n",
      "# I have checked them manually, \n",
      "#these are the ones that the text cleaning functions could not properly clean and contain mostly the code snippet or logs\n",
      "#log_cols = log_cols.drop([3923, 1861, 3801, 2763, 7794, 2157])\n",
      "42/21: log_cols.to_csv('log_cols.csv')\n",
      "42/22:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "42/23: hist_with_perc(log_cols['textLength'],30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "42/24:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.05))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 5% to 95%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 95% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 5% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "42/25:\n",
      "hist_with_perc(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],\n",
      "               30,'Distribution of the longest 5%  texts lengths in the dataset','text length','Count','lightblue')\n",
      "42/26:\n",
      "hist_with_perc(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],\n",
      "               30,'Distribution of the shortest 5%  texts lengths in the dataset','text length','Count','aquamarine')\n",
      "42/27:\n",
      "\n",
      "hist_with_perc(log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,\n",
      "               30,'Distribution of the texts between 5%-95% percentile length in the dataset',\n",
      "               'text length','Count','mediumaquamarine')\n",
      "42/28:\n",
      "#detect the number of rows that fall in top % and bottom %\n",
      "top= 0.01\n",
      "bottom = 0.02\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of top '+str(round(top*100))+'% of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of bottom '+str(round(bottom*100))+'% of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "42/29: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "42/30:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "            words_in_text = words_in_text + len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "42/31:\n",
      "hist_with_perc(user_text_combined['words_in_text'],\n",
      "               30,'','word count in text','Count of such occurences','yellow')\n",
      "42/32: user_text_combined[user_text_combined['words_in_text']>600].shape[0]\n",
      "42/33: user_text_combined[user_text_combined['words_in_text']>600].shape[0]\n",
      "42/34: users.head()\n",
      "42/35:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "42/36:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails.groupby(['user','emailAddress']).count()\n",
      "42/37: users_df[pd.isnull(users_df.emailAddress)==True]\n",
      "42/38: user_texts_emails\n",
      "42/39:\n",
      "user_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails.groupby(['user','emailAddress']).count().to_csv('users_per_email.csv')\n",
      "42/40: user_texts_emails\n",
      "42/41:\n",
      "user_texts_emails = pd.merge(user_text_combined[user_text_combined['words_in_text']>600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails.groupby(['user','emailAddress']).count().to_csv('users_per_email.csv')\n",
      "42/42: user_text_combined[user_text_combined['words_in_text']>=600].shape[0]\n",
      "42/43:\n",
      "user_texts_emails = pd.merge(user_text_combined[user_text_combined['words_in_text']>600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails.groupby(['user','emailAddress']).count()\n",
      "42/44:\n",
      "user_texts_emails = pd.merge(user_text_combined[user_text_combined['words_in_text']>600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails.groupby(['user','emailAddress']).count()\n",
      "\n",
      "user_texts_emails.groupby(['user','emailAddress', 'project']).count()\n",
      "42/45:\n",
      "user_texts_emails = pd.merge(user_text_combined[user_text_combined['words_in_text']>600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails.groupby(['user','emailAddress']).count()\n",
      "\n",
      "user_texts_emails.groupby(['project']).count()\n",
      "42/46:\n",
      "user_texts_emails = pd.merge(user_text_combined[user_text_combined['words_in_text']>600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails.groupby[['user','emailAddress']].drop_duplicates().unique()\n",
      "\n",
      "#user_texts_emails.groupby(['project']).count()\n",
      "42/47:\n",
      "user_texts_emails = pd.merge(user_text_combined[user_text_combined['words_in_text']>600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails.groupby[['user','emailAddress']].drop_duplicates()\n",
      "\n",
      "#user_texts_emails.groupby(['project']).count()\n",
      "42/48:\n",
      "user_texts_emails = pd.merge(user_text_combined[user_text_combined['words_in_text']>600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates()\n",
      "\n",
      "#user_texts_emails.groupby(['project']).count()\n",
      "42/49:\n",
      "user_texts_emails = pd.merge(user_text_combined[user_text_combined['words_in_text']>600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails[['user','emailAddress']]\n",
      "\n",
      "#user_texts_emails.groupby(['project']).count()\n",
      "42/50:\n",
      "user_texts_emails = pd.merge(user_text_combined[user_text_combined['words_in_text']>600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().unique()\n",
      "\n",
      "#user_texts_emails.groupby(['project']).count()\n",
      "42/51:\n",
      "user_texts_emails = pd.merge(user_text_combined[user_text_combined['words_in_text']>600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates()\n",
      "\n",
      "#user_texts_emails.groupby(['project']).count()\n",
      "42/52:\n",
      "user_texts_emails = pd.merge(user_text_combined[user_text_combined['words_in_text']>600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "42/53:\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates()\n",
      "42/54:\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates()\n",
      "42/55:\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates()\n",
      "42/56:\n",
      "user_texts_emails[pd.isnull(user_texts_emails.emailAddress)==True]['emailAddress'] = '-'\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates()\n",
      "42/57:\n",
      "user_texts_emails[pd.isnull(user_texts_emails.emailAddress)==True]['emailAddress'] = '-'\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().head()\n",
      "42/58: user_texts_emails.loc[pd.isnull(user_texts_emails.emailAddress)==True,'emailAddress']\n",
      "42/59:\n",
      "user_texts_emails.loc[pd.isnull(user_texts_emails.emailAddress)==True,'emailAddress'] = '-'\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().head()\n",
      "42/60:\n",
      "user_texts_emails.loc[pd.isnull(user_texts_emails.emailAddress)==True,'emailAddress'] = '-'\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates()\n",
      "42/61:\n",
      "user_texts_emails.loc[pd.isnull(user_texts_emails.emailAddress)==True,'emailAddress'] = '-'\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().shape[]\n",
      "42/62:\n",
      "user_texts_emails.loc[pd.isnull(user_texts_emails.emailAddress)==True,'emailAddress'] = '-'\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().shape[0]\n",
      "42/63:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails.groupby(['project']).count()\n",
      "42/64: users.head()\n",
      "42/65:\n",
      "user_texts_emails = pd.merge(user_text_combined[user_text_combined['words_in_text']>=600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "42/66:\n",
      "user_texts_emails.loc[pd.isnull(user_texts_emails.emailAddress)==True,'emailAddress'] = '-'\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "print(user_texts_emails[['user','emailAddress']].drop_duplicates().shape[0])\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates()\n",
      "42/67:\n",
      "user_texts_emails.loc[pd.isnull(user_texts_emails.emailAddress)==True,'emailAddress'] = '-'\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "print('Total number of unique users invalid user texts dataset: ', user_texts_emails[['user','emailAddress']].drop_duplicates().shape[0])\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates()\n",
      "42/68:\n",
      "user_texts_emails.loc[pd.isnull(user_texts_emails.emailAddress)==True,'emailAddress'] = '-'\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "print('Total number of unique users invalid user texts dataset: ', user_texts_emails[['user','emailAddress']].drop_duplicates().shape[0])\n",
      "user_texts_emails[['user','emailAddress']].drop_duplicates().head(3)\n",
      "42/69:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails.groupby(['project']).count()\n",
      "42/70:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails.groupby(['project']).count()['project', 'user']\n",
      "42/71:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails[['project', 'user']].groupby(['project']).count()\n",
      "42/72:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails[['project', 'user']].groupby(['project']).count().sort_values('user')\n",
      "42/73:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=-False)\n",
      "42/74:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=False)\n",
      "42/75:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails[['project', 'user']].groupby(['project']).count().sort_values('user', -ascending=False)\n",
      "42/76:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=False)\n",
      "42/77:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=-1*False)\n",
      "42/78:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=1*False)\n",
      "42/79:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=1/False)\n",
      "42/80:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=False)\n",
      "42/81:\n",
      "user_texts_emails.groupby(['project']).count().to_csv('project_users.csv')\n",
      "user_texts_emails[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=False)\n",
      "42/82: from big5_personality.personality_insights import big5_personality\n",
      "42/83: users_df[pd.isnull(users_df.emailAddress)==True].head()\n",
      "42/84: user_text_combined.head()\n",
      "42/85:\n",
      "user_all_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "42/86: user_all_texts_emails.head()\n",
      "42/87: user_all_texts_emails[['user','emailAddress']].drop_duplicates().head(3)\n",
      "42/88: user_all_texts_emails[['user','emailAddress']].drop_duplicates().shape[0]\n",
      "42/89: user_all_texts_emails[['user','emailAddress']].shape[0]\n",
      "42/90: user_all_texts_emails[['emailAddress']].shape[0]\n",
      "42/91: user_all_texts_emails[['emailAddress']].drop_duplicates().shape[0]\n",
      "42/92: user_all_texts_emails[['emailAddress']].unique()\n",
      "42/93: user_all_texts_emails['emailAddress'].unique()\n",
      "42/94:\n",
      "#user_all_texts_emails['emailAddress'] = \n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot org', '.org')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot io', '.io')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot me', '.me')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot com', '.com')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot fr', '.fr')\n",
      "\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot ', '.')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' at ', '@')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' ', '')\n",
      "\n",
      "user_all_texts_emails['emailAddress'].unique()\n",
      "42/95:\n",
      "#user_all_texts_emails['emailAddress'] = \n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot org', '.org')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot io', '.io')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot me', '.me')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot com', '.com')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot fr', '.fr')\n",
      "\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot ', '.')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' at ', '@')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' ', '')\n",
      "\n",
      "user_all_texts_emails['emailAddress'].unique().shape[0]\n",
      "42/96:\n",
      "#user_all_texts_emails['emailAddress'] = \n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot org', '.org')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot io', '.io')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot me', '.me')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot com', '.com')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot fr', '.fr')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot ', '.')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' at ', '@')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' ', '')\n",
      "\n",
      "print(user_all_texts_emails['emailAddress'].shape[0])\n",
      "print(user_all_texts_emails['emailAddress'].unique().shape[0])\n",
      "print(user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True].shape[0])\n",
      "42/97: user_all_texts_emails.head()\n",
      "42/98: user_all_texts_emails.groupby(['user', 'email']).count()\n",
      "42/99: user_all_texts_emails.groupby('user', 'email').count()\n",
      "42/100: user_all_texts_emails.groupby('user', 'emailAddress').count()\n",
      "42/101: user_all_texts_emails.groupby(['user', 'emailAddress']).count()\n",
      "42/102: user_all_texts_emails.groupby(['emailAddress']).count()\n",
      "42/103: user_all_texts_emails.groupby(['user', 'emailAddress']).count()\n",
      "42/104: user_all_texts_emails.groupby(['user', 'emailAddress']).count()==1\n",
      "42/105: user_all_texts_emails.groupby(['user', 'emailAddress']).count()>1\n",
      "42/106: user_all_texts_emails.groupby(['user', 'emailAddress']).count()>10\n",
      "42/107: user_all_texts_emails.groupby(['user', 'emailAddress']).count()>500\n",
      "42/108: user_all_texts_emails.groupby(['user', 'emailAddress']).count()<0\n",
      "42/109: user_all_texts_emails.groupby(['user', 'emailAddress']).count()>1\n",
      "42/110: user_all_texts_emails['user', 'emailAddress', 'project'].groupby(['user', 'emailAddress']).count()>1\n",
      "42/111: user_all_texts_emails[['user', 'emailAddress', 'project']].groupby(['user', 'emailAddress']).count()>1\n",
      "42/112: user_all_texts_emails[['user', 'emailAddress', 'project']].groupby(['user', 'emailAddress']).count('project')\n",
      "42/113: user_all_texts_emails[['user', 'emailAddress', 'project']].groupby(['user', 'emailAddress']).count()\n",
      "42/114: users_proj = user_all_texts_emails[['user', 'emailAddress', 'project']].groupby(['user', 'emailAddress']).count()\n",
      "42/115: users_proj.head()\n",
      "42/116: users_proj[users_proj['project']>1]\n",
      "42/117:\n",
      "users_with_duplicates = users_proj[users_proj['project']>1]['user', 'emailAddress']\n",
      "users_with_single = users_proj[users_proj['project']=1]['user', 'emailAddress']\n",
      "users_with_single_noemail=user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True]['user', 'email']\n",
      "42/118:\n",
      "users_with_duplicates = users_proj[users_proj['project']>1]['user', 'emailAddress']\n",
      "users_with_single = users_proj[users_proj['project']==1]['user', 'emailAddress']\n",
      "users_with_single_noemail=user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True]['user', 'email']\n",
      "42/119:\n",
      "users_with_duplicates = users_proj[users_proj['project']>1][['user', 'emailAddress']]\n",
      "users_with_single = users_proj[users_proj['project']==1][['user', 'emailAddress']]\n",
      "users_with_single_noemail=user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True][['user', 'email']]\n",
      "42/120:\n",
      "users_with_duplicates = users_proj[users_proj['project']>1][['user', 'emailAddress']]\n",
      "users_with_single = users_proj[users_proj['project']==1][['user', 'emailAddress']]\n",
      "users_with_single_noemail=user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True][['user', 'emailAddress']]\n",
      "42/121:\n",
      "users_with_duplicates = users_proj[users_proj['project']>1][['user', 'emailAddress']]\n",
      "users_with_single = users_proj[users_proj['project']==1][['user', 'emailAddress']]\n",
      "42/122: users_proj\n",
      "42/123: users_proj['user']\n",
      "42/124:\n",
      "users_proj = user_all_texts_emails[['user', 'emailAddress', 'project']].groupby(['user', 'emailAddress']).count()\n",
      "users_proj.reset_index(level= [0,1,2], inplace=True)\n",
      "42/125:\n",
      "users_proj = user_all_texts_emails[['user', 'emailAddress', 'project']].groupby(['user', 'emailAddress']).count()\n",
      "users_proj.reset_index(level= [0,1], inplace=True)\n",
      "42/126: users_proj\n",
      "42/127: users_proj.head()\n",
      "42/128:\n",
      "users_with_duplicates = users_proj[users_proj['project']>1][['user', 'emailAddress']]\n",
      "users_with_single = users_proj[users_proj['project']==1][['user', 'emailAddress']]\n",
      "users_with_single_noemail=user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True][['user', 'emailAddress']]\n",
      "42/129: users_with_single\n",
      "42/130: users_with_single[pd.isnull(users_with_single.emailAddress)==True]\n",
      "42/131: users_with_duplicates\n",
      "42/132: users_with_duplicates.shape[0]\n",
      "42/133: users_with_duplicates\n",
      "42/134:\n",
      "for i in (0, users_with_duplicates.shape[0]):\n",
      "    print i\n",
      "42/135:\n",
      "for i in (0, users_with_duplicates.shape[0]):\n",
      "    print (i)\n",
      "42/136:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print (i)\n",
      "42/137:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print users_with_duplicates[i]\n",
      "42/138:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print users_with_duplicates[i,]\n",
      "42/139:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print users_with_duplicates.loc[i,]\n",
      "42/140:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print users_with_duplicates.loc[i]\n",
      "42/141:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print (users_with_duplicates.loc[i])\n",
      "42/142:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print (users_with_duplicates.loc[i,])\n",
      "42/143:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print (users_with_duplicates.loc[i,:])\n",
      "42/144:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print (users_with_duplicates.loc[i:])\n",
      "42/145:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print (i, users_with_duplicates.loc[i:])\n",
      "42/146:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print (i)\n",
      "42/147:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print (users_with_duplicates.iloc[i])\n",
      "42/148:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print (users_with_duplicates.iloc[i]['user'])\n",
      "42/149:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print (i, users_with_duplicates.iloc[i]['user'])\n",
      "42/150:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    print (i, users_with_duplicates.iloc[i]['user'], users_with_duplicates.iloc[i]['emailAddress'])\n",
      "42/151: user_text_combined.head()\n",
      "42/152: user_all_texts_emails.head()\n",
      "42/153:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    user_name = users_with_duplicates.iloc[i]['user']\n",
      "    user_email = users_with_duplicates.iloc[i]['emailAddress']\n",
      "    \n",
      "    print(user_all_texts_emails[(user_all_texts_emails['user']==user_name) & (user_all_texts_emails['emailAddress']==user_email)])\n",
      "42/154:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    user_name = users_with_duplicates.iloc[i]['user']\n",
      "    user_email = users_with_duplicates.iloc[i]['emailAddress']\n",
      "    \n",
      "    df = (user_all_texts_emails[(user_all_texts_emails['user']==user_name) & (user_all_texts_emails['emailAddress']==user_email)])\n",
      "    text_=''\n",
      "    count_of_texts_=0\n",
      "    words_in_text_=0\n",
      "    texts_length_=0\n",
      "    for k in range(0, df.shape[0]):\n",
      "        text_ = text_ + df.iloc[k]['text']\n",
      "42/155:\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    user_name = users_with_duplicates.iloc[i]['user']\n",
      "    user_email = users_with_duplicates.iloc[i]['emailAddress']\n",
      "    \n",
      "    df = (user_all_texts_emails[(user_all_texts_emails['user']==user_name) & (user_all_texts_emails['emailAddress']==user_email)])\n",
      "    text_=''\n",
      "    count_of_texts_=0\n",
      "    words_in_text_=0\n",
      "    texts_length_=0\n",
      "    for k in range(0, df.shape[0]):\n",
      "        text_ = text_ + df.iloc[k]['text']\n",
      "df.head()\n",
      "42/156: user_all_texts_emails.iloc['user'=='wluu']\n",
      "42/157: user_all_texts_emails.loc['user'=='wluu']\n",
      "42/158: user_all_texts_emails.loc[user=='wluu']\n",
      "42/159: user_all_texts_emails.loc[user='wluu']\n",
      "42/160: user_all_texts_emails.loc[user=='wluu']\n",
      "42/161: user_all_texts_emails.loc['user'=='wluu']\n",
      "42/162: user_all_texts_emails.loc('user'=='wluu')\n",
      "42/163: user_all_texts_emails.loc(['user']=='wluu')\n",
      "42/164: user_all_texts_emails.loc(user_all_texts_emails['user']=='wluu')\n",
      "42/165: user_all_texts_emails.loc[user_all_texts_emails['user']=='wluu']\n",
      "42/166: user_all_texts_emails.loc[user_all_texts_emails['user']=='wluu' & user_all_texts_emails['emailAddress']=='wluu@appcelerator.com']\n",
      "42/167: user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') & user_all_texts_emails['emailAddress']=='wluu@appcelerator.com']\n",
      "42/168: user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')]\n",
      "42/169:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')]\n",
      "42/170:\n",
      "dup_users_text_df = pd.DataFrame({'user', 'emailAddress', 'count_of_texts'})\n",
      "\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    user_name = users_with_duplicates.iloc[i]['user']\n",
      "    user_email = users_with_duplicates.iloc[i]['emailAddress']\n",
      "    \n",
      "    df = (user_all_texts_emails[(user_all_texts_emails['user']==user_name) & (user_all_texts_emails['emailAddress']==user_email)])\n",
      "    text_=''\n",
      "    count_of_texts_=0\n",
      "    words_in_text_=0\n",
      "    texts_length_=0\n",
      "    for k in range(0, df.shape[0]):\n",
      "        text_ = text_ + df.iloc[k]['text']\n",
      "        count_of_texts_ = count_of_texts_ + df.iloc[k]['count_of_texts']\n",
      "        words_in_text_ = words_in_text_ + df.iloc[k]['words_in_text']\n",
      "        texts_length_ = texts_length_ + df.iloc[k]['texts_length']\n",
      "    \n",
      "    \n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email)]['text']=text_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email)]['count_of_texts']=count_of_texts_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email)]['words_in_text']=words_in_text_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email)]['texts_length']=texts_length_\n",
      "42/171:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')]\n",
      "42/172:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')]\n",
      "42/173:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')]\n",
      "42/174:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')]\n",
      "42/175:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')][0]\n",
      "42/176:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')][1]\n",
      "42/177:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')]\n",
      "42/178:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')].iloc\n",
      "42/179:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')].loc\n",
      "42/180:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')].loc()\n",
      "42/181:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')].loc[]\n",
      "42/182:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')].loc[,]\n",
      "42/183:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')].loc[:]\n",
      "42/184:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')].iloc[:]\n",
      "42/185:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')].iloc[,]\n",
      "42/186:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')].iloc[]\n",
      "42/187:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')].iloc[:]\n",
      "42/188:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')]\n",
      "42/189:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com'), 'user']\n",
      "42/190:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')]\n",
      "42/191:\n",
      "dup_users_text_df = pd.DataFrame({'user', 'emailAddress', 'count_of_texts'})\n",
      "\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    user_name = users_with_duplicates.iloc[i]['user']\n",
      "    user_email = users_with_duplicates.iloc[i]['emailAddress']\n",
      "    \n",
      "    df = (user_all_texts_emails[(user_all_texts_emails['user']==user_name) & (user_all_texts_emails['emailAddress']==user_email)])\n",
      "    text_=''\n",
      "    count_of_texts_=0\n",
      "    words_in_text_=0\n",
      "    texts_length_=0\n",
      "    for k in range(0, df.shape[0]):\n",
      "        text_ = text_ + df.iloc[k]['text']\n",
      "        count_of_texts_ = count_of_texts_ + df.iloc[k]['count_of_texts']\n",
      "        words_in_text_ = words_in_text_ + df.iloc[k]['words_in_text']\n",
      "        texts_length_ = texts_length_ + df.iloc[k]['texts_length']\n",
      "    \n",
      "    \n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'text']=text_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'count_of_texts']=count_of_texts_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'words_in_text']=words_in_text_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'texts_length']=texts_length_\n",
      "42/192:\n",
      "user_all_texts_emails.loc[(user_all_texts_emails['user']=='wluu') \n",
      "                          & (user_all_texts_emails['emailAddress']=='wluu@appcelerator.com')]\n",
      "42/193:\n",
      "user_all_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "\n",
      "\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot org', '.org')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot io', '.io')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot me', '.me')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot com', '.com')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot fr', '.fr')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot ', '.')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' at ', '@')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' ', '')\n",
      "\n",
      "print(user_all_texts_emails['emailAddress'].shape[0])\n",
      "print(user_all_texts_emails['emailAddress'].unique().shape[0])\n",
      "print(user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True].shape[0])\n",
      "42/194:\n",
      "users_proj = user_all_texts_emails[['user', 'emailAddress', 'project']].groupby(['user', 'emailAddress']).count()\n",
      "users_proj.reset_index(level= [0,1], inplace=True)\n",
      "\n",
      "users_with_duplicates = users_proj[users_proj['project']>1][['user', 'emailAddress']]\n",
      "users_with_single = users_proj[users_proj['project']==1][['user', 'emailAddress']]\n",
      "users_with_single_noemail=user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True][['user', 'emailAddress']]\n",
      "\n",
      "\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    user_name = users_with_duplicates.iloc[i]['user']\n",
      "    user_email = users_with_duplicates.iloc[i]['emailAddress']\n",
      "    \n",
      "    df = (user_all_texts_emails[(user_all_texts_emails['user']==user_name) & (user_all_texts_emails['emailAddress']==user_email)])\n",
      "    text_=''\n",
      "    count_of_texts_=0\n",
      "    words_in_text_=0\n",
      "    texts_length_=0\n",
      "    for k in range(0, df.shape[0]):\n",
      "        text_ = text_ + df.iloc[k]['text']\n",
      "        count_of_texts_ = count_of_texts_ + df.iloc[k]['count_of_texts']\n",
      "        words_in_text_ = words_in_text_ + df.iloc[k]['words_in_text']\n",
      "        texts_length_ = texts_length_ + df.iloc[k]['texts_length']\n",
      "    \n",
      "    \n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'text']=text_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'count_of_texts']=count_of_texts_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'words_in_text']=words_in_text_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'texts_length']=texts_length_\n",
      "42/195: user_all_texts_emails.head()\n",
      "42/196: <h3>Combine the texts of the users, that are present in different projects.</h3>\n",
      "42/197:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "42/198:\n",
      "user_all_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot org', '.org')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot io', '.io')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot me', '.me')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot com', '.com')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot fr', '.fr')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot ', '.')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' at ', '@')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' ', '')\n",
      "print(user_all_texts_emails['emailAddress'].shape[0])\n",
      "print(user_all_texts_emails['emailAddress'].unique().shape[0])\n",
      "print(user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True].shape[0])\n",
      "42/199:\n",
      "user_all_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot org', '.org')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot io', '.io')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot me', '.me')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot com', '.com')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot fr', '.fr')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot ', '.')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' at ', '@')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' ', '')\n",
      "42/200:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "42/201:\n",
      "user_all_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot org', '.org')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot io', '.io')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot me', '.me')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot com', '.com')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot fr', '.fr')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot ', '.')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' at ', '@')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' ', '')\n",
      "42/202:\n",
      "users_proj = user_all_texts_emails[['user', 'emailAddress', 'project']].groupby(['user', 'emailAddress']).count()\n",
      "users_proj.reset_index(level= [0,1], inplace=True)\n",
      "\n",
      "users_with_duplicates = users_proj[users_proj['project']>1][['user', 'emailAddress']]\n",
      "users_with_single = users_proj[users_proj['project']==1][['user', 'emailAddress']]\n",
      "users_with_single_noemail=user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True][['user', 'emailAddress']]\n",
      "\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    user_name = users_with_duplicates.iloc[i]['user']\n",
      "    user_email = users_with_duplicates.iloc[i]['emailAddress']   \n",
      "    df = (user_all_texts_emails[(user_all_texts_emails['user']==user_name) & (user_all_texts_emails['emailAddress']==user_email)])\n",
      "    text_=''\n",
      "    count_of_texts_=0\n",
      "    words_in_text_=0\n",
      "    texts_length_=0\n",
      "    for k in range(0, df.shape[0]):\n",
      "        text_ = text_ + df.iloc[k]['text']\n",
      "        count_of_texts_ = count_of_texts_ + df.iloc[k]['count_of_texts']\n",
      "        words_in_text_ = words_in_text_ + df.iloc[k]['words_in_text']\n",
      "        texts_length_ = texts_length_ + df.iloc[k]['texts_length']\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'text']=text_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'count_of_texts']=count_of_texts_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'words_in_text']=words_in_text_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'texts_length']=texts_length_\n",
      "42/203: print(user_text_combined.shape[0], user_all_texts_emails.shape[0])\n",
      "42/204: user_text_combined[user_text_combined['words_in_text']>=600].shape[0]\n",
      "42/205:\n",
      "print(user_text_combined[user_text_combined['words_in_text']>=600].shape[0], \n",
      "      user_all_texts_emails[user_all_texts_emails['words_in_text']>=600].shape[0])\n",
      "42/206:\n",
      "print('before these changes: 'user_text_combined[user_text_combined['words_in_text']>=600].shape[0], \n",
      "      'after these changes: ', user_all_texts_emails[user_all_texts_emails['words_in_text']>=600].shape[0])\n",
      "42/207:\n",
      "print('before these changes: ', user_text_combined[user_text_combined['words_in_text']>=600].shape[0], \n",
      "      'after these changes: ', user_all_texts_emails[user_all_texts_emails['words_in_text']>=600].shape[0])\n",
      "42/208:\n",
      "print('before these changes: ', user_text_combined[user_text_combined['words_in_text']>=600].shape[0], '\\n',\n",
      "      'after these changes: ', user_all_texts_emails[user_all_texts_emails['words_in_text']>=600].shape[0])\n",
      "42/209:\n",
      "user_texts_emails = pd.merge(user_all_texts_emails[user_all_texts_emails['words_in_text']>=600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "42/210:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "42/211:\n",
      "user_texts_emails = pd.merge(user_all_texts_emails[user_all_texts_emails['words_in_text']>=600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "42/212: user_all_texts_emails.head()\n",
      "42/213:\n",
      "user_all_texts_emails.head()\n",
      "users_df.head()\n",
      "42/214: user_all_texts_emails[user_all_texts_emails['words_in_text']>=600].head()\n",
      "42/215:\n",
      "user_texts_emails = pd.merge(user_all_texts_emails[user_all_texts_emails['words_in_text']>=600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "42/216:\n",
      "user_texts_emails = pd.merge(user_all_texts_emails[user_all_texts_emails['words_in_text']>=600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length']]\n",
      "42/217: user_texts_emails.head()\n",
      "42/218:\n",
      "user_texts_emails = pd.merge(user_all_texts_emails[user_all_texts_emails['words_in_text']>=600], users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length', 'emailAddress']]\n",
      "42/219:\n",
      "user_texts_emails = pd.merge(user_all_texts_emails[user_all_texts_emails['words_in_text']>=600], users_df, how = 'left', suffixes=['', '_']\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length', 'emailAddress']]\n",
      "42/220:\n",
      "user_texts_emails = pd.merge(user_all_texts_emails[user_all_texts_emails['words_in_text']>=600], users_df, how = 'left', suffixes=['', '_'],\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length', 'emailAddress']]\n",
      "42/221: user_texts_emails.head()\n",
      "42/222: user_texts_emails.shape[0]\n",
      "42/223: user_texts_emails.head()\n",
      "42/224: user_texts_emails = user_all_texts_emails[user_all_texts_emails['words_in_text']>=600]\n",
      "42/225: user_texts_emails.head()\n",
      "42/226: valid_user_texts = user_all_texts_emails[user_all_texts_emails['words_in_text']>=600]\n",
      "42/227:\n",
      "print(valid_user_texts.shape[0])\n",
      "valid_user_texts.head()\n",
      "42/228:\n",
      "print(valid_user_texts.shape[0])\n",
      "valid_user_texts.head(3)\n",
      "42/229:\n",
      "valid_user_texts.loc[pd.isnull(valid_user_texts.emailAddress)==True,'emailAddress'] = '-'\n",
      "valid_user_texts[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "print('Total number of unique users invalid user texts dataset: ', valid_user_texts[['user','emailAddress']].drop_duplicates().shape[0])\n",
      "valid_user_texts[['user','emailAddress']].drop_duplicates().head(3)\n",
      "42/230:\n",
      "#valid_user_texts.loc[pd.isnull(valid_user_texts.emailAddress)==True,'emailAddress'] = '-'\n",
      "valid_user_texts[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "print('Total number of unique users in valid user texts dataset: ', valid_user_texts[['user','emailAddress']].drop_duplicates().shape[0])\n",
      "valid_user_texts[['user','emailAddress']].drop_duplicates().head(3)\n",
      "42/231:\n",
      "valid_user_texts.loc[pd.isnull(valid_user_texts.emailAddress)==True,'emailAddress'] = '-'\n",
      "valid_user_texts[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "print('Total number of unique users in valid user texts dataset: ', valid_user_texts[['user','emailAddress']].drop_duplicates().shape[0])\n",
      "valid_user_texts[['user','emailAddress']].drop_duplicates().head(3)\n",
      "42/232:\n",
      "valid_user_texts.loc[pd.isnull(valid_user_texts.emailAddress)==True,'emailAddress'] = '-'\n",
      "valid_user_texts[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "print('Total number of unique users in valid user texts dataset: ', valid_user_texts[['user','emailAddress']].drop_duplicates().shape[0])\n",
      "valid_user_texts[['user','emailAddress']].drop_duplicates().head(3)\n",
      "42/233:\n",
      "valid_user_texts.groupby(['project']).count().to_csv('project_users.csv')\n",
      "valid_user_texts[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=False)\n",
      "42/234: users_df[pd.isnull(users_df.emailAddress)==True].head()\n",
      "42/235: valid_user_texts[pd.isnull(valid_user_texts.emailAddress)==True].head()\n",
      "42/236: valid_user_texts.head()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/237: <h2>Prepare data for watson</h2>\n",
      "42/238: valid_user_texts\n",
      "42/239: valid_user_texts.head()\n",
      "42/240: valid_user_texts.[['user','emailAddress']].drop_duplicates()\n",
      "42/241: valid_user_texts[['user','emailAddress']].drop_duplicates()\n",
      "42/242: valid_user_texts.sort_values([['user','emailAddress']])\n",
      "42/243: valid_user_texts.sort_values(['user','emailAddress'])\n",
      "42/244: valid_user_texts.sort_values(['user','emailAddress'])\n",
      "42/245: valid_user_texts[['user','emailAddress']].drop_duplicates()\n",
      "42/246: valid_user_texts[['user','emailAddress', 'text']].drop_duplicates()\n",
      "42/247:\n",
      "valid_user_texts_unique = valid_user_texts[['user','emailAddress', 'text']].drop_duplicates()\n",
      "print(valid_user_texts_unique.shape[0])\n",
      "valid_user_texts_unique.head()\n",
      "42/248:\n",
      "from __future__ import print_function\n",
      "import json\n",
      "from os.path import join, dirname\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import csv\n",
      "import os\n",
      "42/249:\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "\n",
      "'''\n",
      "another account credentials:\n",
      "BFsOsCFLvQgZHLicoAj_ywnJ_92h0ry0gTgKudmuDmjm\n",
      "https://gateway-lon.watsonplatform.net/personality-insights/api\n",
      "'''\n",
      "42/250: valid_user_texts_unique.to_csv('valid_user_texts_unique.csv')\n",
      "42/251:\n",
      "with open(join(dirname('__file__'), 'personality-v3.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "print(json.dumps(profile, indent=2))\n",
      "42/252: '2019-10-26'.toordinal()\n",
      "42/253: '2019-10-26'.datetime.toordinal()\n",
      "42/254:\n",
      "todate('2019-10-26')\n",
      "#.datetime.toordinal()\n",
      "42/255:\n",
      "date('2019-10-26')\n",
      "#.datetime.toordinal()\n",
      "42/256:\n",
      "#date('2019-10-26')\n",
      "datetime.now()\n",
      "#.datetime.toordinal()\n",
      "42/257:\n",
      "import datetime\n",
      "#date('2019-10-26')\n",
      "datetime.now()\n",
      "#.datetime.toordinal()\n",
      "42/258:\n",
      "import datetime\n",
      "#date('2019-10-26')\n",
      "datetime.strptime.now()\n",
      "#.datetime.toordinal()\n",
      "42/259:\n",
      "import datetime\n",
      "datetime.strptime('2019-10-26')\n",
      "#.now()\n",
      "#.datetime.toordinal()\n",
      "42/260:\n",
      "from datetime import datetime\n",
      "datetime.strptime('2019-10-26')\n",
      "#.now()\n",
      "#.datetime.toordinal()\n",
      "42/261:\n",
      "from datetime import datetime\n",
      "datetime('2019-10-26')\n",
      "#.now()\n",
      "#.datetime.toordinal()\n",
      "42/262:\n",
      "from datetime import datetime\n",
      "datetime.strptime(datetime.now())\n",
      "#.now()\n",
      "#.datetime.toordinal()\n",
      "42/263:\n",
      "from datetime import datetime\n",
      "datetime.strptime(datetime.now(), '%Y-%m-%d %H:%M:%S')\n",
      "#.now()\n",
      "#.datetime.toordinal()\n",
      "42/264:\n",
      "from datetime import datetime\n",
      "#datetime.strptime(datetime.now(), '%Y-%m-%d %H:%M:%S')\n",
      "#.now()\n",
      "datetime.now().datetime.toordinal()\n",
      "42/265:\n",
      "from datetime import datetime\n",
      "#datetime.strptime(datetime.now(), '%Y-%m-%d %H:%M:%S')\n",
      "#.now()\n",
      "datetime.now().toordinal()\n",
      "42/266: datetime.now().toordinal()\n",
      "42/267: valid_user_texts_unique.head()\n",
      "42/268:\n",
      "for i in range(0, valid_user_texts_unique.shape[0]):\n",
      "    user_ = valid_user_texts_unique.iloc[i]['user']\n",
      "    email_ = valid_user_texts_unique.iloc[i]['emailAddress']\n",
      "    text_ = valid_user_texts_unique.iloc[i]['text']\n",
      "    \n",
      "    data = {}\n",
      "    data['contentItems'] = []\n",
      "    data['contentItems'].append({\n",
      "        'content': text_,\n",
      "        'contenttype': 'text/plain',\n",
      "        'created': datetime.now().toordinal(),\n",
      "        'id': i,\n",
      "        'language':'en',\n",
      "        'user_name':user_\n",
      "        'email':email_\n",
      "    })\n",
      "\n",
      "    with open('json_input/data.json', 'w') as outfile:\n",
      "        json.dump(data, outfile)\n",
      "42/269:\n",
      "for i in range(0, valid_user_texts_unique.shape[0]):\n",
      "    user_ = valid_user_texts_unique.iloc[i]['user']\n",
      "    email_ = valid_user_texts_unique.iloc[i]['emailAddress']\n",
      "    text_ = valid_user_texts_unique.iloc[i]['text']\n",
      "    \n",
      "    data = {}\n",
      "    data['contentItems'] = []\n",
      "    data['contentItems'].append({\n",
      "        'content': text_,\n",
      "        'contenttype': 'text/plain',\n",
      "        'created': datetime.now().toordinal(),\n",
      "        'id': i,\n",
      "        'language':'en',\n",
      "        'user_name':user_,\n",
      "        'email':email_\n",
      "    })\n",
      "\n",
      "    with open('json_input/data.json', 'w') as outfile:\n",
      "        json.dump(data, outfile)\n",
      "42/270:\n",
      "with open(join(dirname('__file__'), 'data.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "print(json.dumps(profile, indent=2))\n",
      "42/271:\n",
      "for i in range(0, valid_user_texts_unique.shape[0]):\n",
      "    user_ = valid_user_texts_unique.iloc[i]['user']\n",
      "    email_ = valid_user_texts_unique.iloc[i]['emailAddress']\n",
      "    text_ = valid_user_texts_unique.iloc[i]['text']\n",
      "    \n",
      "    data = {}\n",
      "    data['contentItems'] = []\n",
      "    data['contentItems'].append({\n",
      "        'content': text_,\n",
      "        'contenttype': 'text/plain',\n",
      "        'created': datetime.now().toordinal(),\n",
      "        'id': i,\n",
      "        'language':'en',\n",
      "        'user_name':user_,\n",
      "        'email':email_\n",
      "    })\n",
      "    \n",
      "    filename = user_ + ';' + email_\n",
      "    with open('json_input/filename.json', 'w') as outfile:\n",
      "        json.dump(data, outfile)\n",
      "42/272:\n",
      "for i in range(0, valid_user_texts_unique.shape[0]):\n",
      "    user_ = valid_user_texts_unique.iloc[i]['user']\n",
      "    email_ = valid_user_texts_unique.iloc[i]['emailAddress']\n",
      "    text_ = valid_user_texts_unique.iloc[i]['text']\n",
      "    \n",
      "    data = {}\n",
      "    data['contentItems'] = []\n",
      "    data['contentItems'].append({\n",
      "        'content': text_,\n",
      "        'contenttype': 'text/plain',\n",
      "        'created': datetime.now().toordinal(),\n",
      "        'id': i,\n",
      "        'language':'en',\n",
      "        'user_name':user_,\n",
      "        'email':email_\n",
      "    })\n",
      "    \n",
      "    file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "    with open(file, 'w') as outfile:\n",
      "        json.dump(data, outfile)\n",
      "42/273: <h2> Get IBM Watson personality insight results </h2>\n",
      "42/274:\n",
      "from __future__ import print_function\n",
      "import json\n",
      "from os.path import join, dirname\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import csv\n",
      "import os\n",
      "42/275:\n",
      "from __future__ import print_function\n",
      "import json\n",
      "from os.path import join, dirname\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import csv\n",
      "import os\n",
      "\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "\n",
      "'''\n",
      "another account credentials:\n",
      "BFsOsCFLvQgZHLicoAj_ywnJ_92h0ry0gTgKudmuDmjm\n",
      "https://gateway-lon.watsonplatform.net/personality-insights/api\n",
      "'''\n",
      "42/276:\n",
      "with open(join(dirname('__file__'), 'data.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=False,\n",
      "        consumption_preferences=False).get_result()\n",
      "\n",
      "print(json.dumps(profile, indent=2))\n",
      "with open('output_json.json', 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "42/277: profile\n",
      "42/278: profile['word_count']\n",
      "42/279: profile['personality']\n",
      "42/280: profile['personality']['trait_id']\n",
      "42/281: profile['personality']\n",
      "42/282: profile['personality'][]\n",
      "42/283: profile['personality']{}\n",
      "42/284: profile['personality']\n",
      "42/285: profile['personality'][{'trait_id'}]\n",
      "42/286: profile['personality'][{'1'}]\n",
      "42/287: profile['personality'][1]\n",
      "42/288: profile['personality'][2]\n",
      "42/289: profile['personality'][3]\n",
      "42/290: profile['personality'][4]\n",
      "42/291: profile['personality'][5]\n",
      "42/292: profile['personality'][0]\n",
      "42/293: profile['personality'][0]['trait_id']\n",
      "42/294:\n",
      "print(profile['personality'][0]['trait_id'])\n",
      "profile['personality'][0]\n",
      "42/295:\n",
      "print(profile['personality'][0]['trait_id'])\n",
      "profile['personality'][0]['percentile']\n",
      "42/296:\n",
      "print(profile['personality'][0]['trait_id'])\n",
      "print(profile['personality'][0]['percentile'])\n",
      "profile['personality'][0]\n",
      "42/297:\n",
      "print(profile['personality'][0]['trait_id'])\n",
      "print(profile['personality'][0]['percentile'])\n",
      "print(profile['personality'][0]['significant'])\n",
      "42/298:\n",
      "print(profile['personality'][0]['trait_id'])\n",
      "print(profile['personality'][0]['percentile'])\n",
      "print(profile['personality'][0]['significant'])\n",
      "profile['personality'][0]\n",
      "42/299:\n",
      "for p in range(0, 5):\n",
      "    print(p)\n",
      "42/300:\n",
      "for p in range(0, 4.1):\n",
      "    print(p)\n",
      "42/301:\n",
      "for p in range(0, 5):\n",
      "    print(p)\n",
      "42/302:\n",
      "\n",
      "personalities=[]\n",
      "personalities.append({'trait_id': 'trait 1', 'percentile':'0.5'})\n",
      "                      \n",
      "print(personalities)\n",
      "42/303: valid_user_texts_unique.iloc[1]\n",
      "42/304:\n",
      "user_ = 'mark.pollack'\n",
      "    email_ = 'mpollack@gopivotal.com'\n",
      "    text_ = valid_user_texts_unique.iloc[1]['text']\n",
      "    \n",
      "    # Save input JSON file for each user \n",
      "    data = {}\n",
      "    data['contentItems'] = []\n",
      "    data['contentItems'].append({\n",
      "        'content': text_,\n",
      "        'contenttype': 'text/plain',\n",
      "        'created': datetime.now().toordinal(),\n",
      "        'id': i,\n",
      "        'language':'en',\n",
      "        'user_name':user_,\n",
      "        'email':email_\n",
      "    })\n",
      "    \n",
      "    input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "    with open(input_json_file, 'w') as outfile:\n",
      "        json.dump(data, outfile)\n",
      "    \n",
      "    # Call IBM Watson Personality Insights for each user \n",
      "    with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=False,\n",
      "        consumption_preferences=False).get_result()\n",
      "\n",
      "    # Save the personality insights result into output JSON file for each user\n",
      "    output_json_file = 'json_output/'user_ + ';' + email_ + '_result.json'\n",
      "    with open(output_json_file, 'w') as outfile:\n",
      "        json.dump(profile, outfile)\n",
      "    \n",
      "    personalities = []\n",
      "    for p in range(0, 5):\n",
      "        trait_id = profile['personality'][p]['trait_id']\n",
      "        percentile = profile['personality'][p]['percentile']\n",
      "        significant = profile['personality'][p]['significant']\n",
      "        personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                             'percentile': profile['personality'][p]['percentile'],\n",
      "                              'significant': profile['personality'][p]['significant']\n",
      "                             })\n",
      "        \n",
      "        if trait_id == 'big5_openness':\n",
      "            big5_openness = percentile\n",
      "            big5_o_sign = significant\n",
      "        elif trait_id=='big5_conscientiousness':\n",
      "            big5_conscientiousness = percentile\n",
      "            big5_c_sign = significant\n",
      "        elif trait_id == 'big5_extraversion':\n",
      "            big5_extraversion = percentile\n",
      "            big5_e_sign = significant\n",
      "        elif trait_id = 'big5_agreeableness':\n",
      "            big5_agreeableness = percentile\n",
      "            big5_a_sign = significant\n",
      "        elif trait_id = 'big5_neuroticism':\n",
      "            big5_neuroticism = percentile\n",
      "            big5_n_sign = significant\n",
      "        \n",
      "        \n",
      "    user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                               'big5_openness', 'big5_o_sign',\n",
      "                                               'big5_conscientiousness', 'big5_c_sign',\n",
      "                                               'big5_extraversion', 'big5_e_sign',\n",
      "                                               'big5_agreeableness', 'big5_a_sign',\n",
      "                                               'big5_neuroticism', 'big5_n_sign'))\n",
      "    user_personalities = user_personalities.append([{\n",
      "        'user':user_, 'emailAddress':email_, \n",
      "       'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "       'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "       'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "       'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "       'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "    }])\n",
      "42/305:\n",
      "user_ = 'mark.pollack'\n",
      "email_ = 'mpollack@gopivotal.com'\n",
      "text_ = valid_user_texts_unique.iloc[1]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'text/plain',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=False,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/'user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['trait_id']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                         'percentile': profile['personality'][p]['percentile'],\n",
      "                          'significant': profile['personality'][p]['significant']\n",
      "                         })\n",
      "\n",
      "    if trait_id == 'big5_openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='big5_conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'big5_extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id = 'big5_agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id = 'big5_neuroticism':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_n_sign = significant\n",
      "\n",
      "\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'big5_openness', 'big5_o_sign',\n",
      "                                           'big5_conscientiousness', 'big5_c_sign',\n",
      "                                           'big5_extraversion', 'big5_e_sign',\n",
      "                                           'big5_agreeableness', 'big5_a_sign',\n",
      "                                           'big5_neuroticism', 'big5_n_sign'))\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "   'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "   'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "   'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "   'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "}])\n",
      "42/306:\n",
      "user_ = 'mark.pollack'\n",
      "email_ = 'mpollack@gopivotal.com'\n",
      "text_ = valid_user_texts_unique.iloc[1]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'text/plain',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=False,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/'user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['trait_id']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                         'percentile': profile['personality'][p]['percentile'],\n",
      "                          'significant': profile['personality'][p]['significant']\n",
      "                         })\n",
      "\n",
      "    if trait_id == 'big5_openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='big5_conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'big5_extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id = 'big5_agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id = 'big5_neuroticism':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_n_sign = significant\n",
      "\n",
      "\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'big5_openness', 'big5_o_sign',\n",
      "                                           'big5_conscientiousness', 'big5_c_sign',\n",
      "                                           'big5_extraversion', 'big5_e_sign',\n",
      "                                           'big5_agreeableness', 'big5_a_sign',\n",
      "                                           'big5_neuroticism', 'big5_n_sign'))\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "   'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "   'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "   'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "   'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "}])\n",
      "42/307:\n",
      "user_ = 'mark.pollack'\n",
      "email_ = 'mpollack@gopivotal.com'\n",
      "text_ = valid_user_texts_unique.iloc[1]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'text/plain',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=False,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['trait_id']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                         'percentile': profile['personality'][p]['percentile'],\n",
      "                          'significant': profile['personality'][p]['significant']\n",
      "                         })\n",
      "\n",
      "    if trait_id == 'big5_openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='big5_conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'big5_extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id = 'big5_agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id = 'big5_neuroticism':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_n_sign = significant\n",
      "\n",
      "\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'big5_openness', 'big5_o_sign',\n",
      "                                           'big5_conscientiousness', 'big5_c_sign',\n",
      "                                           'big5_extraversion', 'big5_e_sign',\n",
      "                                           'big5_agreeableness', 'big5_a_sign',\n",
      "                                           'big5_neuroticism', 'big5_n_sign'))\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "   'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "   'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "   'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "   'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "}])\n",
      "42/308:\n",
      "user_ = 'mark.pollack'\n",
      "email_ = 'mpollack@gopivotal.com'\n",
      "text_ = valid_user_texts_unique.iloc[1]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'text/plain',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=False,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['trait_id']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                         'percentile': profile['personality'][p]['percentile'],\n",
      "                          'significant': profile['personality'][p]['significant']\n",
      "                         })\n",
      "\n",
      "    if trait_id == 'big5_openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='big5_conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'big5_extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id == 'big5_agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id == 'big5_neuroticism':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_n_sign = significant\n",
      "\n",
      "\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'big5_openness', 'big5_o_sign',\n",
      "                                           'big5_conscientiousness', 'big5_c_sign',\n",
      "                                           'big5_extraversion', 'big5_e_sign',\n",
      "                                           'big5_agreeableness', 'big5_a_sign',\n",
      "                                           'big5_neuroticism', 'big5_n_sign'))\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "   'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "   'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "   'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "   'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "}])\n",
      "42/309:\n",
      "user_ = 'mark.pollack'\n",
      "email_ = 'mpollack@gopivotal.com'\n",
      "text_ = valid_user_texts_unique.iloc[1]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'text/plain',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=False,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['trait_id']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                         'percentile': profile['personality'][p]['percentile'],\n",
      "                          'significant': profile['personality'][p]['significant']\n",
      "                         })\n",
      "\n",
      "    if trait_id == 'big5_openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='big5_conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'big5_extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id == 'big5_agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id == 'big5_neuroticism':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_n_sign = significant\n",
      "\n",
      "\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'big5_openness', 'big5_o_sign',\n",
      "                                           'big5_conscientiousness', 'big5_c_sign',\n",
      "                                           'big5_extraversion', 'big5_e_sign',\n",
      "                                           'big5_agreeableness', 'big5_a_sign',\n",
      "                                           'big5_neuroticism', 'big5_n_sign'))\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "   'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "   'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "   'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "   'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "}])\n",
      "42/310:\n",
      "user_ = 'mark.pollack'\n",
      "email_ = 'mpollack@gopivotal.com'\n",
      "text_ = valid_user_texts_unique.iloc[1]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'application/json',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=False,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['trait_id']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                         'percentile': profile['personality'][p]['percentile'],\n",
      "                          'significant': profile['personality'][p]['significant']\n",
      "                         })\n",
      "\n",
      "    if trait_id == 'big5_openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='big5_conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'big5_extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id == 'big5_agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id == 'big5_neuroticism':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_n_sign = significant\n",
      "\n",
      "\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'big5_openness', 'big5_o_sign',\n",
      "                                           'big5_conscientiousness', 'big5_c_sign',\n",
      "                                           'big5_extraversion', 'big5_e_sign',\n",
      "                                           'big5_agreeableness', 'big5_a_sign',\n",
      "                                           'big5_neuroticism', 'big5_n_sign'))\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "   'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "   'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "   'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "   'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "}])\n",
      "42/311:\n",
      "user_ = 'mark.pollack'\n",
      "email_ = 'mpollack@gopivotal.com'\n",
      "text_ = valid_user_texts_unique.iloc[1]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'application/json',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=False,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['trait_id']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                         'percentile': profile['personality'][p]['percentile'],\n",
      "                          'significant': profile['personality'][p]['significant']\n",
      "                         })\n",
      "\n",
      "    if trait_id == 'big5_openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='big5_conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'big5_extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id == 'big5_agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id == 'big5_neuroticism':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_n_sign = significant\n",
      "\n",
      "\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'big5_openness', 'big5_o_sign',\n",
      "                                           'big5_conscientiousness', 'big5_c_sign',\n",
      "                                           'big5_extraversion', 'big5_e_sign',\n",
      "                                           'big5_agreeableness', 'big5_a_sign',\n",
      "                                           'big5_neuroticism', 'big5_n_sign'))\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "   'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "   'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "   'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "   'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "}])\n",
      "42/312: user_personalities\n",
      "42/313:\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'big5_openness', 'big5_o_sign',\n",
      "                                           'big5_conscientiousness', 'big5_c_sign',\n",
      "                                           'big5_extraversion', 'big5_e_sign',\n",
      "                                           'big5_agreeableness', 'big5_a_sign',\n",
      "                                           'big5_neuroticism', 'big5_n_sign'))\n",
      "\n",
      "for i in range(0, valid_user_texts_unique.shape[0]):\n",
      "    user_ = valid_user_texts_unique.iloc[i]['user']\n",
      "    email_ = valid_user_texts_unique.iloc[i]['emailAddress']\n",
      "    text_ = valid_user_texts_unique.iloc[i]['text']\n",
      "    \n",
      "    # Save input JSON file for each user \n",
      "    data = {}\n",
      "    data['contentItems'] = []\n",
      "    data['contentItems'].append({\n",
      "        'content': text_,\n",
      "        'contenttype': 'application/json',\n",
      "        'created': datetime.now().toordinal(),\n",
      "        'id': i,\n",
      "        'language':'en',\n",
      "        'user_name':user_,\n",
      "        'email':email_\n",
      "    })\n",
      "\n",
      "    input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "    with open(input_json_file, 'w') as outfile:\n",
      "        json.dump(data, outfile)\n",
      "\n",
      "    # Call IBM Watson Personality Insights for each user \n",
      "    with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "        profile_json:\n",
      "        profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=False,\n",
      "        consumption_preferences=False).get_result()\n",
      "\n",
      "    # Save the personality insights result into output JSON file for each user\n",
      "    output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "    with open(output_json_file, 'w') as outfile:\n",
      "        json.dump(profile, outfile)\n",
      "\n",
      "    personalities = []\n",
      "    for p in range(0, 5):\n",
      "        trait_id = profile['personality'][p]['trait_id']\n",
      "        percentile = profile['personality'][p]['percentile']\n",
      "        significant = profile['personality'][p]['significant']\n",
      "        personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                             'percentile': profile['personality'][p]['percentile'],\n",
      "                              'significant': profile['personality'][p]['significant']\n",
      "                             })\n",
      "\n",
      "        if trait_id == 'big5_openness':\n",
      "            big5_openness = percentile\n",
      "            big5_o_sign = significant\n",
      "        elif trait_id=='big5_conscientiousness':\n",
      "            big5_conscientiousness = percentile\n",
      "            big5_c_sign = significant\n",
      "        elif trait_id == 'big5_extraversion':\n",
      "            big5_extraversion = percentile\n",
      "            big5_e_sign = significant\n",
      "        elif trait_id == 'big5_agreeableness':\n",
      "            big5_agreeableness = percentile\n",
      "            big5_a_sign = significant\n",
      "        elif trait_id == 'big5_neuroticism':\n",
      "            big5_neuroticism = percentile\n",
      "            big5_n_sign = significant\n",
      "\n",
      "\n",
      "    user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                               'big5_openness', 'big5_o_sign',\n",
      "                                               'big5_conscientiousness', 'big5_c_sign',\n",
      "                                               'big5_extraversion', 'big5_e_sign',\n",
      "                                               'big5_agreeableness', 'big5_a_sign',\n",
      "                                               'big5_neuroticism', 'big5_n_sign'))\n",
      "    user_personalities = user_personalities.append([{\n",
      "        'user':user_, 'emailAddress':email_, \n",
      "       'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "       'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "       'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "       'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "       'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "    }])\n",
      "42/314: user_personalities\n",
      "42/315: user_personalities\n",
      "42/316: user_personalities\n",
      "42/317:\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'big5_openness', 'big5_o_sign',\n",
      "                                           'big5_conscientiousness', 'big5_c_sign',\n",
      "                                           'big5_extraversion', 'big5_e_sign',\n",
      "                                           'big5_agreeableness', 'big5_a_sign',\n",
      "                                           'big5_neuroticism', 'big5_n_sign'))\n",
      "\n",
      "for i in range(0, valid_user_texts_unique.shape[0]):\n",
      "    user_ = valid_user_texts_unique.iloc[i]['user']\n",
      "    email_ = valid_user_texts_unique.iloc[i]['emailAddress']\n",
      "    text_ = valid_user_texts_unique.iloc[i]['text']\n",
      "    \n",
      "    # Save input JSON file for each user \n",
      "    data = {}\n",
      "    data['contentItems'] = []\n",
      "    data['contentItems'].append({\n",
      "        'content': text_,\n",
      "        'contenttype': 'application/json',\n",
      "        'created': datetime.now().toordinal(),\n",
      "        'id': i,\n",
      "        'language':'en',\n",
      "        'user_name':user_,\n",
      "        'email':email_\n",
      "    })\n",
      "\n",
      "    input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "    with open(input_json_file, 'w') as outfile:\n",
      "        json.dump(data, outfile)\n",
      "\n",
      "    # Call IBM Watson Personality Insights for each user \n",
      "    with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "        profile_json:\n",
      "        profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=False,\n",
      "        consumption_preferences=False).get_result()\n",
      "\n",
      "    # Save the personality insights result into output JSON file for each user\n",
      "    output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "    with open(output_json_file, 'w') as outfile:\n",
      "        json.dump(profile, outfile)\n",
      "\n",
      "    personalities = []\n",
      "    for p in range(0, 5):\n",
      "        trait_id = profile['personality'][p]['trait_id']\n",
      "        percentile = profile['personality'][p]['percentile']\n",
      "        significant = profile['personality'][p]['significant']\n",
      "        personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                             'percentile': profile['personality'][p]['percentile'],\n",
      "                              'significant': profile['personality'][p]['significant']\n",
      "                             })\n",
      "\n",
      "        if trait_id == 'big5_openness':\n",
      "            big5_openness = percentile\n",
      "            big5_o_sign = significant\n",
      "        elif trait_id=='big5_conscientiousness':\n",
      "            big5_conscientiousness = percentile\n",
      "            big5_c_sign = significant\n",
      "        elif trait_id == 'big5_extraversion':\n",
      "            big5_extraversion = percentile\n",
      "            big5_e_sign = significant\n",
      "        elif trait_id == 'big5_agreeableness':\n",
      "            big5_agreeableness = percentile\n",
      "            big5_a_sign = significant\n",
      "        elif trait_id == 'big5_neuroticism':\n",
      "            big5_neuroticism = percentile\n",
      "            big5_n_sign = significant\n",
      "\n",
      "    user_personalities = user_personalities.append([{\n",
      "        'user':user_, 'emailAddress':email_, \n",
      "       'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "       'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "       'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "       'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "       'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "    }])\n",
      "42/318: user_personalities\n",
      "42/319:\n",
      "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
      "user_personalities\n",
      "42/320:\n",
      "user_ = 'dpandey'\n",
      "email_ = 'deepti.pandey@globallogic.com'\n",
      "text_ = valid_user_texts_unique.iloc[100]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'application/json',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=True,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['trait_id']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                         'percentile': profile['personality'][p]['percentile'],\n",
      "                          'significant': profile['personality'][p]['significant']\n",
      "                         })\n",
      "\n",
      "    if trait_id == 'big5_openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='big5_conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'big5_extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id == 'big5_agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id == 'big5_neuroticism':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_n_sign = significant\n",
      "\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "   'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "   'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "   'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "   'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "}])\n",
      "42/321:\n",
      "user_ = 'dpandey'\n",
      "email_ = 'deepti.pandey@globallogic.com'\n",
      "text_ = valid_user_texts_unique.iloc[100]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'application/json',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=True,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['trait_id']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                         'percentile': profile['personality'][p]['percentile'],\n",
      "                          'significant': profile['personality'][p]['significant']\n",
      "                         })\n",
      "\n",
      "    if trait_id == 'big5_openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='big5_conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'big5_extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id == 'big5_agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id == 'big5_neuroticism':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_n_sign = significant\n",
      "\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "   'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "   'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "   'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "   'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "}])\n",
      "42/322:\n",
      "user_ = 'dpandey'\n",
      "email_ = 'deepti.pandey@globallogic.com'\n",
      "text_ = valid_user_texts_unique.iloc[99]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'application/json',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=True,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['trait_id']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    personalities.append({'trait_id': profile['personality'][p]['trait_id'],\n",
      "                         'percentile': profile['personality'][p]['percentile'],\n",
      "                          'significant': profile['personality'][p]['significant']\n",
      "                         })\n",
      "\n",
      "    if trait_id == 'big5_openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='big5_conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'big5_extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id == 'big5_agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id == 'big5_neuroticism':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_n_sign = significant\n",
      "\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'big5_openness':big5_openness, 'big5_o_sign':big5_o_sign,\n",
      "   'big5_conscientiousness':big5_conscientiousness, 'big5_c_sign':big5_c_sign,\n",
      "   'big5_extraversion':big5_extraversion, 'big5_e_sign':big5_e_sign,\n",
      "   'big5_agreeableness':big5_agreeableness, 'big5_a_sign':big5_a_sign,\n",
      "   'big5_neuroticism':big5_neuroticism, 'big5_n_sign':big5_n_sign\n",
      "}])\n",
      "42/323:\n",
      "\n",
      "user_personalities\n",
      "#profile\n",
      "42/324:\n",
      "\n",
      "profile\n",
      "42/325:\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'openness', 'o_raw', 'o_sign',\n",
      "                                           'conscientiousness', 'c_raw', 'c_sign',\n",
      "                                           'extraversion', 'e_raw', 'e_sign',\n",
      "                                           'agreeableness', 'a_raw', 'a_sign',\n",
      "                                           'neuroticism', 'n_raw', 'n_sign'))\n",
      "user_ = 'dpandey'\n",
      "email_ = 'deepti.pandey@globallogic.com'\n",
      "text_ = valid_user_texts_unique.iloc[99]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'application/json',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=False,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['name']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    raw_score = profile['personality'][p]['raw_score']\n",
      "\n",
      "    if trait_id == 'openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_raw = raw_score\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_raw = raw_score\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_raw = raw_score\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id == 'agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_raw = raw_score\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id == 'neuroticism':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_a_raw = raw_score\n",
      "        big5_n_sign = significant\n",
      "\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'openness':big5_openness, 'o_raw':big5_o_raw, 'o_sign':big5_o_sign,\n",
      "   'conscientiousness':big5_conscientiousness, 'c_raw':big5_c_raw, 'c_sign':big5_c_sign,\n",
      "   'extraversion':big5_extraversion, 'e_raw':big5_e_raw, 'e_sign':big5_e_sign,\n",
      "   'agreeableness':big5_agreeableness, 'a_raw':big5_a_raw, 'a_sign':big5_a_sign,\n",
      "   'neuroticism':big5_neuroticism, 'n_raw':big5_n_raw, 'n_sign':big5_n_sign\n",
      "}])\n",
      "42/326: profile\n",
      "42/327: profile['personality'][p]\n",
      "42/328:\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'openness', 'o_raw', 'o_sign',\n",
      "                                           'conscientiousness', 'c_raw', 'c_sign',\n",
      "                                           'extraversion', 'e_raw', 'e_sign',\n",
      "                                           'agreeableness', 'a_raw', 'a_sign',\n",
      "                                           'neuroticism', 'n_raw', 'n_sign'))\n",
      "user_ = 'dpandey'\n",
      "email_ = 'deepti.pandey@globallogic.com'\n",
      "text_ = valid_user_texts_unique.iloc[99]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'application/json',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=True,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['name']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    raw_score = profile['personality'][p]['raw_score']\n",
      "\n",
      "    if trait_id == 'openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_raw = raw_score\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_raw = raw_score\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_raw = raw_score\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id == 'agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_raw = raw_score\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id == 'neuroticism':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_a_raw = raw_score\n",
      "        big5_n_sign = significant\n",
      "\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'openness':big5_openness, 'o_raw':big5_o_raw, 'o_sign':big5_o_sign,\n",
      "   'conscientiousness':big5_conscientiousness, 'c_raw':big5_c_raw, 'c_sign':big5_c_sign,\n",
      "   'extraversion':big5_extraversion, 'e_raw':big5_e_raw, 'e_sign':big5_e_sign,\n",
      "   'agreeableness':big5_agreeableness, 'a_raw':big5_a_raw, 'a_sign':big5_a_sign,\n",
      "   'neuroticism':big5_neuroticism, 'n_raw':big5_n_raw, 'n_sign':big5_n_sign\n",
      "}])\n",
      "42/329: profile['personality'][p]\n",
      "42/330:\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'openness', 'o_raw', 'o_sign',\n",
      "                                           'conscientiousness', 'c_raw', 'c_sign',\n",
      "                                           'extraversion', 'e_raw', 'e_sign',\n",
      "                                           'agreeableness', 'a_raw', 'a_sign',\n",
      "                                           'neuroticism', 'n_raw', 'n_sign'))\n",
      "user_ = 'dpandey'\n",
      "email_ = 'deepti.pandey@globallogic.com'\n",
      "text_ = valid_user_texts_unique.iloc[99]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'application/json',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=True,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['name']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    raw_score = profile['personality'][p]['raw_score']\n",
      "\n",
      "    if trait_id == 'openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_raw = raw_score\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_raw = raw_score\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_raw = raw_score\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id == 'agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_raw = raw_score\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id == 'Emotional range':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_a_raw = raw_score\n",
      "        big5_n_sign = significant\n",
      "\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'openness':big5_openness, 'o_raw':big5_o_raw, 'o_sign':big5_o_sign,\n",
      "   'conscientiousness':big5_conscientiousness, 'c_raw':big5_c_raw, 'c_sign':big5_c_sign,\n",
      "   'extraversion':big5_extraversion, 'e_raw':big5_e_raw, 'e_sign':big5_e_sign,\n",
      "   'agreeableness':big5_agreeableness, 'a_raw':big5_a_raw, 'a_sign':big5_a_sign,\n",
      "   'neuroticism':big5_neuroticism, 'n_raw':big5_n_raw, 'n_sign':big5_n_sign\n",
      "}])\n",
      "42/331: profile['personality']\n",
      "42/332:\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'openness', 'o_raw', 'o_sign',\n",
      "                                           'conscientiousness', 'c_raw', 'c_sign',\n",
      "                                           'extraversion', 'e_raw', 'e_sign',\n",
      "                                           'agreeableness', 'a_raw', 'a_sign',\n",
      "                                           'neuroticism', 'n_raw', 'n_sign'))\n",
      "user_ = 'dpandey'\n",
      "email_ = 'deepti.pandey@globallogic.com'\n",
      "text_ = valid_user_texts_unique.iloc[99]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'application/json',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=True,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['name']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    raw_score = profile['personality'][p]['raw_score']\n",
      "\n",
      "    if trait_id == 'Openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_raw = raw_score\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='Conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_raw = raw_score\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'Extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_raw = raw_score\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id == 'Agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_raw = raw_score\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id == 'Emotional range':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_a_raw = raw_score\n",
      "        big5_n_sign = significant\n",
      "\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'openness':big5_openness, 'o_raw':big5_o_raw, 'o_sign':big5_o_sign,\n",
      "   'conscientiousness':big5_conscientiousness, 'c_raw':big5_c_raw, 'c_sign':big5_c_sign,\n",
      "   'extraversion':big5_extraversion, 'e_raw':big5_e_raw, 'e_sign':big5_e_sign,\n",
      "   'agreeableness':big5_agreeableness, 'a_raw':big5_a_raw, 'a_sign':big5_a_sign,\n",
      "   'neuroticism':big5_neuroticism, 'n_raw':big5_n_raw, 'n_sign':big5_n_sign\n",
      "}])\n",
      "42/333:\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'openness', 'o_raw', 'o_sign',\n",
      "                                           'conscientiousness', 'c_raw', 'c_sign',\n",
      "                                           'extraversion', 'e_raw', 'e_sign',\n",
      "                                           'agreeableness', 'a_raw', 'a_sign',\n",
      "                                           'neuroticism', 'n_raw', 'n_sign'))\n",
      "user_ = 'dpandey'\n",
      "email_ = 'deepti.pandey@globallogic.com'\n",
      "text_ = valid_user_texts_unique.iloc[99]['text']\n",
      "\n",
      "# Save input JSON file for each user \n",
      "data = {}\n",
      "data['contentItems'] = []\n",
      "data['contentItems'].append({\n",
      "    'content': text_,\n",
      "    'contenttype': 'application/json',\n",
      "    'created': datetime.now().toordinal(),\n",
      "    'id': i,\n",
      "    'language':'en',\n",
      "    'user_name':user_,\n",
      "    'email':email_\n",
      "})\n",
      "\n",
      "input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "with open(input_json_file, 'w') as outfile:\n",
      "    json.dump(data, outfile)\n",
      "\n",
      "# Call IBM Watson Personality Insights for each user \n",
      "with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "    profile_json:\n",
      "    profile = service.profile(\n",
      "    profile_json.read(),\n",
      "    'application/json',\n",
      "    raw_scores=True,\n",
      "    consumption_preferences=False).get_result()\n",
      "\n",
      "# Save the personality insights result into output JSON file for each user\n",
      "output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "with open(output_json_file, 'w') as outfile:\n",
      "    json.dump(profile, outfile)\n",
      "\n",
      "personalities = []\n",
      "for p in range(0, 5):\n",
      "    trait_id = profile['personality'][p]['name']\n",
      "    percentile = profile['personality'][p]['percentile']\n",
      "    significant = profile['personality'][p]['significant']\n",
      "    raw_score = profile['personality'][p]['raw_score']\n",
      "\n",
      "    if trait_id == 'Openness':\n",
      "        big5_openness = percentile\n",
      "        big5_o_raw = raw_score\n",
      "        big5_o_sign = significant\n",
      "    elif trait_id=='Conscientiousness':\n",
      "        big5_conscientiousness = percentile\n",
      "        big5_c_raw = raw_score\n",
      "        big5_c_sign = significant\n",
      "    elif trait_id == 'Extraversion':\n",
      "        big5_extraversion = percentile\n",
      "        big5_e_raw = raw_score\n",
      "        big5_e_sign = significant\n",
      "    elif trait_id == 'Agreeableness':\n",
      "        big5_agreeableness = percentile\n",
      "        big5_a_raw = raw_score\n",
      "        big5_a_sign = significant\n",
      "    elif trait_id == 'Emotional range':\n",
      "        big5_neuroticism = percentile\n",
      "        big5_n_raw = raw_score\n",
      "        big5_n_sign = significant\n",
      "\n",
      "user_personalities = user_personalities.append([{\n",
      "    'user':user_, 'emailAddress':email_, \n",
      "   'openness':big5_openness, 'o_raw':big5_o_raw, 'o_sign':big5_o_sign,\n",
      "   'conscientiousness':big5_conscientiousness, 'c_raw':big5_c_raw, 'c_sign':big5_c_sign,\n",
      "   'extraversion':big5_extraversion, 'e_raw':big5_e_raw, 'e_sign':big5_e_sign,\n",
      "   'agreeableness':big5_agreeableness, 'a_raw':big5_a_raw, 'a_sign':big5_a_sign,\n",
      "   'neuroticism':big5_neuroticism, 'n_raw':big5_n_raw, 'n_sign':big5_n_sign\n",
      "}])\n",
      "42/334: user_personalities\n",
      "42/335:\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'openness', 'o_raw', 'o_sign',\n",
      "                                           'conscientiousness', 'c_raw', 'c_sign',\n",
      "                                           'extraversion', 'e_raw', 'e_sign',\n",
      "                                           'agreeableness', 'a_raw', 'a_sign',\n",
      "                                           'neuroticism', 'n_raw', 'n_sign'))\n",
      "\n",
      "for i in range(0, valid_user_texts_unique.shape[0]):\n",
      "    user_ = valid_user_texts_unique.iloc[i]['user']\n",
      "    email_ = valid_user_texts_unique.iloc[i]['emailAddress']\n",
      "    text_ = valid_user_texts_unique.iloc[i]['text']\n",
      "    \n",
      "    # Save input JSON file for each user \n",
      "    data = {}\n",
      "    data['contentItems'] = []\n",
      "    data['contentItems'].append({\n",
      "        'content': text_,\n",
      "        'contenttype': 'application/json',\n",
      "        'created': datetime.now().toordinal(),\n",
      "        'id': i,\n",
      "        'language':'en',\n",
      "        'user_name':user_,\n",
      "        'email':email_\n",
      "    })\n",
      "\n",
      "    input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "    with open(input_json_file, 'w') as outfile:\n",
      "        json.dump(data, outfile)\n",
      "\n",
      "    # Call IBM Watson Personality Insights for each user \n",
      "    with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "        profile_json:\n",
      "        profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=False).get_result()\n",
      "\n",
      "    # Save the personality insights result into output JSON file for each user\n",
      "    output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "    with open(output_json_file, 'w') as outfile:\n",
      "        json.dump(profile, outfile)\n",
      "\n",
      "    personalities = []\n",
      "    for p in range(0, 5):\n",
      "        trait_id = profile['personality'][p]['name']\n",
      "        percentile = profile['personality'][p]['percentile']\n",
      "        significant = profile['personality'][p]['significant']\n",
      "        raw_score = profile['personality'][p]['raw_score']\n",
      "\n",
      "        if trait_id == 'Openness':\n",
      "            big5_openness = percentile\n",
      "            big5_o_raw = raw_score\n",
      "            big5_o_sign = significant\n",
      "        elif trait_id=='Conscientiousness':\n",
      "            big5_conscientiousness = percentile\n",
      "            big5_c_raw = raw_score\n",
      "            big5_c_sign = significant\n",
      "        elif trait_id == 'Extraversion':\n",
      "            big5_extraversion = percentile\n",
      "            big5_e_raw = raw_score\n",
      "            big5_e_sign = significant\n",
      "        elif trait_id == 'Agreeableness':\n",
      "            big5_agreeableness = percentile\n",
      "            big5_a_raw = raw_score\n",
      "            big5_a_sign = significant\n",
      "        elif trait_id == 'Emotional range':\n",
      "            big5_neuroticism = percentile\n",
      "            big5_n_raw = raw_score\n",
      "            big5_n_sign = significant\n",
      "\n",
      "    user_personalities = user_personalities.append([{\n",
      "        'user':user_, 'emailAddress':email_, \n",
      "       'openness':big5_openness, 'o_raw':big5_o_raw, 'o_sign':big5_o_sign,\n",
      "       'conscientiousness':big5_conscientiousness, 'c_raw':big5_c_raw, 'c_sign':big5_c_sign,\n",
      "       'extraversion':big5_extraversion, 'e_raw':big5_e_raw, 'e_sign':big5_e_sign,\n",
      "       'agreeableness':big5_agreeableness, 'a_raw':big5_a_raw, 'a_sign':big5_a_sign,\n",
      "       'neuroticism':big5_neuroticism, 'n_raw':big5_n_raw, 'n_sign':big5_n_sign\n",
      "    }])\n",
      "42/336:\n",
      "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
      "user_personalities\n",
      "42/337:\n",
      "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
      "user_personalities.to_csv('user_personalities.csv')\n",
      "user_personalities\n",
      "42/338:\n",
      "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
      "user_personalities.to_csv('user_personalities.csv')\n",
      "user_personalities\n",
      "42/339:\n",
      "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
      "#user_personalities.to_csv('user_personalities.csv')\n",
      "user_personalities\n",
      "42/340:\n",
      "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
      "user_personalities.to_csv('user_personalities.csv')\n",
      "user_personalities.head()\n",
      "42/341: user_personalities['openness'].hist()\n",
      "42/342:\n",
      "hist_with_perc(user_personalities['openness'],\n",
      "               30,'','word count in text','Count of such occurences','yellow')\n",
      "42/343:\n",
      "hist_with_perc(user_personalities['openness'],\n",
      "               50,'','word count in text','Count of such occurences','yellow')\n",
      "42/344:\n",
      "hist_with_perc(user_personalities['openness']*100,\n",
      "               50,'','word count in text','Count of such occurences','yellow')\n",
      "42/345:\n",
      "hist_with_perc(user_personalities['openness']*100,\n",
      "               50,'','Openness','yellow')\n",
      "42/346:\n",
      "hist_with_perc(user_personalities['openness']*100,\n",
      "               50,'','word count in text','Count of such occurences','yellow')\n",
      "42/347:\n",
      "hist_with_perc(user_personalities['openness']*100,\n",
      "               50,'','Openness','Count of Occurences','yellow')\n",
      "42/348:\n",
      "hist_with_perc(user_personalities['openness']*100,\n",
      "               10,'','Openness','Count of Occurences','yellow')\n",
      "42/349: #user_personalities['openness'].hist()\n",
      "42/350: #hist_with_perc(user_personalities['openness']*100,10,'','Openness','Count of Occurences','yellow')\n",
      "43/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "43/2:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "43/3: changelog.head(3)\n",
      "43/4: issues.head(3)\n",
      "43/5: sprints.head(3)\n",
      "43/6: users.head(3)\n",
      "43/7: changelog.head(3)\n",
      "43/8:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "43/9:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "43/10:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field', 'author'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "43/11:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "43/12:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "43/13:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove multiple whitespaces (needed for removing 'at at' texts, regex is the next command below)\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\|#)', ' ', str(text))\n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "43/14:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "43/15:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "43/16:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment',\n",
      "                                            'Acceptance Criteria', 'Migration Impact', 'QA Test Plan', 'Out of Scope'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "43/17:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "43/18: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "43/19:\n",
      "#remove outliers\n",
      "# I have checked them manually, \n",
      "#these are the ones that the text cleaning functions could not properly clean and contain mostly the code snippet or logs\n",
      "#log_cols = log_cols.drop([3923, 1861, 3801, 2763, 7794, 2157])\n",
      "43/20: log_cols.to_csv('log_cols.csv')\n",
      "43/21:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "43/22: hist_with_perc(log_cols['textLength'],30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "43/23:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.05))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 5% to 95%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 95% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 5% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "43/24:\n",
      "hist_with_perc(log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],\n",
      "               30,'Distribution of the longest 5%  texts lengths in the dataset','text length','Count','lightblue')\n",
      "43/25:\n",
      "hist_with_perc(log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],\n",
      "               30,'Distribution of the shortest 5%  texts lengths in the dataset','text length','Count','aquamarine')\n",
      "43/26:\n",
      "\n",
      "hist_with_perc(log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,\n",
      "               30,'Distribution of the texts between 5%-95% percentile length in the dataset',\n",
      "               'text length','Count','mediumaquamarine')\n",
      "43/27:\n",
      "#detect the number of rows that fall in top % and bottom %\n",
      "top= 0.01\n",
      "bottom = 0.02\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of top '+str(round(top*100))+'% of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of bottom '+str(round(bottom*100))+'% of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "#log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "#log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "43/28: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "43/29:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "            words_in_text = words_in_text + len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "43/30:\n",
      "hist_with_perc(user_text_combined['words_in_text'],\n",
      "               30,'','word count in text','Count of such occurences','yellow')\n",
      "43/31: user_text_combined[user_text_combined['words_in_text']>=600].shape[0]\n",
      "43/32:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "43/33:\n",
      "user_all_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot org', '.org')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot io', '.io')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot me', '.me')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot com', '.com')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot fr', '.fr')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot ', '.')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' at ', '@')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' ', '')\n",
      "43/34:\n",
      "users_proj = user_all_texts_emails[['user', 'emailAddress', 'project']].groupby(['user', 'emailAddress']).count()\n",
      "users_proj.reset_index(level= [0,1], inplace=True)\n",
      "\n",
      "users_with_duplicates = users_proj[users_proj['project']>1][['user', 'emailAddress']]\n",
      "users_with_single = users_proj[users_proj['project']==1][['user', 'emailAddress']]\n",
      "users_with_single_noemail=user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True][['user', 'emailAddress']]\n",
      "\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    user_name = users_with_duplicates.iloc[i]['user']\n",
      "    user_email = users_with_duplicates.iloc[i]['emailAddress']   \n",
      "    df = (user_all_texts_emails[(user_all_texts_emails['user']==user_name) & (user_all_texts_emails['emailAddress']==user_email)])\n",
      "    text_=''\n",
      "    count_of_texts_=0\n",
      "    words_in_text_=0\n",
      "    texts_length_=0\n",
      "    for k in range(0, df.shape[0]):\n",
      "        text_ = text_ + df.iloc[k]['text']\n",
      "        count_of_texts_ = count_of_texts_ + df.iloc[k]['count_of_texts']\n",
      "        words_in_text_ = words_in_text_ + df.iloc[k]['words_in_text']\n",
      "        texts_length_ = texts_length_ + df.iloc[k]['texts_length']\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'text']=text_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'count_of_texts']=count_of_texts_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'words_in_text']=words_in_text_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'texts_length']=texts_length_\n",
      "43/35:\n",
      "print('before these changes: ', user_text_combined[user_text_combined['words_in_text']>=600].shape[0], '\\n',\n",
      "      'after these changes: ', user_all_texts_emails[user_all_texts_emails['words_in_text']>=600].shape[0])\n",
      "43/36: user_all_texts_emails[user_all_texts_emails['words_in_text']>=600].head()\n",
      "43/37: valid_user_texts = user_all_texts_emails[user_all_texts_emails['words_in_text']>=600]\n",
      "43/38:\n",
      "print(valid_user_texts.shape[0])\n",
      "valid_user_texts.head(3)\n",
      "43/39:\n",
      "valid_user_texts.loc[pd.isnull(valid_user_texts.emailAddress)==True,'emailAddress'] = '-'\n",
      "#valid_user_texts[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "print('Total number of unique users in valid user texts dataset: ', valid_user_texts[['user','emailAddress']].drop_duplicates().shape[0])\n",
      "valid_user_texts[['user','emailAddress']].drop_duplicates().head(3)\n",
      "43/40:\n",
      "#valid_user_texts.groupby(['project']).count().to_csv('project_users.csv')\n",
      "valid_user_texts[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=False)\n",
      "43/41:\n",
      "valid_user_texts_unique = valid_user_texts[['user','emailAddress', 'text']].drop_duplicates()\n",
      "print(valid_user_texts_unique.shape[0])\n",
      "valid_user_texts_unique.head()\n",
      "43/42: #valid_user_texts_unique.to_csv('valid_user_texts_unique.csv')\n",
      "43/43: valid_user_texts_unique.head()\n",
      "43/44:\n",
      "from __future__ import print_function\n",
      "import json\n",
      "from os.path import join, dirname\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import csv\n",
      "import os\n",
      "\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "\n",
      "'''\n",
      "another account credentials:\n",
      "BFsOsCFLvQgZHLicoAj_ywnJ_92h0ry0gTgKudmuDmjm\n",
      "https://gateway-lon.watsonplatform.net/personality-insights/api\n",
      "'''\n",
      "43/45: valid_user_texts_unique.head()\n",
      "43/46: user_personalities['openness'].hist()\n",
      "43/47:\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'openness', 'o_raw', 'o_sign',\n",
      "                                           'conscientiousness', 'c_raw', 'c_sign',\n",
      "                                           'extraversion', 'e_raw', 'e_sign',\n",
      "                                           'agreeableness', 'a_raw', 'a_sign',\n",
      "                                           'neuroticism', 'n_raw', 'n_sign'))\n",
      "\n",
      "for i in range(0, valid_user_texts_unique.shape[0]):\n",
      "    user_ = valid_user_texts_unique.iloc[i]['user']\n",
      "    email_ = valid_user_texts_unique.iloc[i]['emailAddress']\n",
      "    text_ = valid_user_texts_unique.iloc[i]['text']\n",
      "    \n",
      "    # define destination json structure Save input JSON file for each user \n",
      "    data = {}\n",
      "    data['contentItems'] = []\n",
      "    data['contentItems'].append({\n",
      "        'content': text_,\n",
      "        'contenttype': 'application/json',\n",
      "        'created': datetime.now().toordinal(),\n",
      "        'id': i,\n",
      "        'language':'en',\n",
      "        'user_name':user_,\n",
      "        'email':email_\n",
      "    })\n",
      "\n",
      "    input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "    with open(input_json_file, 'w') as outfile:\n",
      "        json.dump(data, outfile)\n",
      "\n",
      "    # Call IBM Watson Personality Insights for each user \n",
      "    with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "        profile_json:\n",
      "        profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=False).get_result()\n",
      "\n",
      "    # Save the personality insights result into output JSON file for each user\n",
      "    output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "    with open(output_json_file, 'w') as outfile:\n",
      "        json.dump(profile, outfile)\n",
      "    \n",
      "    # parse JSON, loop through each personality and get the scores\n",
      "    personalities = []\n",
      "    for p in range(0, 5):\n",
      "        trait_id = profile['personality'][p]['name']\n",
      "        percentile = profile['personality'][p]['percentile']\n",
      "        significant = profile['personality'][p]['significant']\n",
      "        raw_score = profile['personality'][p]['raw_score']\n",
      "\n",
      "        if trait_id == 'Openness':\n",
      "            big5_openness = percentile\n",
      "            big5_o_raw = raw_score\n",
      "            big5_o_sign = significant\n",
      "        elif trait_id=='Conscientiousness':\n",
      "            big5_conscientiousness = percentile\n",
      "            big5_c_raw = raw_score\n",
      "            big5_c_sign = significant\n",
      "        elif trait_id == 'Extraversion':\n",
      "            big5_extraversion = percentile\n",
      "            big5_e_raw = raw_score\n",
      "            big5_e_sign = significant\n",
      "        elif trait_id == 'Agreeableness':\n",
      "            big5_agreeableness = percentile\n",
      "            big5_a_raw = raw_score\n",
      "            big5_a_sign = significant\n",
      "        elif trait_id == 'Emotional range':\n",
      "            big5_neuroticism = percentile\n",
      "            big5_n_raw = raw_score\n",
      "            big5_n_sign = significant\n",
      "    \n",
      "    # Save the user personalities into dataset\n",
      "    user_personalities = user_personalities.append([{\n",
      "        'user':user_, 'emailAddress':email_, \n",
      "       'openness':big5_openness, 'o_raw':big5_o_raw, 'o_sign':big5_o_sign,\n",
      "       'conscientiousness':big5_conscientiousness, 'c_raw':big5_c_raw, 'c_sign':big5_c_sign,\n",
      "       'extraversion':big5_extraversion, 'e_raw':big5_e_raw, 'e_sign':big5_e_sign,\n",
      "       'agreeableness':big5_agreeableness, 'a_raw':big5_a_raw, 'a_sign':big5_a_sign,\n",
      "       'neuroticism':big5_neuroticism, 'n_raw':big5_n_raw, 'n_sign':big5_n_sign\n",
      "    }])\n",
      "43/48:\n",
      "from __future__ import print_function\n",
      "import json\n",
      "from os.path import join, dirname\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import csv\n",
      "import os\n",
      "from datetime import datetime\n",
      "\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "\n",
      "'''\n",
      "another account credentials:\n",
      "BFsOsCFLvQgZHLicoAj_ywnJ_92h0ry0gTgKudmuDmjm\n",
      "https://gateway-lon.watsonplatform.net/personality-insights/api\n",
      "'''\n",
      "43/49:\n",
      "user_personalities = pd.DataFrame(columns=('user', 'emailAddress', \n",
      "                                           'openness', 'o_raw', 'o_sign',\n",
      "                                           'conscientiousness', 'c_raw', 'c_sign',\n",
      "                                           'extraversion', 'e_raw', 'e_sign',\n",
      "                                           'agreeableness', 'a_raw', 'a_sign',\n",
      "                                           'neuroticism', 'n_raw', 'n_sign'))\n",
      "\n",
      "for i in range(0, valid_user_texts_unique.shape[0]):\n",
      "    user_ = valid_user_texts_unique.iloc[i]['user']\n",
      "    email_ = valid_user_texts_unique.iloc[i]['emailAddress']\n",
      "    text_ = valid_user_texts_unique.iloc[i]['text']\n",
      "    \n",
      "    # define destination json structure Save input JSON file for each user \n",
      "    data = {}\n",
      "    data['contentItems'] = []\n",
      "    data['contentItems'].append({\n",
      "        'content': text_,\n",
      "        'contenttype': 'application/json',\n",
      "        'created': datetime.now().toordinal(),\n",
      "        'id': i,\n",
      "        'language':'en',\n",
      "        'user_name':user_,\n",
      "        'email':email_\n",
      "    })\n",
      "\n",
      "    input_json_file = 'json_input/' + user_ + ';' + email_ + '.json'\n",
      "    with open(input_json_file, 'w') as outfile:\n",
      "        json.dump(data, outfile)\n",
      "\n",
      "    # Call IBM Watson Personality Insights for each user \n",
      "    with open(join(dirname('__file__'), input_json_file)) as \\\n",
      "        profile_json:\n",
      "        profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=False).get_result()\n",
      "\n",
      "    # Save the personality insights result into output JSON file for each user\n",
      "    output_json_file = 'json_output/' + user_ + ';' + email_ + '_result.json'\n",
      "    with open(output_json_file, 'w') as outfile:\n",
      "        json.dump(profile, outfile)\n",
      "    \n",
      "    # parse JSON, loop through each personality and get the scores\n",
      "    personalities = []\n",
      "    for p in range(0, 5):\n",
      "        trait_id = profile['personality'][p]['name']\n",
      "        percentile = profile['personality'][p]['percentile']\n",
      "        significant = profile['personality'][p]['significant']\n",
      "        raw_score = profile['personality'][p]['raw_score']\n",
      "\n",
      "        if trait_id == 'Openness':\n",
      "            big5_openness = percentile\n",
      "            big5_o_raw = raw_score\n",
      "            big5_o_sign = significant\n",
      "        elif trait_id=='Conscientiousness':\n",
      "            big5_conscientiousness = percentile\n",
      "            big5_c_raw = raw_score\n",
      "            big5_c_sign = significant\n",
      "        elif trait_id == 'Extraversion':\n",
      "            big5_extraversion = percentile\n",
      "            big5_e_raw = raw_score\n",
      "            big5_e_sign = significant\n",
      "        elif trait_id == 'Agreeableness':\n",
      "            big5_agreeableness = percentile\n",
      "            big5_a_raw = raw_score\n",
      "            big5_a_sign = significant\n",
      "        elif trait_id == 'Emotional range':\n",
      "            big5_neuroticism = percentile\n",
      "            big5_n_raw = raw_score\n",
      "            big5_n_sign = significant\n",
      "    \n",
      "    # Save the user personalities into dataset\n",
      "    user_personalities = user_personalities.append([{\n",
      "        'user':user_, 'emailAddress':email_, \n",
      "       'openness':big5_openness, 'o_raw':big5_o_raw, 'o_sign':big5_o_sign,\n",
      "       'conscientiousness':big5_conscientiousness, 'c_raw':big5_c_raw, 'c_sign':big5_c_sign,\n",
      "       'extraversion':big5_extraversion, 'e_raw':big5_e_raw, 'e_sign':big5_e_sign,\n",
      "       'agreeableness':big5_agreeableness, 'a_raw':big5_a_raw, 'a_sign':big5_a_sign,\n",
      "       'neuroticism':big5_neuroticism, 'n_raw':big5_n_raw, 'n_sign':big5_n_sign\n",
      "    }])\n",
      "43/50:\n",
      "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
      "user_personalities.to_csv('user_personalities.csv')\n",
      "user_personalities.head()\n",
      "43/51: user_personalities['openness'].hist()\n",
      "43/52:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "for i, trait_ in enumerate(traits):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(50, 70); ax.set_ylim(0, 1);\n",
      "plt.tight_layout();\n",
      "43/53:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(traits):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(50, 70); ax.set_ylim(0, 1);\n",
      "plt.tight_layout();\n",
      "43/54:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(50, 70); ax.set_ylim(0, 1);\n",
      "plt.tight_layout();\n",
      "43/55:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(50, 70); ax.set_ylim(0, 1);\n",
      "plt.tight_layout();\n",
      "43/56:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(50, 70); ax.set_ylim(0, 1);\n",
      "plt.tight_layout();\n",
      "43/57:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 70); ax.set_ylim(0, 1);\n",
      "plt.tight_layout();\n",
      "43/58:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0, 1);\n",
      "plt.tight_layout();\n",
      "43/59:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0,100);\n",
      "plt.tight_layout();\n",
      "43/60:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0,10);\n",
      "plt.tight_layout();\n",
      "43/61:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,10);\n",
      "plt.tight_layout();\n",
      "43/62:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 1); \n",
      "plt.tight_layout();\n",
      "43/63:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0,20);\n",
      "plt.tight_layout();\n",
      "43/64:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=10, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0,20);\n",
      "plt.tight_layout();\n",
      "43/65:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=10, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/66:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=10, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/67:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=10, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=4, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/68:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=10, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/69:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=10, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title('title here')\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/70:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=10, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/71: x\n",
      "43/72: max(x)\n",
      "43/73:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=10, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, max(x)); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/74:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=10, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 0.01); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/75:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=10, density=True, stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/76:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=10,  stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/77: user_personalities['agreeableness'].hist()\n",
      "43/78:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (ax, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    ax.hist(x, alpha=0.5, bins=10,  stacked=True, label=trait_, color=colors[i])\n",
      "    ax.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/79:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.5, bins=10,  stacked=True, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/80:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x)#, alpha=0.5, bins=10,  stacked=True, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/81:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.5, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/82:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.5, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "    axx.set_xlim(min(x), max(x))\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/83:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.5, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "    axx.set_xlim(0, max(x))\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/84:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=True, sharey=True)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.5, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "    axx.set_xlim(min(x), max(x))\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/85:\n",
      "\n",
      "fig = plt.figure()\n",
      "\n",
      "plt.subplot(5, 1, 1)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "plt.subplot(5, 1, 2)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "plt.subplot(5, 1, 3)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "plt.subplot(5, 1, 4)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "plt.subplot(5, 1, 5)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "43/86:\n",
      "\n",
      "fig = plt.figure()\n",
      "\n",
      "plt.subplot(1, 5, 1)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "plt.subplot(1, 5, 2)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "plt.subplot(1, 5, 3)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "plt.subplot(1, 5, 4)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "plt.subplot(1, 5, 5)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "43/87:\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "fig = plt.figure()\n",
      "\n",
      "plt.subplot(1, 5, 1)\n",
      "user_personalities[traits[0]].hist()\n",
      "\n",
      "plt.subplot(1, 5, 2)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "plt.subplot(1, 5, 3)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "plt.subplot(1, 5, 4)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "plt.subplot(1, 5, 5)\n",
      "user_personalities['openness'].hist()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "43/88:\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "fig = plt.figure()\n",
      "\n",
      "plt.subplot(1, 5, 1)\n",
      "user_personalities[traits[0]].hist()\n",
      "\n",
      "plt.subplot(1, 5, 2)\n",
      "user_personalities[traits[1]].hist()\n",
      "\n",
      "plt.subplot(1, 5, 3)\n",
      "user_personalities[traits[2]].hist()\n",
      "\n",
      "plt.subplot(1, 5, 4)\n",
      "user_personalities[traits[3]].hist()\n",
      "\n",
      "plt.subplot(1, 5, 5)\n",
      "user_personalities[traits[4]].hist()\n",
      "\n",
      "\n",
      "plt.show()\n",
      "43/89:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=False, sharey=False)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.5, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/90:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=False, sharey=False)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/91:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=False, sharey=False)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:yellow', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/92:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=False, sharey=False)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/93:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=False, sharey=False)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:yellow', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/94:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=False, sharey=False)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:black', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/95:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=False, sharey=False)\n",
      "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:purple', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/96:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/97:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/98:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=50, sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/99:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=80, sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/100:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=150, sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/101:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=120, sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/102:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(10,2.5), dpi=100, sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "#for i, (ax, cut) in enumerate(zip(axes.flatten(), df.cut.unique())):\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/103:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(12,4), dpi=100, sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/104:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(12,4), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/105:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(12,3), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=False, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/106:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(12,3), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=10,  stacked=True, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/107:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(12,3), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=20,  stacked=True, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/108:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(12,3), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=50,  stacked=True, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/109:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(12,3), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25,  stacked=True, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/110:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(12,4), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25,  stacked=True, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/111:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(14,4), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25,  stacked=True, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/112:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(1, 5, figsize=(14,3.5), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "\n",
      "    \n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25,  stacked=True, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "#ax.set_xlim(0, 1); ax.set_ylim(0,16);\n",
      "plt.tight_layout();\n",
      "43/113:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 5, figsize=(14,3.5), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism',\n",
      "         'o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "\n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25,  stacked=True, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/114:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 6, figsize=(14,3.5), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism',\n",
      "         'o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "\n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25,  stacked=True, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/115:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 5, figsize=(14,3.5), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism',\n",
      "         'o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "\n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25,  label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/116:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 5, figsize=(14,3.5), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism',\n",
      "         'o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "\n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/117:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 5, figsize=(14,8), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism',\n",
      "         'o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "\n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/118:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 5, figsize=(14,7), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism',\n",
      "         'o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "\n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Probability Histogram of Diamond Depths', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/119:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 5, figsize=(14,7), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism',\n",
      "         'o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "\n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Histogram of personality trait raw scores and percentiles', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/120:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 5, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism',\n",
      "         'o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "\n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Histogram of personality trait raw scores and percentiles', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/121:\n",
      "with open(join(dirname('__file__'), 'data.json')) as \\\n",
      "        profile_json:\n",
      "    profile = service.profile(\n",
      "        profile_json.read(),\n",
      "        'application/json',\n",
      "        raw_scores=True,\n",
      "        consumption_preferences=True).get_result()\n",
      "\n",
      "#print(json.dumps(profile, indent=2))\n",
      "43/122:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 5, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism',\n",
      "         'o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "\n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Histogram of all users personality trait raw scores and percentiles', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/123: valid_user_texts[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=False)\n",
      "43/124:\n",
      "#user_personalities\n",
      "valid_user_texts.head()\n",
      "43/125:\n",
      "user_personalities.head()\n",
      "#valid_user_texts['project', 'user', 'emailAddress']\n",
      "43/126:\n",
      "for col in user_personalities.columns:\n",
      "    print col\n",
      "43/127:\n",
      "for col in user_personalities.columns:\n",
      "    print (col)\n",
      "43/128:\n",
      "p_cols = []\n",
      "for col in user_personalities.columns:\n",
      "    p_cols.append(col)\n",
      "p_cols\n",
      "43/129:\n",
      "user_personalities.head()\n",
      "#valid_user_texts['project', 'user', 'emailAddress']\n",
      "p_cols = ['project']\n",
      "for col in user_personalities.columns:\n",
      "    p_cols.append(col)\n",
      "project_user_personalities = pd.merge(user_personalities, valid_user_texts, how = 'inner',\n",
      "                                   left_on = ['user', 'emailAddress'],\n",
      "                                   right_on = ['user', 'emailAddress'])[p_cols]\n",
      "43/130: project_user_personalities\n",
      "43/131: project_user_personalities.shape[0]\n",
      "43/132: user_personalities.shape[0]\n",
      "43/133: project_user_personalities.shape[0]\n",
      "43/134:\n",
      "user_personalities.head()\n",
      "#valid_user_texts['project', 'user', 'emailAddress']\n",
      "p_cols = ['project']\n",
      "for col in user_personalities.columns:\n",
      "    p_cols.append(col)\n",
      "project_user_personalities = pd.merge(user_personalities, valid_user_texts, how = 'inner',\n",
      "                                   left_on = ['user', 'emailAddress'],\n",
      "                                   right_on = ['user', 'emailAddress'])[p_cols]\n",
      "\n",
      "print(valid_user_texts.shape[0], project_user_personalities.shape[0])\n",
      "43/135:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 5 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            n = n + 1\n",
      "43/136:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(data, x_cols, y_cols)\n",
      "43/137:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/138:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/139:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/140:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 6 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            n = n + 1\n",
      "43/141:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/142:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            n = n + 1\n",
      "43/143:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/144:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            g.suptitle('Personality trait percentiles for projects', y=1.05, size=16)\n",
      "            n = n + 1\n",
      "43/145:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/146:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            g.subtitle('Personality trait percentiles for projects', y=1.05, size=16)\n",
      "            n = n + 1\n",
      "43/147:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/148:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 5, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism',\n",
      "         'o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "\n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Histogram of all users personality trait raw scores and percentiles', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/149:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            #g.subtitle('Personality trait percentiles for projects', y=1.05, size=16)\n",
      "            n = n + 1\n",
      "43/150:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/151:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            plt.subtitle('Personality trait percentiles for projects', y=1.05, size=16)\n",
      "            n = n + 1\n",
      "43/152:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            plt.subtitle('Personality trait percentiles for projects', y=1.05, size=16)\n",
      "            n = n + 1\n",
      "43/153:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/154:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            plt.suptitle('Personality trait percentiles for projects', y=1.05, size=16)\n",
      "            n = n + 1\n",
      "43/155:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/156:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            plt.suptitle('Personality trait percentiles for projects', size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.05)\n",
      "            n = n + 1\n",
      "43/157:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/158:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            plt.suptitle('Personality trait percentiles for projects', size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.5)\n",
      "            n = n + 1\n",
      "43/159:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/160:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            plt.suptitle('Personality trait percentiles for projects', size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.3)\n",
      "            n = n + 1\n",
      "43/161:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            plt.suptitle('Personality trait percentiles for projects', size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.3)\n",
      "            n = n + 1\n",
      "43/162:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/163:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            plt.suptitle('Personality trait percentiles for projects', size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/164:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=20)\n",
      "            plt.suptitle('Personality trait percentiles for projects', size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/165:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/166:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(15, 3 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle('Personality trait percentiles for projects', size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/167:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/168:\n",
      "def boxpl(dt, x_cols, y_cols):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(16, 3 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle('Personality trait percentiles for projects', size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/169:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/170:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "print(boxpl(project_user_personalities, x_cols, y_cols))\n",
      "43/171:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "\n",
      "y_cols = ['o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "x_cols = ['project']\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/172:\n",
      "def boxpl(dt, x_cols, y_cols, title):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(16, 3 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle(title, size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/173:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "title = 'Personality trait percentiles for projects'\n",
      "boxpl(project_user_personalities, x_cols, y_cols, title)\n",
      "\n",
      "y_cols = ['o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "x_cols = ['project']\n",
      "title = 'Perconality trait raw scores for projects'\n",
      "boxpl(project_user_personalities, x_cols, y_cols)\n",
      "43/174:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "title = 'Personality trait percentiles for projects'\n",
      "boxpl(project_user_personalities, x_cols, y_cols, title)\n",
      "\n",
      "y_cols = ['o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "x_cols = ['project']\n",
      "title = 'Perconality trait raw scores for projects'\n",
      "boxpl(project_user_personalities, x_cols, y_cols, title)\n",
      "43/175:\n",
      "def boxpl(dt, x_cols, y_cols, title):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(16, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle(title, size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/176:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "title = 'Personality trait percentiles for projects'\n",
      "boxpl(project_user_personalities, x_cols, y_cols, title)\n",
      "\n",
      "y_cols = ['o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "x_cols = ['project']\n",
      "title = 'Perconality trait raw scores for projects'\n",
      "boxpl(project_user_personalities, x_cols, y_cols, title)\n",
      "43/177:\n",
      "def boxpl(dt, x_cols, y_cols, title):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(17, 4 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle(title, size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/178:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "title = 'Personality trait percentiles for projects'\n",
      "boxpl(project_user_personalities, x_cols, y_cols, title)\n",
      "\n",
      "y_cols = ['o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "x_cols = ['project']\n",
      "title = 'Perconality trait raw scores for projects'\n",
      "boxpl(project_user_personalities, x_cols, y_cols, title)\n",
      "43/179:\n",
      "def boxpl(dt, x_cols, y_cols, title):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(18, 3 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle(title, size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/180:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "title = 'Personality trait percentiles for projects'\n",
      "boxpl(project_user_personalities, x_cols, y_cols, title)\n",
      "\n",
      "y_cols = ['o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "x_cols = ['project']\n",
      "title = 'Perconality trait raw scores for projects'\n",
      "boxpl(project_user_personalities, x_cols, y_cols, title)\n",
      "43/181:\n",
      "def boxpl(dt, x_cols, y_cols, title):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(18, 3.5 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle(title, size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/182: sns.scatter(project_user_personalities['o_raw'])\n",
      "43/183: sns.scatterplot(project_user_personalities['o_raw'])\n",
      "43/184: sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['project'])\n",
      "43/185: sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['openness'])\n",
      "43/186:\n",
      "sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['openness'])\n",
      "sns.scatterplot(project_user_personalities['c_raw'], project_user_personalities['conscieitousness'])\n",
      "43/187:\n",
      "sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['openness'])\n",
      "sns.scatterplot(project_user_personalities['c_raw'], project_user_personalities['conscientiousness'])\n",
      "43/188:\n",
      "sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['openness'])\n",
      "sns.scatterplot(project_user_personalities['c_raw'], project_user_personalities['conscientiousness'])\n",
      "sns.scatterplot(project_user_personalities['e_raw'], project_user_personalities['extraversion'])\n",
      "43/189:\n",
      "sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['openness'])\n",
      "sns.scatterplot(project_user_personalities['c_raw'], project_user_personalities['conscientiousness'])\n",
      "sns.scatterplot(project_user_personalities['e_raw'], project_user_personalities['extraversion'])\n",
      "sns.scatterplot(project_user_personalities['a_raw'], project_user_personalities['agreeableness'])\n",
      "43/190:\n",
      "sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['openness'])\n",
      "sns.scatterplot(project_user_personalities['c_raw'], project_user_personalities['conscientiousness'])\n",
      "sns.scatterplot(project_user_personalities['e_raw'], project_user_personalities['extraversion'])\n",
      "sns.scatterplot(project_user_personalities['a_raw'], project_user_personalities['agreeableness'])\n",
      "sns.scatterplot(project_user_personalities['n_raw'], project_user_personalities['neuroticism'])\n",
      "43/191:\n",
      "sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['openness'])\n",
      "sns.scatterplot(project_user_personalities['c_raw'], project_user_personalities['conscientiousness'])\n",
      "sns.scatterplot(project_user_personalities['e_raw'], project_user_personalities['extraversion'])\n",
      "sns.scatterplot(project_user_personalities['a_raw'], project_user_personalities['agreeableness'])\n",
      "sns.scatterplot(project_user_personalities['n_raw'], project_user_personalities['neuroticism'])\n",
      "43/192:\n",
      "sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['openness'], title = 'o')\n",
      "sns.scatterplot(project_user_personalities['c_raw'], project_user_personalities['conscientiousness'])\n",
      "sns.scatterplot(project_user_personalities['e_raw'], project_user_personalities['extraversion'])\n",
      "sns.scatterplot(project_user_personalities['a_raw'], project_user_personalities['agreeableness'])\n",
      "sns.scatterplot(project_user_personalities['n_raw'], project_user_personalities['neuroticism'])\n",
      "43/193:\n",
      "sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['openness'], 'o')\n",
      "sns.scatterplot(project_user_personalities['c_raw'], project_user_personalities['conscientiousness'])\n",
      "sns.scatterplot(project_user_personalities['e_raw'], project_user_personalities['extraversion'])\n",
      "sns.scatterplot(project_user_personalities['a_raw'], project_user_personalities['agreeableness'])\n",
      "sns.scatterplot(project_user_personalities['n_raw'], project_user_personalities['neuroticism'])\n",
      "43/194:\n",
      "sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['openness'])\n",
      "sns.scatterplot(project_user_personalities['c_raw'], project_user_personalities['conscientiousness'])\n",
      "sns.scatterplot(project_user_personalities['e_raw'], project_user_personalities['extraversion'])\n",
      "sns.scatterplot(project_user_personalities['a_raw'], project_user_personalities['agreeableness'])\n",
      "sns.scatterplot(project_user_personalities['n_raw'], project_user_personalities['neuroticism'])\n",
      "43/195:\n",
      "sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['openness'])\n",
      "sns.legend('')\n",
      "sns.scatterplot(project_user_personalities['c_raw'], project_user_personalities['conscientiousness'])\n",
      "sns.scatterplot(project_user_personalities['e_raw'], project_user_personalities['extraversion'])\n",
      "sns.scatterplot(project_user_personalities['a_raw'], project_user_personalities['agreeableness'])\n",
      "sns.scatterplot(project_user_personalities['n_raw'], project_user_personalities['neuroticism'])\n",
      "43/196:\n",
      "sns.scatterplot(project_user_personalities['o_raw'], project_user_personalities['openness'])\n",
      "sns.scatterplot(project_user_personalities['c_raw'], project_user_personalities['conscientiousness'])\n",
      "sns.scatterplot(project_user_personalities['e_raw'], project_user_personalities['extraversion'])\n",
      "sns.scatterplot(project_user_personalities['a_raw'], project_user_personalities['agreeableness'])\n",
      "sns.scatterplot(project_user_personalities['n_raw'], project_user_personalities['neuroticism'])\n",
      "45/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "45/2:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "45/3: changelog.head(3)\n",
      "45/4: issues.head(3)\n",
      "45/5: sprints.head(3)\n",
      "45/6: users.head(3)\n",
      "45/7: changelog.head(3)\n",
      "45/8:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "45/9:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "45/10:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field', 'author'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "45/11:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "45/12:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "45/13:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove multiple whitespaces (needed for removing 'at at' texts, regex is the next command below)\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\|#)', ' ', str(text))\n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "45/14:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "45/15:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "45/16:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment',\n",
      "                                            'Acceptance Criteria', 'Migration Impact', 'QA Test Plan', 'Out of Scope'])]\n",
      "log_cols.to_csv('io_files/log_cols.csv')\n",
      "45/17:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment',\n",
      "                                            'Acceptance Criteria', 'Migration Impact', 'QA Test Plan', 'Out of Scope'])]\n",
      "log_cols.to_csv('io_files/log_cols.csv')\n",
      "46/1: log_cols = load_csv('io_files/log_cols.csv')\n",
      "46/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "46/3: log_cols = load_csv('io_files/log_cols.csv')\n",
      "46/4: log_cols = pd.read_csv('io_files/log_cols.csv')\n",
      "46/5:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "46/6: log_cols = pd.read_csv('io_files/log_cols.csv')\n",
      "46/7:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "46/8: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "46/9: log_cols.to_csv('log_cols.csv')\n",
      "46/10:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(figsize=(16,10))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/11: hist_with_perc(log_cols['textLength'],30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "46/12:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/13: hist_with_perc(log_cols['textLength'],30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "46/14: hist_with_perc(log_cols['textLength'], log_cols[log_cols['textLength']>=400]['textLength'],30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "46/15:\n",
      "def hist_with_perc(_data1,_data2 _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(2, 2, figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data1, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "    counts, bins, patches = ax.hist(_data2, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    \n",
      "    plt.show()\n",
      "46/16:\n",
      "def hist_with_perc(_data1,_data2, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(2, 2, figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data1, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "    counts, bins, patches = ax.hist(_data2, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    \n",
      "    plt.show()\n",
      "46/17: hist_with_perc(log_cols['textLength'], log_cols[log_cols['textLength']>=400]['textLength'],30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "46/18:\n",
      "def hist_with_perc(_data1,_data2, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(2, 2, figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data1, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "\n",
      "    plt.show()\n",
      "46/19: hist_with_perc(log_cols['textLength'], log_cols[log_cols['textLength']>=400]['textLength'],30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "46/20:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/21: hist_with_perc(log_cols['textLength'], 30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "46/22:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(2, 2, figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/23: hist_with_perc(log_cols['textLength'], 30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "46/24:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/25: hist_with_perc(log_cols['textLength'], 30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "46/26:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    fig, ax = plt.subplots(figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    \n",
      "    fig, ax = plt.subplots(figsize=(16,6))\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/27: hist_with_perc(log_cols['textLength'], 30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "46/28:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    #fig, ax = plt.subplots(figsize=(16,6))\n",
      "    \n",
      "    fig = plt.figure(figsize=(18, 10))\n",
      "    ax = figure.add_suplot(2,2,1)\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    \n",
      "    #fig, ax = plt.subplots(figsize=(16,6))\n",
      "    ax = figure.add_suplot(2,2,2)\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/29: hist_with_perc(log_cols['textLength'], 30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "46/30:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    #fig, ax = plt.subplots(figsize=(16,6))\n",
      "    \n",
      "    fig = plt.figure(figsize=(18, 10))\n",
      "    ax = fig.add_suplot(2,2,1)\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    \n",
      "    #fig, ax = plt.subplots(figsize=(16,6))\n",
      "    ax = fig.add_suplot(2,2,2)\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/31: hist_with_perc(log_cols['textLength'], 30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "46/32:\n",
      "def hist_with_perc(_data, _bins, _title, _xlabel, _ylabel, _color):\n",
      "    #fig, ax = plt.subplots(figsize=(16,6))\n",
      "    \n",
      "    fig = plt.figure(figsize=(18, 10))\n",
      "    ax = fig.add_subplot(2,2,1)\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    \n",
      "    #fig, ax = plt.subplots(figsize=(16,6))\n",
      "    ax = fig.add_subplot(2,2,2)\n",
      "    counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "    ax.set_xticks(bins.round(0))\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "    bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "    for i in range(len(bins)-1):\n",
      "        bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "        plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/33: hist_with_perc(log_cols['textLength'], 30,'Distribution of text lengths in the whole dataset','text length','Count','gold')\n",
      "46/34:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, \n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 10))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/35:\n",
      "hist_with_perc(log_cols['textLength'], 30,'Distribution of text lengths in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],30,'Distribution of the longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],30,'Distribution of the shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,\n",
      "               30,'Distribution of the texts between 5%-95% percentile length in the dataset')\n",
      "46/36:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, \n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 10))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/37:\n",
      "hist_with_perc(log_cols['textLength'], 30,'Distribution of text lengths in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],30,'Distribution of the longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],30,'Distribution of the shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,\n",
      "               30,'Distribution of the texts between 5%-95% percentile length in the dataset')\n",
      "46/38:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.05))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 5% to 95%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 95% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 5% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "46/39:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, \n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 10))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/40:\n",
      "hist_with_perc(log_cols['textLength'], 30,'Distribution of text lengths in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],30,'Distribution of the longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],30,'Distribution of the shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,\n",
      "               30,'Distribution of the texts between 5%-95% percentile length in the dataset')\n",
      "46/41:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, _color,\n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 10))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/42:\n",
      "hist_with_perc(log_cols['textLength'], 30,'Distribution of text lengths in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],30,'Distribution of the longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],30,'Distribution of the shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,\n",
      "               30,'Distribution of the texts between 5%-95% percentile length in the dataset')\n",
      "46/43:\n",
      "hist_with_perc(log_cols['textLength'], 20,'Distribution of text lengths in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],20,'Distribution of the longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],20,'Distribution of the shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,20,'Distribution of the texts between 5%-95% percentile length in the dataset')\n",
      "46/44:\n",
      "hist_with_perc(log_cols['textLength'], 20,'Distribution of text lengths <br/> in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],20,'Distribution of the longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],20,'Distribution of the shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,20,'Distribution of the texts between 5%-95% percentile length in the dataset')\n",
      "46/45:\n",
      "hist_with_perc(log_cols['textLength'], 20,'Distribution of text lengths in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],20,'Distribution of the longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],20,'Distribution of the shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,20,'Distribution of the texts between 5%-95% percentile length in the dataset')\n",
      "46/46:\n",
      "hist_with_perc(log_cols['textLength'], 20,'Distribution of text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],20,'Distribution of the longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],20,'Distribution of the shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,20,'Distribution of the texts between 5%-95% percentile length in the dataset')\n",
      "46/47:\n",
      "hist_with_perc(log_cols['textLength'], 20,'Distribution of text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],20,'Distribution of the \\n longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],20,'Distribution of the \\n shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,20,'Distribution of the texts \\n between 5%-95% percentile length in the dataset')\n",
      "46/48:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, _color,\n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/49:\n",
      "hist_with_perc(log_cols['textLength'], 20,'Distribution of text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],20,'Distribution of the \\n longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],20,'Distribution of the \\n shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,20,'Distribution of the texts \\n between 5%-95% percentile length in the dataset')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/50:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, _color,\n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/51:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, _color,\n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/52:\n",
      "hist_with_perc(log_cols['textLength'], 20,'Distribution of text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],20,'Distribution of the \\n longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],20,'Distribution of the \\n shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,20,'Distribution of the texts \\n between 5%-95% percentile length in the dataset')\n",
      "46/53:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, _color,\n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.4)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/54:\n",
      "hist_with_perc(log_cols['textLength'], 20,'Distribution of text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],20,'Distribution of the \\n longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],20,'Distribution of the \\n shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,20,'Distribution of the texts \\n between 5%-95% percentile length in the dataset')\n",
      "46/55:\n",
      "hist_with_perc(log_cols['textLength'], 15,'Distribution of text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],15,'Distribution of the \\n longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],15,'Distribution of the \\n shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,15,'Distribution of the texts \\n between 5%-95% percentile length in the dataset')\n",
      "46/56:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, _color,\n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(16, 10))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.4)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/57:\n",
      "hist_with_perc(log_cols['textLength'], 15,'Distribution of text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],15,'Distribution of the \\n longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],15,'Distribution of the \\n shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,15,'Distribution of the texts \\n between 5%-95% percentile length in the dataset')\n",
      "46/58:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, _color,\n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(16, 10))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/59:\n",
      "hist_with_perc(log_cols['textLength'], 15,'Distribution of text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],15,'Distribution of the \\n longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],15,'Distribution of the \\n shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,15,'Distribution of the texts \\n between 5%-95% percentile length in the dataset')\n",
      "46/60:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, _color,\n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(16, 10))\n",
      "    plt.title('distribution of')\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/61:\n",
      "hist_with_perc(log_cols['textLength'], 15,'Distribution of text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],15,'Distribution of the \\n longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],15,'Distribution of the \\n shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,15,'Distribution of the texts \\n between 5%-95% percentile length in the dataset')\n",
      "46/62:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, _color,\n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(16, 10))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "46/63:\n",
      "hist_with_perc(log_cols['textLength'], 15,'Distribution of text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],15,'Distribution of the \\n longest 5%  texts lengths in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],15,'Distribution of the \\n shortest 5%  texts lengths in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,15,'Distribution of the texts \\n between 5%-95% percentile length in the dataset')\n",
      "46/64:\n",
      "hist_with_perc(log_cols['textLength'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],15,'the \\n shortest 5%  texts lengths \\n in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "46/65:\n",
      "hist_with_perc(log_cols['textLength'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "43/197:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.05))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 5% to 95%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 95% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 5% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "43/198:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, _color,\n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(16, 10))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "43/199:\n",
      "hist_with_perc(log_cols['textLength'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "43/200:\n",
      "#detect the number of rows that fall in top % and bottom %\n",
      "top= 0.01\n",
      "bottom = 0.02\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of top '+str(round(top*100))+'% of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of bottom '+str(round(bottom*100))+'% of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "#log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "#log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "43/201: changelog.head()\n",
      "43/202: changelog.field.unique()\n",
      "43/203: issues.head()\n",
      "43/204: changelog[changelog['field']=='status']\n",
      "43/205: changelog[changelog['field']=='status'].head()\n",
      "43/206: changelog[changelog['field']=='status']['toString'].unique()\n",
      "43/207: issues[issues['field']=='status'].head()\n",
      "43/208: issues['fields.status.name'].unique()\n",
      "43/209: issues['fields.status.Category.name'].unique()\n",
      "43/210: issues['fields.status.Categoryname'].unique()\n",
      "43/211: issues.head()\n",
      "43/212:\n",
      "print(issues['fields.status.name'].unique())\n",
      "print(issues['fields.status.statusCategory.name'].unique())\n",
      "43/213: print(issues['fields.status.statusCategory.name'].unique())\n",
      "43/214:\n",
      "changelog[changelog['field']=='status']['toString'].unique()\n",
      "# Done, Resolved, Closed,\n",
      "43/215: issues[issues['fields.status.statusCategory.name']=='Done']['fields.status.name'].unique()\n",
      "43/216:\n",
      "changelog[changelog['field']=='status']['toString'].unique()\n",
      "# Done, Resolved, Closed,\n",
      "43/217: issues[issues['fields.status.statusCategory.name'].isin('Done', 'Complete')]['fields.status.name'].unique()\n",
      "43/218: issues[issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])]['fields.status.name'].unique()\n",
      "43/219: issues[issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])]['fields.status.name'].unique()\n",
      "43/220: issues[issues['fields.status.statusCategory.name'].isin(['Complete'])]['fields.status.name'].unique()\n",
      "43/221: issues[issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])]['fields.status.name'].unique()\n",
      "43/222: issues.groupby(['fields.status.statusCategory.name', 'fields.status.name']).count()\n",
      "43/223: issues[['fields.status.statusCategory.name', 'fields.status.name', 'project']].groupby(['fields.status.statusCategory.name', 'fields.status.name']).count()\n",
      "43/224:\n",
      "issues[(issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])) &\n",
      "      (issues['fields.status.name'].isin('['Closed', 'Resolved', 'Done']'))]\n",
      "43/225:\n",
      "issues[(issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])) &\n",
      "      (issues['fields.status.name'].isin(['Closed', 'Resolved', 'Done']))]\n",
      "43/226:\n",
      "issues[(issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])) &\n",
      "      (issues['fields.status.name'].isin(['Closed', 'Resolved', 'Done']))].shape[0]\n",
      "43/227:\n",
      "print(issues.shape[0])\n",
      "print(issues[(issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])) &\n",
      "      (issues['fields.status.name'].isin(['Closed', 'Resolved', 'Done']))].shape[0])\n",
      "43/228:\n",
      "print(issues.shape[0])\n",
      "print(issues[(issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])) &\n",
      "      (issues['fields.status.name'].isin(['Closed', 'Resolved', 'Done']))].shape[0])\n",
      "43/229:\n",
      "closed_issues = issues[(issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])) &\n",
      "      (issues['fields.status.name'].isin(['Closed', 'Resolved', 'Done']))]\n",
      "closed_issues.head()\n",
      "43/230:\n",
      "closed_issues = issues[(issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])) &\n",
      "      (issues['fields.status.name'].isin(['Closed', 'Resolved', 'Done']))]\n",
      "closed_issues.key.unique()\n",
      "43/231:\n",
      "closed_issues = issues[(issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])) &\n",
      "      (issues['fields.status.name'].isin(['Closed', 'Resolved', 'Done']))]\n",
      "closed_issues.key.unique().shape[0]\n",
      "43/232: closed_issues.key.unique().shape[0]\n",
      "43/233: closed_issues.head()\n",
      "43/234:\n",
      "print(issues.shape[0])\n",
      "print(issues[(issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])) &\n",
      "      (issues['fields.status.name'].isin(['Closed', 'Resolved', 'Done']))].shape[0])\n",
      "\n",
      "closed_issues = issues[(issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])) &\n",
      "      (issues['fields.status.name'].isin(['Closed', 'Resolved', 'Done']))]\n",
      "closed_issues.key.unique().shape[0]\n",
      "43/235: closed_issues[closed_issues['key']=='DNN-27698']\n",
      "43/236: closed_issues[closed_issues['key']=='DNN-27698'].to_csv('keycheck.csv')\n",
      "43/237:\n",
      "print(issues.shape[0])\n",
      "print(issues[(issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])) &\n",
      "      (issues['fields.status.name'].isin(['Closed', 'Resolved', 'Done']))].shape[0])\n",
      "\n",
      "closed_issues = issues[(issues['fields.status.statusCategory.name'].isin(['Done', 'Complete'])) &\n",
      "      (issues['fields.status.name'].isin(['Closed', 'Resolved', 'Done']))]\n",
      "print(closed_issues.key.unique().shape[0])\n",
      "print(closed_issues.drop_duplicates().shape[0])\n",
      "43/238: closed_issues = closed_issues.drop_duplicates()\n",
      "43/239: closed_issues.shape[0]\n",
      "43/240: closed_issues.key.unique().shape[0]\n",
      "43/241: closed_issues.groupby('key').count()\n",
      "43/242: closed_issues]'key'].groupby('key').count()\n",
      "43/243: closed_issues[]'key'].groupby('key').count()\n",
      "43/244: closed_issues['key'].groupby('key').count()\n",
      "43/245: closed_issues['key','project'].groupby('key').count()\n",
      "43/246: closed_issues[['key','project']].groupby('key').count()\n",
      "43/247: closed_issues[['key','project']].groupby('key').count()>1\n",
      "43/248: closed_issues[['key','project']].groupby('key').count\n",
      "43/249: closed_issues[['key','project']].groupby('key').size\n",
      "43/250: closed_issues[['key','project']].groupby('key').size()\n",
      "43/251: closed_issues[closed_issues[['key','project']].groupby('key').size()>1]\n",
      "43/252: closed_issues[closed_issues[['key','project']].groupby('key').count()>1]\n",
      "43/253: closed_issues[['key','project']].groupby('key').shape()\n",
      "43/254: closed_issues[['key','project']].groupby('key').size()\n",
      "43/255: closed_issues.groupby('key').size()\n",
      "43/256: closed_issues.groupby('key').shape()\n",
      "43/257: closed_issues.groupby('key').count()\n",
      "43/258: closed_issues.groupby('key').size()\n",
      "43/259:\n",
      "k = closed_issues.groupby('key').size()\n",
      "k\n",
      "43/260: k = closed_issues.groupby('key').count()\n",
      "43/261:\n",
      "k = closed_issues.groupby('key').count()\n",
      "k\n",
      "43/262:\n",
      "k = closed_issues.groupby('key').count()\n",
      "k[k['project']>1]\n",
      "43/263:\n",
      "k = closed_issues.groupby('key').count()\n",
      "k[k['project']>2]\n",
      "43/264:\n",
      "k = closed_issues.groupby('key').count()\n",
      "k[k['project']>1]\n",
      "\n",
      "\n",
      "#'TIMOB-14806'\n",
      "#'TIMOB-13520'\n",
      "43/265:\n",
      "k = closed_issues.groupby('key').count()\n",
      "k[k['project']>1]\n",
      "\n",
      "\n",
      "#'TIMOB-14806'\n",
      "#'TIMOB-13520'\n",
      "# 'TIMOB-8832'\n",
      "closed_issues[closed_issues['key'].isin(['TIMOB-14806', 'TIMOB-13520', 'TIMOB-8832'])].to_csv('keycheck.csv')\n",
      "43/266: closed_issues.head()\n",
      "43/267: closed_issues[['project','key', 'fields.status.name']].drop_duplicates()\n",
      "43/268: closed_issues[['project','key', 'fields.status.name']].drop_duplicates().shape[0]\n",
      "43/269:\n",
      "gr_closed_issues = closed_issues.groupby((['project','key', 'fields.status.name'])).agg({'sprint':'max'})\n",
      "gr_closed_issues.reset_index(level= [0,1,2], inplace=True)\n",
      "43/270: gr_closed_issues.shape[0]\n",
      "43/271: gr_closed_issues.head()\n",
      "43/272: gr_closed_issues[pd.isnull(gr_closed_issues['sprint'])==True]\n",
      "43/273: gr_closed_issues.loc([pd.isnull(gr_closed_issues['sprint'])==True], 'sprint')\n",
      "43/274: gr_closed_issues.loc(pd.isnull(gr_closed_issues['sprint'])==True, 'sprint')\n",
      "43/275: gr_closed_issues.loc(gr_closed_issues[pd.isnull(gr_closed_issues['sprint'])==True], 'sprint')\n",
      "43/276: gr_closed_issues.iloc(gr_closed_issues[pd.isnull(gr_closed_issues['sprint'])==True], 'sprint')\n",
      "43/277: gr_closed_issues[pd.isnull(gr_closed_issues['sprint'])==True]\n",
      "43/278: gr_closed_issues.loc(pd.isnull(gr_closed_issues['sprint'])==True, 'sprint')\n",
      "43/279: gr_closed_issues[pd.isnull(gr_closed_issues['sprint'])==True]\n",
      "43/280: gr_closed_issues.loc(gr_closed_issues[pd.isnull(gr_closed_issues['sprint'])==True], gr_closed_issues['sprint'])\n",
      "43/281: gr_closed_issues.loc[gr_closed_issues[pd.isnull(gr_closed_issues['sprint'])==True], gr_closed_issues['sprint']]\n",
      "43/282: gr_closed_issues.loc[gr_closed_issues[pd.isnull(gr_closed_issues['sprint'])==True], 'sprint']\n",
      "43/283: gr_closed_issues.loc[pd.isnull(gr_closed_issues['sprint'])==True, 'sprint']\n",
      "43/284: gr_closed_issues.loc[pd.isnull(gr_closed_issues['sprint'])==True, 'sprint']='-'\n",
      "43/285: gr_closed_issues.head()\n",
      "43/286: changelog.head()\n",
      "43/287: changelog.groupby(['key', 'project']).agg({'created':'min', 'created':'max'})\n",
      "43/288: changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max]})\n",
      "43/289: changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max]}, 'author':'sum')\n",
      "43/290: changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max], 'author':'sum'})\n",
      "43/291: changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max], 'author':lambda x: ','.join(x.unique())})\n",
      "43/292: changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max], 'author':(lambda x: ','.join(x.unique()))})\n",
      "43/293:\n",
      "changelog.groupby(['key', 'project']).agg(first_date=('created', 'min')\n",
      "                                          , last_date=('created', 'max')\n",
      "                                          , authors=('author',', '.join))\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/294:\n",
      "changelog.groupby(['key', 'project']).agg(first_date=('created', 'min')\n",
      "                                          , last_date=('created', 'max')\n",
      "                                          , authors=('author',', '.join)).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/295:\n",
      "changelog.groupby(['key', 'project']).agg({first_date=('created', 'min')\n",
      "                                          , last_date=('created', 'max')\n",
      "                                          , authors=('author',', '.join)}).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/296:\n",
      "changelog.groupby(['key', 'project']).agg({first_date:('created', 'min')\n",
      "                                          , last_date:('created', 'max')\n",
      "                                          , authors:('author',', '.join)}).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/297:\n",
      "changelog.groupby(['key', 'project']).agg({'first_date':('created', 'min')\n",
      "                                          , 'last_date':('created', 'max')\n",
      "                                          , 'authors':('author',', '.join)}).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/298:\n",
      "changelog.groupby(['key', 'project']).agg({('created', 'min')\n",
      "                                          , ('created', 'max')\n",
      "                                          , ('author',', '.join)}).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/299:\n",
      "changelog.groupby(['key', 'project']).agg({'created', 'min'\n",
      "                                          , 'created', 'max'\n",
      "                                          , ('author',', '.join)}).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/300:\n",
      "changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max]\n",
      "                                          , ('author',', '.join)}).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/301:\n",
      "changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max]\n",
      "                                          , 'author' :', '.join}).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/302:\n",
      "changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max]\n",
      "                                          , str('author') :', '.join}).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/303:\n",
      "changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max]}).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/304:\n",
      "changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max], 'author':[np.sum]}).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/305:\n",
      "changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max], str('author'):[np.sum]}).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/306:\n",
      "changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max]}).reset_index()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/307:\n",
      "changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max]}).reset_index().head()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/308: changelog.groupby(['key', 'project'])['author'].agg(lambda x: ','.join(x.unique()))\n",
      "43/309: changelog.groupby(['key', 'project'])['author'].agg(lambda x: ','.join(str(x).unique()))\n",
      "43/310: changelog.groupby(['key', 'project'])['author'].agg(lambda x: ','.join(x.to_string().unique()))\n",
      "43/311: changelog.groupby(['key', 'project'])['author'].agg(lambda x: ','.join(to_string(x).unique()))\n",
      "43/312: changelog.groupby(['key', 'project'])['author'].agg(lambda x: ','.join(x.unique()))\n",
      "43/313:\n",
      "changelog.fillna('').groupby(['key', 'project']).agg({'created':[np.min, np.max]}).reset_index().head()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/314:\n",
      "changelog.groupby(['key', 'project']).agg({'created':[np.min, np.max]}).reset_index().head()\n",
      "#    , 'author':(lambda x: ','.join(x.unique()))})\n",
      "\n",
      "\n",
      "#agg(B_sum=('B','sum'),\n",
      "#                          C=('C', ', '.join)).reset_index()\n",
      "43/315: changelog.fillna('').groupby(['key', 'project'])['author'].agg(lambda x: ','.join(x.unique()))\n",
      "43/316: changelog.fillna('').groupby(['key', 'project'])['author'].agg(lambda x: ','.join(x.unique())).head(3)\n",
      "43/317: changelog[changelog['key']=='APSTUD-1378']\n",
      "43/318: changelog[changelog['key']=='APSTUD-1378'].sort_values('created', ascending=False)\n",
      "43/319: profile\n",
      "43/320: closed_issues.columns\n",
      "43/321: closed_issues['fields.resolutiondate']\n",
      "43/322: closed_issues.head()\n",
      "43/323: unique_issues=issues.drop_duplicates()\n",
      "43/324: unique_issues.shape[0]\n",
      "43/325:\n",
      "k = unique_issues.groupby('key').count()\n",
      "k[k['project']>1]\n",
      "\n",
      "\n",
      "#'TIMOB-14806'\n",
      "#'TIMOB-13520'\n",
      "# 'TIMOB-8832'\n",
      "#closed_issues[closed_issues['key'].isin(['TIMOB-14806', 'TIMOB-13520', 'TIMOB-8832'])].to_csv('keycheck.csv')\n",
      "43/326:\n",
      "k = unique_issues.groupby('key').count()\n",
      "#k[k['project']>1]\n",
      "\n",
      "\n",
      "#'TIMOB-14806'\n",
      "#'TIMOB-13520'\n",
      "# 'TIMOB-8832'\n",
      "#closed_issues[closed_issues['key'].isin(['TIMOB-14806', 'TIMOB-13520', 'TIMOB-8832'])].to_csv('keycheck.csv')\n",
      "43/327: k.head()\n",
      "43/328: unique_issues.head()\n",
      "43/329:\n",
      "gr_closed_issues = unique_issues.groupby((['project','key', 'fields.status.name'])).agg({'sprint':'max'})\n",
      "gr_closed_issues.reset_index(level= [0,1,2], inplace=True)\n",
      "43/330: gr_closed_issues.loc[pd.isnull(gr_closed_issues['sprint'])==True, 'sprint']='-'\n",
      "43/331: gr_closed_issues.head()\n",
      "43/332: changelog.head()\n",
      "43/333: changelog[changelog['key']=='APSTUD-1378'].sort_values('created', ascending=False)\n",
      "43/334: changelog[changelog['key']=='APSTUD-1378'].sort_values('created', ascending=False).head()\n",
      "43/335: changelog.field.unique()\n",
      "43/336: changelog.field.unique().sort_values()\n",
      "43/337: changelog.sort_values('field').field.unique()\n",
      "43/338: changelog[changelog['field']=='status']\n",
      "43/339: changelog[(changelog['field']=='status') & (changelog['fromString']=='In Progress')]\n",
      "43/340: changelog_inprogress = changelog[(changelog['field']=='status') & (changelog['fromString']=='In Progress')]\n",
      "43/341:\n",
      "changelog_inprogress = changelog[(changelog['field']=='status') & (changelog['fromString']=='In Progress')]\n",
      "changelog_inprogress.head()\n",
      "43/342:\n",
      "changelog_from_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['fromString']=='In Progress')\n",
      "                                    & (changelog['toString']<>'In Progress')]\n",
      "changelog_to_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['toString']=='In Progress') \n",
      "                                    & (changelog['fromString']<>'In Progress')]\n",
      "changelog_from_inprogress.head()\n",
      "43/343:\n",
      "changelog_from_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['fromString']=='In Progress')\n",
      "                                    & (changelog['toString']!='In Progress')]\n",
      "changelog_to_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['toString']=='In Progress') \n",
      "                                    & (changelog['fromString']!='In Progress')]\n",
      "changelog_from_inprogress.head()\n",
      "43/344:\n",
      "changelog_from_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['fromString']=='In Progress')\n",
      "                                    & (changelog['toString']!='In Progress')]\n",
      "changelog_to_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['toString']=='In Progress') \n",
      "                                    & (changelog['fromString']!='In Progress')]\n",
      "changelog_to_inprogress.head()\n",
      "43/345: import pandasql as ps\n",
      "43/346: import pandasql as ps\n",
      "43/347:\n",
      "q1 = \"\"\"SELECT * FROM changelog_to_inprogress \"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/348:\n",
      "q1 = \"\"\"SELECT top 10 * FROM changelog_to_inprogress \"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/349:\n",
      "q1 = \"\"\"SELECT * FROM changelog_to_inprogress limit 10\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/350:\n",
      "q1 = \"\"\"SELECT * FROM changelog_to_inprogress limit 5\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/351:\n",
      "q1 = \"\"\"SELECT * FROM changelog_to_inprogress AS A\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/352:\n",
      "q1 = \"\"\"\n",
      "SELECT * \n",
      "FROM changelog_to_inprogress AS TO \n",
      "    \"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/353:\n",
      "q1 = \"\"\"\n",
      "SELECT * \n",
      "FROM changelog_to_inprogress AS TO\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/354:\n",
      "q1 = \"\"\"\n",
      "SELECT * \n",
      "FROM changelog_to_inprogress \"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/355:\n",
      "q1 = \"\"\"\n",
      "SELECT * \n",
      "FROM changelog_to_inprogress as A\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/356:\n",
      "q1 = \"\"\"\n",
      "SELECT * \n",
      "FROM changelog_to_inprogress as T\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/357:\n",
      "q1 = \"\"\"\n",
      "SELECT * \n",
      "FROM changelog_to_inprogress as TO\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/358:\n",
      "q1 = \"\"\"\n",
      "SELECT * \n",
      "FROM changelog_to_inprogress as to\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/359:\n",
      "q1 = \"\"\"\n",
      "SELECT * \n",
      "FROM changelog_to_inprogress as t\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/360:\n",
      "q1 = \"\"\"\n",
      "SELECT * \n",
      "FROM changelog_to_inprogress as T\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/361:\n",
      "q1 = \"\"\"\n",
      "SELECT * \n",
      "FROM changelog_to_inprogress AS T\n",
      "LEFT JOIN changelog_to_inprogress AS F\n",
      "    ON T.key = F.key\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/362:\n",
      "import pandasql as ps\n",
      "print(changelog_to_inprogress.shape[0], changelog_from_inprogress.shape[0])\n",
      "43/363:\n",
      "q1 = \"\"\"\n",
      "SELECT * \n",
      "FROM changelog_to_inprogress AS T\n",
      "LEFT JOIN changelog_to_inprogress AS F\n",
      "    ON T.key = F.key AND T.project = F.project\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/364:\n",
      "q1 = \"\"\"\n",
      "SELECT * \n",
      "FROM changelog_to_inprogress AS T\n",
      "LEFT JOIN changelog_to_inprogress AS F\n",
      "    ON (1 = 1)\n",
      "    AND T.key = F.key \n",
      "    AND T.project = F.project \n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/365:\n",
      "q1 = \"\"\"\n",
      "SELECT count(*)\n",
      "FROM changelog_to_inprogress AS T\n",
      "LEFT JOIN changelog_to_inprogress AS F\n",
      "    ON (1 = 1)\n",
      "    AND T.key = F.key \n",
      "    AND T.project = F.project \n",
      "     \n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/366:\n",
      "q1 = \"\"\"\n",
      "SELECT count(*)\n",
      "FROM changelog_from_inprogress AS F\n",
      "LEFT JOIN changelog_to_inprogress AS T\n",
      "    ON (1 = 1)\n",
      "    AND F.key = T.key \n",
      "    AND F.project = T.project \n",
      "    AND F.\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/367:\n",
      "q1 = \"\"\"\n",
      "SELECT count(*)\n",
      "FROM changelog_from_inprogress AS F\n",
      "LEFT JOIN changelog_to_inprogress AS T\n",
      "    ON (1 = 1)\n",
      "    AND F.key = T.key \n",
      "    AND F.project = T.project \n",
      "    --AND F.\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/368:\n",
      "q1 = \"\"\"\n",
      "SELECT count(*)\n",
      "FROM changelog_from_inprogress AS F\n",
      "LEFT JOIN changelog_to_inprogress AS T\n",
      "    ON (1 = 1)\n",
      "    AND F.key = T.key \n",
      "    AND F.project = T.project \n",
      "    AND F.created > T.created\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/369:\n",
      "q1 = \"\"\"\n",
      "SELECT count(*)\n",
      "FROM changelog_from_inprogress AS F\n",
      "CRPSS APPLY (\n",
      "    SELECT * \n",
      "    FROM changelog_to_inprogress AS T\n",
      "    WHERE (1 = 1)\n",
      "        AND F.key = T.key \n",
      "        AND F.project = T.project \n",
      "        AND F.created > T.created\n",
      "    LIMIT 1) AS T\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/370:\n",
      "q1 = \"\"\"\n",
      "SELECT count(*)\n",
      "FROM changelog_from_inprogress AS F\n",
      "CROSS APPLY (\n",
      "    SELECT * \n",
      "    FROM changelog_to_inprogress AS T\n",
      "    WHERE (1 = 1)\n",
      "        AND F.key = T.key \n",
      "        AND F.project = T.project \n",
      "        AND F.created > T.created\n",
      "    LIMIT 1) AS T\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/371:\n",
      "q1 = \"\"\"\n",
      "SELECT count(*)\n",
      "FROM changelog_from_inprogress AS F\n",
      "OUTER APPLY (\n",
      "    SELECT * \n",
      "    FROM changelog_to_inprogress AS T\n",
      "    WHERE (1 = 1)\n",
      "        AND F.key = T.key \n",
      "        AND F.project = T.project \n",
      "        AND F.created > T.created\n",
      "    LIMIT 1) AS T\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/372:\n",
      "q1 = \"\"\"\n",
      "SELECT count(*)\n",
      "FROM changelog_from_inprogress AS F\n",
      "OUTTER APPLY (\n",
      "    SELECT * \n",
      "    FROM changelog_to_inprogress AS T\n",
      "    WHERE (1 = 1)\n",
      "        AND F.key = T.key \n",
      "        AND F.project = T.project \n",
      "        AND F.created > T.created\n",
      "    LIMIT 1) AS T\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/373:\n",
      "for _key in changelog_from_inprogress['key']:\n",
      "    key\n",
      "43/374:\n",
      "for _key in changelog_from_inprogress['key']:\n",
      "    _key\n",
      "43/375:\n",
      "for _key in changelog_from_inprogress['key']:\n",
      "    _key\n",
      "print _key\n",
      "43/376:\n",
      "for _key in changelog_from_inprogress['key']:\n",
      "    _key\n",
      "print (_key)\n",
      "43/377:\n",
      "i = 0\n",
      "for _key in changelog_from_inprogress['key']:\n",
      "    _key\n",
      "    i = i + 1\n",
      "i\n",
      "43/378:\n",
      "changelog_from_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['fromString']=='In Progress')\n",
      "                                    & (changelog['toString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['toString']=='In Progress') \n",
      "                                    & (changelog['fromString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress.head()\n",
      "43/379:\n",
      "for row in changelog_from_inprogress.rows:\n",
      "    \n",
      "    _key\n",
      "43/380:\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    print(index, row)\n",
      "43/381:\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    index, row\n",
      "43/382:\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    index, row\n",
      "print(index, row)\n",
      "43/383:\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    index, row\n",
      "print(index, row)\n",
      "43/384:\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    index, row\n",
      "print(index)\n",
      "43/385:\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    index, row\n",
      "print(row)\n",
      "43/386:\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    index, row\n",
      "print(row['key'])\n",
      "43/387:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project)\n",
      "                            & (changelog_to_inprogress['created'] < _created)].sort_values('created', ascending=False).head(1)\n",
      "    \n",
      "    changelog_from_inprogress.iloc[index, 'prev_status'] = _to_row['fromString']\n",
      "    changelog_from_inprogress.iloc[index, 'prev_status_created'] = _to_row['created']\n",
      "43/388:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project)\n",
      "                            & (changelog_to_inprogress['created'] < _created)].sort_values('created', ascending=False).head(1)\n",
      "    \n",
      "    changelog_from_inprogress.loc[index, 'prev_status'] = _to_row['fromString']\n",
      "    changelog_from_inprogress.loc[index, 'prev_status_created'] = _to_row['created']\n",
      "43/389: changelog_from_inprogress.head()\n",
      "43/390: changelog_from_inprogress.loc[1]\n",
      "43/391: changelog_from_inprogress.loc[1]['author']\n",
      "43/392:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project)\n",
      "                            & (changelog_to_inprogress['created'] < _created)].sort_values('created', ascending=False).head(1)\n",
      "    \n",
      "    changelog_from_inprogress.loc[index]['prev_status'] = _to_row['fromString']\n",
      "    changelog_from_inprogress.loc[index]['prev_status_created'] = _to_row['created']\n",
      "43/393: changelog_from_inprogress.head()\n",
      "43/394: changelog_from_inprogress.loc[1]['to']\n",
      "43/395: changelog_from_inprogress.loc[1]['to']=1000\n",
      "43/396: changelog_from_inprogress.iloc[1]['to']\n",
      "43/397: changelog_from_inprogress.iloc[1]['to']=1000\n",
      "43/398: changelog_from_inprogress.iloc[1,'to']\n",
      "43/399: changelog_from_inprogress.loc[1,'to']\n",
      "43/400: changelog_from_inprogress.loc[1,'to']='1000'\n",
      "43/401:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project)\n",
      "                            & (changelog_to_inprogress['created'] < _created)].sort_values('created', ascending=False).head(1)\n",
      "    \n",
      "    changelog_from_inprogress.loc[index,'prev_status'] = _to_row['fromString']\n",
      "    changelog_from_inprogress.loc[index,'prev_status_created'] = _to_row['created']\n",
      "43/402:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project)\n",
      "                            & (changelog_to_inprogress['created'] < _created)].sort_values('created', ascending=False).head(1)\n",
      "    \n",
      "    changelog_from_inprogress.loc[index,'prev_status'] = _to_row['fromString'].head(1)\n",
      "    changelog_from_inprogress.loc[index,'prev_status_created'] = _to_row['created'].head(1)\n",
      "43/403:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project)\n",
      "                            & (changelog_to_inprogress['created'] < _created)].sort_values('created', ascending=False).head(1)\n",
      "    \n",
      "    changelog_from_inprogress.loc[index,'prev_status'] = _to_row['fromString'].head(1)\n",
      "    changelog_from_inprogress.loc[index,'prev_status_created'] = _to_row['created'].head(1)\n",
      "43/404: _to_row['fromString']\n",
      "43/405: _to_row.loc['fromString']\n",
      "43/406: _to_row.loc[,'fromString']\n",
      "43/407: _to_row.loc[:,'fromString']\n",
      "43/408: _to_row['fromString']\n",
      "43/409:\n",
      "for i in _to_row['fromString']:\n",
      "    i\n",
      "43/410:\n",
      "for i in _to_row['fromString']:\n",
      "    print(i)\n",
      "43/411:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project)\n",
      "                            & (changelog_to_inprogress['created'] < _created)].sort_values('created', ascending=False).head(1)\n",
      "    for st in _to_row['fromString']:\n",
      "        _prev_st = st \n",
      "    for cr in _to_row['fromString']:\n",
      "        _prev_st_created = cr \n",
      "    \n",
      "    changelog_from_inprogress.loc[index,'prev_status'] = _prev_st\n",
      "    changelog_from_inprogress.loc[index,'prev_status_created'] = _prev_st_created\n",
      "43/412: changelog_from_inprogress.head()\n",
      "43/413:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project)\n",
      "                            & (changelog_to_inprogress['created'] < _created)].sort_values('created', ascending=False).head(1)\n",
      "    for st in _to_row['fromString']:\n",
      "        _prev_st = st \n",
      "    for cr in _to_row['created']:\n",
      "        _prev_st_created = cr \n",
      "    \n",
      "    changelog_from_inprogress.loc[index,'prev_status'] = _prev_st\n",
      "    changelog_from_inprogress.loc[index,'created'] = _prev_st_created\n",
      "43/414: changelog_from_inprogress.head()\n",
      "43/415: changelog_from_inprogress\n",
      "43/416:\n",
      "changelog_to_inprogress[(changelog_to_inprogress['key'] == 'XD-3751')\n",
      "                            & (changelog_to_inprogress['project'] == 'xd')\n",
      "                            & (changelog_to_inprogress['created'] < '2016-03-03 18:40:53.171')].sort_values('created', ascending=False)\n",
      "43/417:\n",
      "changelog_to_inprogress[(changelog_to_inprogress['key'] == 'XD-3751')\n",
      "                            & (changelog_to_inprogress['project'] == 'xd')].sort_values('created', ascending=False)\n",
      "43/418: changelog_from_inprogress.head()\n",
      "43/419:\n",
      "changelog_from_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['fromString']=='In Progress')\n",
      "                                    & (changelog['toString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['toString']=='In Progress') \n",
      "                                    & (changelog['fromString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress.head()\n",
      "43/420:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project)\n",
      "                            & (changelog_to_inprogress['created'] < _created)].sort_values('created', ascending=False).head(1)\n",
      "    for st in _to_row['fromString']:\n",
      "        _prev_st = st \n",
      "    for cr in _to_row['created']:\n",
      "        _prev_st_created = cr \n",
      "    \n",
      "    changelog_from_inprogress.loc[index,'prev_status'] = _prev_st\n",
      "    changelog_from_inprogress.loc[index,'prev_status_created'] = _prev_st_created\n",
      "43/421: changelog_from_inprogress.head()\n",
      "43/422: changelog_from_inprogress.shape[0]\n",
      "43/423: changelog_from_inprogress.head()\n",
      "43/424: changelog_from_inprogress.groupby('toString').count()\n",
      "43/425: changelog_from_inprogress.groupby('toString')\n",
      "43/426: changelog_from_inprogress.head()\n",
      "43/427: changelog_from_inprogress.head(1)['created']-changelog_from_inprogress.head(1)['prev_status_created']\n",
      "43/428: changelog_from_inprogress.head(1)['created']\n",
      "43/429: pd.to_datetime(changelog_from_inprogress['created'])\n",
      "43/430: pd.to_datetime(changelog_from_inprogress['created']) -pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "43/431: pd.to_datetime(changelog_from_inprogress['created']) -pd.to_datetime(changelog_from_inprogress['prev_status_created']) + pd.to_datetime(changelog_from_inprogress['created']) -pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "43/432:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "changelog_from_inprogress['created'] = pd.to_datetime(changelog_from_inprogress['created'])\n",
      "changelog_to_inprogress['created'] = pd.to_datetime(changelog_to_inprogress['created'])\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project)\n",
      "                            & (changelog_to_inprogress['created'] < _created)].sort_values('created', ascending=False).head(1)\n",
      "    for st in _to_row['fromString']:\n",
      "        _prev_st = st \n",
      "    for cr in _to_row['created']:\n",
      "        _prev_st_created = cr \n",
      "    \n",
      "    changelog_from_inprogress.loc[index,'prev_status'] = _prev_st\n",
      "    changelog_from_inprogress.loc[index,'prev_status_created'] = _prev_st_created\n",
      "43/433: changelog_from_inprogress\n",
      "43/434: changelog_from_inprogress.head()\n",
      "43/435:\n",
      "changelog_from_inprogress['time_spent']=changelog_from_inprogress['status']-changelog_from_inprogress['prev_status_created'] \n",
      "changelog_from_inprogress.head()\n",
      "43/436:\n",
      "changelog_from_inprogress['time_spent']=changelog_from_inprogress['created']-changelog_from_inprogress['prev_status_created'] \n",
      "changelog_from_inprogress.head()\n",
      "43/437:\n",
      "#changelog_from_inprogress['time_spent']=\n",
      "changelog_from_inprogress['created']-changelog_from_inprogress['prev_status_created'] \n",
      "#changelog_from_inprogress.head()\n",
      "43/438:\n",
      "#df_test['Difference'] = df_test['First_Date'].sub(df_test['Second Date'], axis=0)\n",
      "changelog_from_inprogress['time_spent']=changelog_from_inprogress['status'].sub(\n",
      "    changelog_from_inprogress['prev_status_created'], axis = 0) \n",
      "changelog_from_inprogress.head()\n",
      "43/439:\n",
      "#df_test['Difference'] = df_test['First_Date'].sub(df_test['Second Date'], axis=0)\n",
      "changelog_from_inprogress['time_spent']=changelog_from_inprogress['created'].sub(\n",
      "    changelog_from_inprogress['prev_status_created'], axis = 0) \n",
      "changelog_from_inprogress.head()\n",
      "43/440:\n",
      "#df_test['Difference'] = df_test['First_Date'].sub(df_test['Second Date'], axis=0)\n",
      "changelog_from_inprogress['time_spent']=changelog_from_inprogress['created'].sub(changelog_from_inprogress['prev_status_created'], axis = 0) \n",
      "changelog_from_inprogress.head()\n",
      "43/441:\n",
      "\n",
      "changelog_from_inprogress['time_spent']=changelog_from_inprogress['created']-changelog_from_inprogress['prev_status_created'] \n",
      "changelog_from_inprogress.head()\n",
      "43/442:\n",
      "\n",
      "changelog_from_inprogress['time_spent']=pd.Timedelta(changelog_from_inprogress['created']-changelog_from_inprogress['prev_status_created'])\n",
      "changelog_from_inprogress.head()\n",
      "43/443:\n",
      "changelog_from_inprogress['prev_status_created'] = pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "\n",
      "changelog_from_inprogress['time_spent']=pd.Timedelta(changelog_from_inprogress['created']-changelog_from_inprogress['prev_status_created'])\n",
      "changelog_from_inprogress.head()\n",
      "43/444:\n",
      "changelog_from_inprogress['prev_status_created'] = pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "\n",
      "changelog_from_inprogress['time_spent']=pd.to_datetime(changelog_from_inprogress['created'])-pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "changelog_from_inprogress.head()\n",
      "43/445:\n",
      "changelog_from_inprogress['time_spent']=pd.to_datetime(changelog_from_inprogress['created'])-pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "changelog_from_inprogress.head()\n",
      "43/446:\n",
      "changelog_from_inprogress['time_spent']=pd.to_datetime(changelog_from_inprogress['created'])-pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "changelog_from_inprogress\n",
      "43/447:\n",
      "changelog_from_inprogress['time_spent']=pd.to_datetime(changelog_from_inprogress['created'])-pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "changelog_from_inprogress.head()\n",
      "43/448:\n",
      "changelog_from_inprogress['time_spent']=pd.to_datetime(changelog_from_inprogress['created'])-pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "changelog_from_inprogress['time_spent']\n",
      "43/449:\n",
      "changelog_from_inprogress['time_spent']=pd.to_datetime(changelog_from_inprogress['created'])-pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "changelog_from_inprogress['time_spent']/np.timedelta64(1, 's')\n",
      "43/450:\n",
      "changelog_from_inprogress['time_spent']=pd.to_datetime(changelog_from_inprogress['created'])-pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "changelog_from_inprogress['time_spent']/np.timedelta64(1, 'm')\n",
      "43/451:\n",
      "changelog_from_inprogress['time_spent']=pd.to_datetime(changelog_from_inprogress['created'])-pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "changelog_from_inprogress['time_spent']/np.timedelta64(1, 'h')\n",
      "43/452:\n",
      "changelog_from_inprogress['time_spent']=pd.to_datetime(changelog_from_inprogress['created'])-pd.to_datetime(changelog_from_inprogress['prev_status_created'])\n",
      "changelog_from_inprogress['time_spent']/np.timedelta64(1, 'm')\n",
      "43/453:\n",
      "changelog_from_inprogress['minutes_spent']=(pd.to_datetime(changelog_from_inprogress['created'])\n",
      "                            - pd.to_datetime(changelog_from_inprogress['prev_status_created'])) / np.timedelta64(1, 'm')\n",
      "changelog_from_inprogress.head(10)\n",
      "43/454:\n",
      "changelog_from_inprogress['minutes_spent']=(pd.to_datetime(changelog_from_inprogress['created'])\n",
      "                            - pd.to_datetime(changelog_from_inprogress['prev_status_created'])) / np.timedelta64(1, 'm')\n",
      "changelog_from_inprogress.head(3)\n",
      "43/455: changelog_from_inprogress.groupby(['key', 'project', 'author']).agg({'minutes_spent':'sum'})\n",
      "43/456:\n",
      "user_key_timespent=changelog_from_inprogress.groupby(['key', 'project', 'author']).agg({'minutes_spent':'sum'})\n",
      "user_key_timespent.reset_index(level= [0,1,2,3], inplace=True)\n",
      "43/457:\n",
      "user_key_timespent=changelog_from_inprogress.groupby(['key', 'project', 'author']).agg({'minutes_spent':'sum'})\n",
      "user_key_timespent.reset_index(level= [0,1,2], inplace=True)\n",
      "43/458: user_key_timespent\n",
      "43/459: user_key_timespent['minutes_spent'].hist()\n",
      "43/460: user_key_timespent['minutes_spent'].hist(bins=20)\n",
      "43/461: user_key_timespent['minutes_spent'].hist(bins=50)\n",
      "43/462: user_key_timespent['minutes_spent'].hist(bins=100)\n",
      "43/463: user_key_timespent['minutes_spent'].hist(bins=200)\n",
      "43/464: user_key_timespent[user_key_timespent['minutes_spent']<250000]['minutes_spent'].hist(bins=200)\n",
      "43/465: user_key_timespent[user_key_timespent['minutes_spent']<100000]['minutes_spent'].hist(bins=200)\n",
      "43/466: user_key_timespent[user_key_timespent['minutes_spent']<40000]['minutes_spent'].hist(bins=200)\n",
      "43/467: user_key_timespent[user_key_timespent['minutes_spent']<10000]['minutes_spent'].hist(bins=200)\n",
      "43/468: user_key_timespent[user_key_timespent['minutes_spent']<10000]['minutes_spent'].hist(bins=10)\n",
      "43/469: user_key_timespent[(user_key_timespent['minutes_spent']>0) & (user_key_timespent['minutes_spent']<10000)]['minutes_spent'].hist(bins=10)\n",
      "43/470: user_key_timespent[(user_key_timespent['minutes_spent']>0) & (user_key_timespent['minutes_spent']<10000)]['minutes_spent'].hist(bins=20)\n",
      "43/471: user_key_timespent[(user_key_timespent['minutes_spent']>0) & (user_key_timespent['minutes_spent']<1500)]['minutes_spent'].hist(bins=20)\n",
      "43/472: user_key_timespent[(user_key_timespent['minutes_spent']>0) & (user_key_timespent['minutes_spent']<150)]['minutes_spent'].hist(bins=20)\n",
      "43/473: user_key_timespent[(user_key_timespent['minutes_spent']>0) & (user_key_timespent['minutes_spent']<20)]['minutes_spent'].hist(bins=20)\n",
      "43/474: user_key_timespent[(user_key_timespent['minutes_spent']>0) & (user_key_timespent['minutes_spent']<3)]['minutes_spent'].hist(bins=20)\n",
      "43/475: user_key_timespent[(user_key_timespent['minutes_spent']>0) & (user_key_timespent['minutes_spent']<1)]['minutes_spent'].hist(bins=20)\n",
      "43/476: user_key_timespent[(user_key_timespent['minutes_spent']<0)]['minutes_spent'].hist(bins=20)\n",
      "43/477:\n",
      "changelog_from_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['fromString']=='In Progress')\n",
      "                                    & (changelog['toString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['toString']=='In Progress') \n",
      "                                    & (changelog['fromString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress.head()\n",
      "43/478:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "changelog_from_inprogress['created'] = pd.to_datetime(changelog_from_inprogress['created'])\n",
      "changelog_to_inprogress['created'] = pd.to_datetime(changelog_to_inprogress['created'])\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project)\n",
      "                            & (changelog_to_inprogress['created'] < _created)].sort_values('created', ascending=False).head(1)\n",
      "    for st in _to_row['fromString']:\n",
      "        _prev_st = st \n",
      "    for cr in _to_row['created']:\n",
      "        _prev_st_created = cr \n",
      "    \n",
      "    changelog_from_inprogress.loc[index,'prev_status'] = _prev_st\n",
      "    changelog_from_inprogress.loc[index,'prev_status_created'] = _prev_st_created\n",
      "43/479:\n",
      "changelog_from_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['fromString']=='In Progress')\n",
      "                                    & (changelog['toString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['toString']=='In Progress') \n",
      "                                    & (changelog['fromString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress.head()\n",
      "43/480:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "changelog_from_inprogress['created'] = pd.to_datetime(changelog_from_inprogress['created'])\n",
      "changelog_to_inprogress['created'] = pd.to_datetime(changelog_to_inprogress['created'])\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project)\n",
      "                            & (changelog_to_inprogress['created'] < _created)].sort_values('created', ascending=False).head(1)\n",
      "    for st in _to_row['fromString']:\n",
      "        _prev_st = st \n",
      "    for cr in _to_row['created']:\n",
      "        _prev_st_created = cr \n",
      "    \n",
      "    changelog_from_inprogress.loc[index,'prev_status'] = _prev_st\n",
      "    changelog_from_inprogress.loc[index,'prev_status_created'] = _prev_st_created\n",
      "43/481:\n",
      "changelog_from_inprogress['minutes_spent']=(pd.to_datetime(changelog_from_inprogress['created'])\n",
      "                            - pd.to_datetime(changelog_from_inprogress['prev_status_created'])) / np.timedelta64(1, 'm')\n",
      "changelog_from_inprogress.head(3)\n",
      "43/482: user_key_timespent[(user_key_timespent['minutes_spent']<0) & (user_key_timespent['minutes_spent']<1)]['minutes_spent'].hist(bins=20)\n",
      "43/483:\n",
      "user_key_timespent[(user_key_timespent['minutes_spent']>0) \n",
      "                   & (user_key_timespent['minutes_spent']<1)]['minutes_spent'].hist(bins=20)\n",
      "43/484:\n",
      "user_key_timespent[(user_key_timespent['minutes_spent']>0) \n",
      "                   & (user_key_timespent['minutes_spent']<100)]['minutes_spent'].hist(bins=20)\n",
      "43/485:\n",
      "user_key_timespent[(user_key_timespent['minutes_spent']>0) \n",
      "                   & (user_key_timespent['minutes_spent']<10)]['minutes_spent'].hist(bins=20)\n",
      "43/486: user_key_timespent[(user_key_timespent['minutes_spent']>0))]['minutes_spent'].hist(bins=20)\n",
      "43/487: user_key_timespent[(user_key_timespent['minutes_spent']>0)]['minutes_spent'].hist(bins=20)\n",
      "43/488:\n",
      "user_key_timespent=changelog_from_inprogress.groupby(['key', 'project', 'author']).agg({'minutes_spent':'sum'})\n",
      "user_key_timespent.reset_index(level= [0,1,2], inplace=True)\n",
      "43/489: user_key_timespent[(user_key_timespent['minutes_spent']<0)]['minutes_spent'].hist(bins=20)\n",
      "43/490: user_key_timespent[(user_key_timespent['minutes_spent']<0)]#['minutes_spent'].hist(bins=20)\n",
      "43/491: changelog_from_inprogress[(changelog_from_inprogress['minutes_spent']<0)]#['minutes_spent'].hist(bins=20)\n",
      "43/492:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "43/493:\n",
      "changelog_from_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['fromString']=='In Progress')\n",
      "                                    & (changelog['toString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['toString']=='In Progress') \n",
      "                                    & (changelog['fromString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress.head()\n",
      "43/494:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "changelog_from_inprogress['created'] = pd.to_datetime(changelog_from_inprogress['created'])\n",
      "changelog_to_inprogress['created'] = pd.to_datetime(changelog_to_inprogress['created'])\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(((changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project))\n",
      "                            & (changelog_to_inprogress['created'] < _created))].sort_values('created', ascending=False).head(1)\n",
      "    for st in _to_row['fromString']:\n",
      "        _prev_st = st \n",
      "    for cr in _to_row['created']:\n",
      "        _prev_st_created = cr \n",
      "    \n",
      "    changelog_from_inprogress.loc[index,'prev_status'] = _prev_st\n",
      "    changelog_from_inprogress.loc[index,'prev_status_created'] = _prev_st_created\n",
      "43/495:\n",
      "changelog_from_inprogress['minutes_spent']=(pd.to_datetime(changelog_from_inprogress['created'])\n",
      "                            - pd.to_datetime(changelog_from_inprogress['prev_status_created'])) / np.timedelta64(1, 'm')\n",
      "changelog_from_inprogress.head(3)\n",
      "43/496: changelog_from_inprogress[(changelog_from_inprogress['minutes_spent']<0)]#['minutes_spent'].hist(bins=20)\n",
      "43/497:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "43/498:\n",
      "changelog_from_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['fromString']=='In Progress')\n",
      "                                    & (changelog['toString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['toString']=='In Progress') \n",
      "                                    & (changelog['fromString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress.head()\n",
      "43/499:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "changelog_from_inprogress['created'] = pd.to_datetime(changelog_from_inprogress['created'])\n",
      "changelog_to_inprogress['created'] = pd.to_datetime(changelog_to_inprogress['created'])\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(((changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project))\n",
      "                            & (pd.to_datetime(changelog_to_inprogress['created']) < pd.to_datetime(_created)))].sort_values('created', ascending=False).head(1)\n",
      "    for st in _to_row['fromString']:\n",
      "        _prev_st = st \n",
      "    for cr in _to_row['created']:\n",
      "        _prev_st_created = cr \n",
      "    \n",
      "    changelog_from_inprogress.loc[index,'prev_status'] = _prev_st\n",
      "    changelog_from_inprogress.loc[index,'prev_status_created'] = _prev_st_created\n",
      "43/500:\n",
      "changelog_from_inprogress['minutes_spent']=(pd.to_datetime(changelog_from_inprogress['created'])\n",
      "                            - pd.to_datetime(changelog_from_inprogress['prev_status_created'])) / np.timedelta64(1, 'm')\n",
      "changelog_from_inprogress.head(3)\n",
      "43/501: changelog_from_inprogress[(changelog_from_inprogress['minutes_spent']<0)]#['minutes_spent'].hist(bins=20)\n",
      "43/502:\n",
      "user_key_timespent=changelog_from_inprogress.groupby(['key', 'project', 'author']).agg({'minutes_spent':'sum'})\n",
      "user_key_timespent.reset_index(level= [0,1,2], inplace=True)\n",
      "43/503: changelog_from_inprogress[(changelog_from_inprogress['minutes_spent']<0)]#['minutes_spent'].hist(bins=20)\n",
      "43/504: user_key_timespent[(user_key_timespent['minutes_spent']<0)]#['minutes_spent'].hist(bins=20)\n",
      "43/505: user_key_timespent[(user_key_timespent['minutes_spent']<0)]['minutes_spent'].hist(bins=20)\n",
      "43/506: user_key_timespent[(user_key_timespent['minutes_spent']>0)]['minutes_spent'].hist(bins=20)\n",
      "43/507: user_key_timespent['minutes_spent']\n",
      "43/508: user_key_timespent.sort_values('minutes_spent')\n",
      "43/509: user_key_timespent.sort_values('minutes_spent').shape[0]\n",
      "43/510: user_key_timespent[(user_key_timespent['minutes_spent']<1)].shape[0]\n",
      "43/511: user_key_timespent[(user_key_timespent['minutes_spent']<2)].shape[0]\n",
      "43/512: user_key_timespent[(user_key_timespent['minutes_spent']<5)].shape[0]\n",
      "43/513: user_key_timespent[(user_key_timespent['minutes_spent']<10)].shape[0]\n",
      "43/514: user_key_timespent[(user_key_timespent['minutes_spent']<60)].shape[0]\n",
      "43/515: user_key_timespent[(user_key_timespent['minutes_spent']>1000)].shape[0]\n",
      "43/516: user_key_timespent[(user_key_timespent['minutes_spent']>10000)].shape[0]\n",
      "43/517: user_key_timespent[(user_key_timespent['minutes_spent']>40000)].shape[0]\n",
      "43/518:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.05))\n",
      "cut_val_top_length = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "cut_val_bottom_length = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "\n",
      "print('nr of rows from 5% to 95%: ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top_length) & (user_key_timespent['minutes_spent']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 95% and minimum length of them: ', user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 5% and maximum length of them: ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', user_key_timespent.shape[0])\n",
      "43/519:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , user_key_timespent[user_key_timespent['textLength']>=cut_val_top_length]['textLength'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[user_key_timespent['textLength']<=cut_val_bottom_length]['textLength'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[(user_key_timespent['textLength']<cut_val_top_length)&(user_key_timespent['textLength']>cut_val_bottom_length)].textLength,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "43/520:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top_length]['minutes_spent'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom_length]['minutes_spent'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top_length)&(user_key_timespent['minutes_spent']>cut_val_bottom_length)].textLength,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "43/521:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top_length]['minutes_spent'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom_length]['minutes_spent'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top_length)&(user_key_timespent['minutes_spent']>cut_val_bottom_length)].minutes_spent,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "43/522:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.1))\n",
      "cut_val_top_length = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "cut_val_bottom_length = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "\n",
      "print('nr of rows from 5% to 95%: ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top_length) & (user_key_timespent['minutes_spent']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 95% and minimum length of them: ', user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 5% and maximum length of them: ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', user_key_timespent.shape[0])\n",
      "43/523:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top_length]['minutes_spent'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom_length]['minutes_spent'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top_length)&(user_key_timespent['minutes_spent']>cut_val_bottom_length)].minutes_spent,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "43/524:\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * 0.05))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * 0.1))\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', cut_val_b, ' to ', str(100-cut_val_t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top_length) & (user_key_timespent['minutes_spent']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above ', str(100-cut_val_t), user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top_length].shape[0])\n",
      "print('nr of rows below, ', cut_val_b, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom_length].shape[0])\n",
      "print('Total number of rows in dataset', user_key_timespent.shape[0])\n",
      "43/525:\n",
      "t=0.05\n",
      "b=0.1\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b, ' to ', str(100-t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top_length) & (user_key_timespent['minutes_spent']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above ', str(100-t), user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top_length].shape[0])\n",
      "print('nr of rows below, ', b, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom_length].shape[0])\n",
      "print('Total number of rows in dataset', user_key_timespent.shape[0])\n",
      "43/526:\n",
      "t=0.05\n",
      "b=0.1\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b, ' to ', str(100-t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top_length) & (user_key_timespent['minutes_spent']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above ', str(100-t), ': ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top_length].shape[0])\n",
      "print('nr of rows below, ', b, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom_length].shape[0])\n",
      "print('Total number of rows in dataset', user_key_timespent.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/527:\n",
      "t=0.05\n",
      "b=0.1\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b, ' to ', str(100-t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top_length) & (user_key_timespent['minutes_spent']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above ', str(100-t), ': ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top_length].shape[0])\n",
      "print('nr of rows below, ', b, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom_length].shape[0])\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/528:\n",
      "t=0.05\n",
      "b=0.1\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b, ' to ', str(100-t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top) & (user_key_timespent['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-t), ': ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/529:\n",
      "t=0.05\n",
      "b=0.1\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b, ' to ', str(100-100*t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top) & (user_key_timespent['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-t), ': ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/530:\n",
      "t=0.05\n",
      "b=0.1\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, ' to ', str(100-100*t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top) & (user_key_timespent['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-t), ': ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/531:\n",
      "t=0.05\n",
      "b=0.1\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val).iloc[cut_val-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, ' to ', str(100-100*t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top) & (user_key_timespent['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-100*t), ': ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b*100, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/532:\n",
      "t=0.05\n",
      "b=0.1\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val_t).iloc[cut_val-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val_b).iloc[cut_val-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, ' to ', str(100-100*t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top) & (user_key_timespent['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-100*t), ': ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b*100, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/533:\n",
      "t=0.05\n",
      "b=0.1\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "print(cut_val_t, cut_val_b)\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val_t).iloc[cut_val-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val_b).iloc[cut_val-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, ' to ', str(100-100*t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top) & (user_key_timespent['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-100*t), ': ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b*100, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/534:\n",
      "t=0.05\n",
      "b=0.1\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "print(cut_val_t, cut_val_b)\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val_t).iloc[cut_val_t-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val_b).iloc[cut_val_b-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, ' to ', str(100-100*t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top) & (user_key_timespent['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-100*t), ': ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b*100, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/535:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_t]['minutes_spent'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_b]['minutes_spent'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_t)&(user_key_timespent['minutes_spent']>cut_val_b)].minutes_spent,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "43/536:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_t]['minutes_spent'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_b]['minutes_spent'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_t)&(user_key_timespent['minutes_spent']>cut_val_b)].minutes_spent,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "43/537: user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_t)&(user_key_timespent['minutes_spent']>cut_val_b)].minutes_spent\n",
      "43/538: user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_t)&(user_key_timespent['minutes_spent']>cut_val_b)]\n",
      "43/539: user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_t)]\n",
      "43/540: user_key_timespent[(user_key_timespent['minutes_spent']>cut_val_b)]\n",
      "43/541:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top]['minutes_spent'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom]['minutes_spent'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top)&(user_key_timespent['minutes_spent']>cut_val_bottom)].minutes_spent,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "43/542: user_key_timespent.sort_values('minutes_spent', ascending=True)\n",
      "43/543: user_key_timespent.sort_values('minutes_spent', ascending=True).head()\n",
      "43/544: user_key_timespent[1957]\n",
      "43/545: user_key_timespent.loc[1957]\n",
      "43/546: print(user_key_timespent.sort_values('minutes_spent', ascending=True).head())\n",
      "43/547:\n",
      "print(user_key_timespent.sort_values('minutes_spent', ascending=True).head())\n",
      "print('------------')\n",
      "print(user_key_timespent.sort_values('minutes_spent', ascending=False).head())\n",
      "43/548: user_key_timespent = user_key_timespent.drop([1957])\n",
      "43/549:\n",
      "t=0.05\n",
      "b=0.1\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "print(cut_val_t, cut_val_b)\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val_t).iloc[cut_val_t-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val_b).iloc[cut_val_b-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, ' to ', str(100-100*t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top) & (user_key_timespent['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-100*t), ': ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b*100, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/550:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top]['minutes_spent'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom]['minutes_spent'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top)&(user_key_timespent['minutes_spent']>cut_val_bottom)].minutes_spent,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "43/551:\n",
      "t=0.05\n",
      "b=0.2\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "print(cut_val_t, cut_val_b)\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val_t).iloc[cut_val_t-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val_b).iloc[cut_val_b-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, ' to ', str(100-100*t), ' : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top) & (user_key_timespent['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-100*t), ': ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b*100, ': ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/552:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top]['minutes_spent'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom]['minutes_spent'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top)&(user_key_timespent['minutes_spent']>cut_val_bottom)].minutes_spent,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "43/553:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'spent minutes by developers \\n in the whole dataset','minute','occurences count','gold'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top]['minutes_spent'],15,'the longest ', str(t*100), '% spent minutes by developers'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom]['minutes_spent'],15,'the shortest ', str(b*100),'%spent minutes by developers'\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top)&(user_key_timespent['minutes_spent']>cut_val_bottom)].minutes_spent,15,'the minutes spent between ', str(t*100), '% - ', str(t*100), '% percentile')\n",
      "43/554:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'spent minutes by developers \\n in the whole dataset','minute','occurences count','gold'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top]['minutes_spent'],15,'the longest '+str(t*100)+'% spent minutes by developers'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom]['minutes_spent'],15,'the shortest '+str(b*100)+'%spent minutes by developers'\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top)&(user_key_timespent['minutes_spent']>cut_val_bottom)].minutes_spent,15,'the minutes spent between '+str(t*100)+'% - '+str(t*100)'% percentile')\n",
      "43/555:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'spent minutes by developers \\n in the whole dataset','minute','occurences count','gold'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top]['minutes_spent'],15,('the longest '+str(t*100)+'% spent minutes by developers')\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom]['minutes_spent'],15,('the shortest '+str(b*100)+'%spent minutes by developers')\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top)&(user_key_timespent['minutes_spent']>cut_val_bottom)].minutes_spent,15,('the minutes spent between '+str(t*100)+'% - '+str(t*100)'% percentile'))\n",
      "43/556:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'spent minutes by developers \\n in the whole dataset','minute','occurences count','gold'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top]['minutes_spent'],15,('the longest '+str(t*100)+'% spent minutes by developers')\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom]['minutes_spent'],15,('the shortest '+str(b*100)+'%spent minutes by developers')\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top)&(user_key_timespent['minutes_spent']>cut_val_bottom)].minutes_spent,15,('the minutes spent between '+str(t*100)+'% - '+str(t*100)+'% percentile'))\n",
      "43/557:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'spent minutes by developers \\n in the whole dataset','minute','occurences count','gold'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top]['minutes_spent'],15,('the longest '+str(t*100)+'% spent \\n minutes by developers')\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom]['minutes_spent'],15,('the shortest '+str(b*100)+'% \\n spent minutes by developers')\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top)&(user_key_timespent['minutes_spent']>cut_val_bottom)].minutes_spent,15,('the minutes spent between \\n '+str(t*100)+'% - '+str(t*100)+'% percentile'))\n",
      "43/558:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'spent minutes by developers \\n in the whole dataset','minute','occurences count','gold'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top]['minutes_spent'],15,('the longest '+str(t*100)+'% spent \\n minutes by developers')\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom]['minutes_spent'],15,('the shortest '+str(b*100)+'% \\n spent minutes by developers')\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top)&(user_key_timespent['minutes_spent']>cut_val_bottom)].minutes_spent,15,('the minutes spent between \\n '+str(b*100)+'% - '+str(t*100)+'% percentile'))\n",
      "43/559:\n",
      "hist_with_perc(user_key_timespent['minutes_spent'], 15,'spent minutes by developers \\n in the whole dataset','minute','occurences count','gold'\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top]['minutes_spent'],15,('the longest '+str(t*100)+'% spent \\n minutes by developers')\n",
      "            , user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom]['minutes_spent'],15,('the shortest '+str(b*100)+'% \\n spent minutes by developers')\n",
      "            , user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top)&(user_key_timespent['minutes_spent']>cut_val_bottom)].minutes_spent,15,('the minutes spent between \\n '+str(b*100)+'% - '+str(100-t*100)+'% percentile'))\n",
      "43/560: user_key_timespent['minutes_spent'].head()\n",
      "43/561: user_key_timespent['minutes_spent'].median()\n",
      "43/562:\n",
      "print(user_key_timespent['minutes_spent'].median())\n",
      "print(user_key_timespent['minutes_spent'].mean())\n",
      "43/563:\n",
      "print('median: ', user_key_timespent['minutes_spent'].median(),\n",
      "     '\\n nr of rows more than median', user_key_timespent[user_key_timespent['minutes_spent'] > user_key_timespent['minutes_spent'].median()].shape[0],\n",
      "      '\\n nr of rows less or equal than median', user_key_timespent[user_key_timespent['minutes_spent'] <= user_key_timespent['minutes_spent'].median()].shape[0]\n",
      "     )\n",
      "print(user_key_timespent['minutes_spent'].mean())\n",
      "43/564:\n",
      "print('median: ', user_key_timespent['minutes_spent'].median(),\n",
      "     '\\n total nr of rows: ', user_key_timespent['minutes_spent'].shape[0]'\n",
      "     '\\n nr of rows more than median', user_key_timespent[user_key_timespent['minutes_spent'] > user_key_timespent['minutes_spent'].median()].shape[0],\n",
      "      '\\n nr of rows less or equal than median', user_key_timespent[user_key_timespent['minutes_spent'] <= user_key_timespent['minutes_spent'].median()].shape[0]\n",
      "     )\n",
      "print(user_key_timespent['minutes_spent'].mean())\n",
      "43/565:\n",
      "print('median: ', user_key_timespent['minutes_spent'].median(),\n",
      "     '\\n total nr of rows: ', user_key_timespent['minutes_spent'].shape[0],\n",
      "     '\\n nr of rows more than median', user_key_timespent[user_key_timespent['minutes_spent'] > user_key_timespent['minutes_spent'].median()].shape[0],\n",
      "      '\\n nr of rows less or equal than median', user_key_timespent[user_key_timespent['minutes_spent'] <= user_key_timespent['minutes_spent'].median()].shape[0]\n",
      "     )\n",
      "print(user_key_timespent['minutes_spent'].mean())\n",
      "43/566:\n",
      "print('median: ', user_key_timespent['minutes_spent'].median(),\n",
      "     '\\n total nr of rows: ', user_key_timespent['minutes_spent'].shape[0],\n",
      "     '\\n nr of rows more than median', user_key_timespent[user_key_timespent['minutes_spent'] > user_key_timespent['minutes_spent'].median()].shape[0],\n",
      "      '\\n nr of rows less or equal than median', user_key_timespent[user_key_timespent['minutes_spent'] <= user_key_timespent['minutes_spent'].median()].shape[0]\n",
      "     )\n",
      "print('median: ', user_key_timespent['minutes_spent'].mean(),\n",
      "     '\\n total nr of rows: ', user_key_timespent['minutes_spent'].shape[0],\n",
      "     '\\n nr of rows more than mean', user_key_timespent[user_key_timespent['minutes_spent'] > user_key_timespent['minutes_spent'].mean()].shape[0],\n",
      "      '\\n nr of rows less or equal than mean', user_key_timespent[user_key_timespent['minutes_spent'] <= user_key_timespent['minutes_spent'].mean()].shape[0]\n",
      "     )\n",
      "43/567:\n",
      "print('median: ', user_key_timespent['minutes_spent'].median(),\n",
      "     '\\n total nr of rows: ', user_key_timespent['minutes_spent'].shape[0],\n",
      "     '\\n nr of rows more than median', user_key_timespent[user_key_timespent['minutes_spent'] > user_key_timespent['minutes_spent'].median()].shape[0],\n",
      "      '\\n nr of rows less or equal than median', user_key_timespent[user_key_timespent['minutes_spent'] <= user_key_timespent['minutes_spent'].median()].shape[0]\n",
      "     )\n",
      "print('mean: ', user_key_timespent['minutes_spent'].mean(),\n",
      "     '\\n total nr of rows: ', user_key_timespent['minutes_spent'].shape[0],\n",
      "     '\\n nr of rows more than mean', user_key_timespent[user_key_timespent['minutes_spent'] > user_key_timespent['minutes_spent'].mean()].shape[0],\n",
      "      '\\n nr of rows less or equal than mean', user_key_timespent[user_key_timespent['minutes_spent'] <= user_key_timespent['minutes_spent'].mean()].shape[0]\n",
      "     )\n",
      "43/568: user_key_timespent.head(3)\n",
      "43/569: issues[['fields.status.statusCategory.name', 'fields.status.name', 'project']].groupby(['fields.status.statusCategory.name', 'fields.status.name']).count()\n",
      "43/570: changelog[changelog['field']=='status'].fromString.unique()\n",
      "43/571: changelog[changelog['field']=='status'].toString.unique()\n",
      "43/572: changelog[changelog['field']=='status'].groupby('toString').count()\n",
      "43/573: changelog[changelog['field']=='status'].groupby('toString').count().sort_values('author')\n",
      "43/574: changelog[changelog['field']=='status'].groupby('toString').count().sort_values('author', ascending=False)\n",
      "43/575:\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "43/576: changelog[changelog['field']=='status'].groupby('toString').count().sort_values('author', ascending=False)[['toString', 'author']]\n",
      "43/577: changelog[changelog['field']=='status'].groupby('toString').count().sort_values('author', ascending=False)[['toString']]\n",
      "43/578: changelog[changelog['field']=='status'][['toString', 'author']].groupby('toString').count().sort_values('author', ascending=False)\n",
      "43/579: changelog[changelog['field']=='status'][['toString', 'author']].groupby('toString').count().sort_values('author', ascending=False)\n",
      "43/580: user_key_timespent\n",
      "43/581: user_key_timespent.head()\n",
      "43/582: user_personalities.head()\n",
      "43/583: user_personalities.shape[0]\n",
      "43/584: user_personalities.head()\n",
      "43/585: user_personalities.user.unique().shape[0]\n",
      "43/586:\n",
      "pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')\n",
      "43/587:\n",
      "pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author').user.unique()\n",
      "43/588:\n",
      "pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author').user.unique().shape[0]\n",
      "43/589:\n",
      "pd.merge(user_personalities, user_key_timespent, how = 'left',\n",
      "        left_on = 'user', right_on = 'author').user.unique().shape[0]\n",
      "43/590:\n",
      "pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author').user.unique().shape[0]\n",
      "43/591:\n",
      "q1 = \"\"\"\n",
      "Select * \n",
      "From user_personalities AS A\n",
      "Left Join user_key_timespent AS B on A.user_id = B.author\n",
      "Where B.author is null\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/592:\n",
      "q1 = \"\"\"\n",
      "Select * \n",
      "From user_personalities AS A\n",
      "Left Join user_key_timespent AS B on A.user = B.author\n",
      "Where B.author is null\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/593:\n",
      "q1 = \"\"\"\n",
      "Select user \n",
      "From user_personalities AS A\n",
      "Left Join user_key_timespent AS B on A.user = B.author\n",
      "Where B.author is null\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/594:\n",
      "q1=\"\"\"\n",
      "Select *\n",
      "From changelog\n",
      "where author = 'dpandey'\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/595:\n",
      "q1=\"\"\"\n",
      "Select *\n",
      "From changelog\n",
      "where author = 'dpandey'\n",
      "and field = 'status'\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/596:\n",
      "q1=\"\"\"\n",
      "Select *\n",
      "From changelog\n",
      "where author = 'dpandey'\n",
      "and field = 'status'\n",
      "\"\"\"\n",
      "\n",
      "ps.sqldf(q1, locals())\n",
      "43/597:\n",
      "q1=\"\"\"\n",
      "Select *\n",
      "From changelog\n",
      "where author = 'dpandey'\n",
      "and field = 'status'\n",
      "and fromString='In Progress'\n",
      "\"\"\"\n",
      "\n",
      "ps.sqldf(q1, locals())\n",
      "43/598:\n",
      "q1=\"\"\"\n",
      "Select *\n",
      "From changelog\n",
      "where author = 'dpandey'\n",
      "and field = 'status'\n",
      "\"\"\"\n",
      "\n",
      "ps.sqldf(q1, locals())\n",
      "43/599:\n",
      "print(pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author').user.unique().shape[0])\n",
      "\n",
      "pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')\n",
      "43/600: changelog[changelog['field']=='status'][['toString', 'author']].groupby('toString').count().sort_values('author', ascending=False)\n",
      "43/601: issues[['fields.status.statusCategory.name', 'fields.status.name', 'project']].groupby(['fields.status.statusCategory.name', 'fields.status.name']).count()\n",
      "43/602: issues[['fields.status.statusCategory.name', 'fields.status.name', 'project']].groupby(['fields.status.statusCategory.name', 'fields.status.name']).count()\n",
      "43/603: changelog[changelog['field']=='status'][['toString', 'author']].groupby('toString').count().sort_values('author', ascending=False)\n",
      "43/604: changelog[changelog['field']=='status'][['toString', 'author']].groupby('toString').count().sort_values('author', ascending=False)\n",
      "43/605:\n",
      "\n",
      "log_inprogress = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_inprogress))]\n",
      "43/606: log_inprogress.head()\n",
      "43/607:\n",
      "log_todo = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_todo))]\n",
      "log_inprogress = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_inprogress))]\n",
      "log_done = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_done))]\n",
      "43/608: log_todo.groupby('author').agg({'created':'count'})\n",
      "43/609: log_todo.groupby('author').agg({'created':'count'}).sort_values('created')\n",
      "43/610: log_todo.groupby('author').agg({'created':'count'})\n",
      "43/611:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'}).reset_index(level= [0,1], inplace=True)\n",
      "inprogress_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "done_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "43/612:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'}).reset_index(level= [0], inplace=True)\n",
      "inprogress_actions = log_todo.groupby('author').agg({'created':'count'}).reset_index(level= [0], inplace=True)\n",
      "done_actions = log_todo.groupby('author').agg({'created':'count'}).reset_index(level= [0], inplace=True)\n",
      "43/613: print(todo_actions.shape[0], inprogress_actions.shape[0], done_actions.shape[0])\n",
      "43/614:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'}).reset_index(level= [0], inplace=True)\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'}).reset_index(level= [0], inplace=True)\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'}).reset_index(level= [0], inplace=True)\n",
      "43/615: print(todo_actions.shape[0], inprogress_actions.shape[0], done_actions.shape[0])\n",
      "43/616:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "43/617: print(todo_actions.shape[0], inprogress_actions.shape[0], done_actions.shape[0])\n",
      "43/618:\n",
      "print(todo_actions.head(5))\n",
      "print(done_actions.head(5))\n",
      "43/619: pd.merge(user_personalities, todo_actions, how = 'left', left_on = 'user', right_on = 'author')\n",
      "43/620:\n",
      "pd.merge(\n",
      "    pd.merge(user_personalities, todo_actions, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "    inprogress_actions, how = 'left', left_on = 'user', right_on = 'author')\n",
      "43/621:\n",
      "pd.merge(\n",
      "    pd.merge(\n",
      "        pd.merge(user_personalities, todo_actions, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "            inprogress_actions, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "                done_actions, how = 'left', left_on = 'user', right_on = 'author')\n",
      "43/622:\n",
      "pd.merge(\n",
      "    pd.merge(\n",
      "        pd.merge(user_personalities, todo_actions, how = 'inner', left_on = 'user', right_on = 'author'),\n",
      "            inprogress_actions, how = 'inner', left_on = 'user', right_on = 'author'),\n",
      "                done_actions, how = 'iiner', left_on = 'user', right_on = 'author')\n",
      "43/623:\n",
      "pd.merge(\n",
      "    pd.merge(\n",
      "        pd.merge(user_personalities, todo_actions, how = 'inner', left_on = 'user', right_on = 'author'),\n",
      "            inprogress_actions, how = 'inner', left_on = 'user', right_on = 'author'),\n",
      "                done_actions, how = 'inner', left_on = 'user', right_on = 'author')\n",
      "43/624:\n",
      "users_personalities_tasks = pd.merge(\n",
      "    pd.merge(\n",
      "        pd.merge(user_personalities, todo_actions, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "            inprogress_actions, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "                done_actions, how = 'left', left_on = 'user', right_on = 'author')\n",
      "43/625: users_personalities_tasks.head()\n",
      "43/626: todo_actions\n",
      "43/627: todo_actions.head()\n",
      "43/628: users_personalities_tasks.shape[0]\n",
      "43/629: users_personalities_tasks.head()\n",
      "43/630: user_personalities.columns\n",
      "43/631: users_personalities_tasks.columns\n",
      "43/632:\n",
      "users_personalities_tasks = pd.merge(\n",
      "    pd.merge(\n",
      "        pd.merge(user_personalities, todo_actions, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "            inprogress_actions, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "                done_actions, how = 'left', left_on = 'user', right_on = 'author')['user', 'emailAddress', 'openness', 'o_raw',\n",
      "        'o_sign','conscientiousness', 'c_raw', 'c_sign', 'extraversion', 'e_raw', 'e_sign', 'agreeableness', 'a_raw', 'a_sign',\n",
      "        'neuroticism', 'n_raw','n_sign', 'created', 'created_x', 'created_y']\n",
      "43/633:\n",
      "users_personalities_tasks = pd.merge(\n",
      "    pd.merge(\n",
      "        pd.merge(user_personalities, todo_actions, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "            inprogress_actions, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "                done_actions, how = 'left', left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'openness', 'o_raw',\n",
      "        'o_sign','conscientiousness', 'c_raw', 'c_sign', 'extraversion', 'e_raw', 'e_sign', 'agreeableness', 'a_raw', 'a_sign',\n",
      "        'neuroticism', 'n_raw','n_sign', 'created', 'created_x', 'created_y']]\n",
      "43/634: users_personalities_tasks.head()\n",
      "43/635:\n",
      "users_personalities_tasks = pd.merge(\n",
      "    pd.merge(\n",
      "        pd.merge(user_personalities, todo_actions, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "            inprogress_actions, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "                done_actions, how = 'left', left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'created', 'created_x', 'created_y']]\n",
      "43/636: users_personalities_tasks.head()\n",
      "43/637:\n",
      "print(users_personalities_tasks.head(3),\n",
      "      todo_actions.head(3))\n",
      "43/638:\n",
      "print(users_personalities_tasks.sort_values('user').head(3),\n",
      "      todo_actions.sort_values('author').head(3))\n",
      "43/639:\n",
      " pd.merge(user_personalities, todo_actions, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "            inprogress_actions, how = 'left', left_on = 'user', right_on = 'author')\n",
      "43/640:  pd.merge(user_personalities, todo_actions, how = 'left', left_on = 'user', right_on = 'author')\n",
      "43/641:  pd.merge(user_personalities, todo_actions, how = 'left', left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'created']]\n",
      "43/642:\n",
      " pd.merge(user_personalities, todo_actions, \n",
      "          how = 'left', left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'created']]\n",
      "43/643:\n",
      "pd.merge(user_personalities, todo_actions, \n",
      "          how = 'left', left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'created']]\n",
      "43/644:\n",
      "print(users_personalities_tasks.sort_values('user').head(3))\n",
      "print(todo_actions.sort_values('author').head(3))\n",
      "print(done_actions.sort_values('author').head(3))\n",
      "print(inprogress_actions.sort_values('author').head(3))\n",
      "43/645:\n",
      "print(users_personalities_tasks.sort_values('user').head(3), '\\n')\n",
      "print(todo_actions.sort_values('author').head(3), '\\n')\n",
      "print(done_actions.sort_values('author').head(3), '\\n')\n",
      "print(inprogress_actions.sort_values('author').head(3))\n",
      "43/646: users_personalities_tasks.rename(columns={'created':'done', 'created_x':'todo', 'created_y':'inprogress'})\n",
      "43/647: users_personalities_tasks.head()\n",
      "43/648: users_personalities_tasks = users_personalities_tasks = users_personalities_tasks.rename(columns={'created':'done', 'created_x':'todo', 'created_y':'inprogress'})\n",
      "43/649: users_personalities_tasks.head()\n",
      "43/650:\n",
      "print(users_personalities_tasks.sort_values('user').head(3), '\\n---todo')\n",
      "print(todo_actions.sort_values('author').head(3), '\\n--done')\n",
      "print(done_actions.sort_values('author').head(3), '\\n--inprogress')\n",
      "print(inprogress_actions.sort_values('author').head(3))\n",
      "43/651:\n",
      "print(users_personalities_tasks.sort_values('user').head(3), '\\n---todo below')\n",
      "print(todo_actions.sort_values('author').head(3), '\\n--done below')\n",
      "print(done_actions.sort_values('author').head(3), '\\n--inprogress below')\n",
      "print(inprogress_actions.sort_values('author').head(3))\n",
      "43/652:\n",
      "users_personalities_tasks = users_personalities_tasks.rename(\n",
      "    columns={'created':'done', 'created_x':'todo', 'created_y':'inprogress'})\n",
      "43/653:\n",
      "users_personalities_tasks = users_personalities_tasks.rename(\n",
      "    columns={'created':'done', 'created_x':'todo', 'created_y':'inprogress'})\n",
      "43/654:\n",
      "users_personalities_tasks = users_personalities_tasks.rename(\n",
      "    columns={'created':'done', 'created_x':'todo', 'created_y':'inprogress'})\n",
      "43/655: users_personalities_tasks.head()\n",
      "43/656: users_personalities_tasks[pd.isnull(users_personalities_tasks['done'])==False].shape[0]\n",
      "43/657:\n",
      "users_personalities_tasks[(pd.isnull(users_personalities_tasks['done'])==False)\n",
      "                         & (pd.isnull(users_personalities_tasks['todo'])==False)].shape[0]\n",
      "43/658:\n",
      "users_personalities_tasks[(pd.isnull(users_personalities_tasks['done'])==False)\n",
      "                         & (pd.isnull(users_personalities_tasks['todo'])==False)\n",
      "                         & (pd.isnull(users_personalities_tasks['inprogress'])==False)].shape[0]\n",
      "43/659:\n",
      "users_personalities_tasks[(pd.isnull(users_personalities_tasks['done'])>0)\n",
      "                         & (pd.isnull(users_personalities_tasks['todo'])==False)\n",
      "                         & (pd.isnull(users_personalities_tasks['inprogress'])==False)].shape[0]\n",
      "43/660:\n",
      "t=0.05\n",
      "b=0.2\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "print(cut_val_t, cut_val_b)\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val_t).iloc[cut_val_t-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val_b).iloc[cut_val_b-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, '% to ', str(100-100*t), '% : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top) & (user_key_timespent['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-100*t), '%: ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b*100, '%: ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/661:\n",
      "t=0.05\n",
      "b=0.2\n",
      "cut_val_t = round((int(user_key_timespent.shape[0]) * t))\n",
      "cut_val_b = round((int(user_key_timespent.shape[0]) * b))\n",
      "cut_val_top = user_key_timespent.sort_values('minutes_spent', ascending=False).head(cut_val_t).iloc[cut_val_t-1].minutes_spent\n",
      "cut_val_bottom = user_key_timespent.sort_values('minutes_spent', ascending=True).head(cut_val_b).iloc[cut_val_b-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, '% to ', str(100-100*t), '% : ', user_key_timespent[(user_key_timespent['minutes_spent']<cut_val_top) & (user_key_timespent['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-100*t), '%: ',user_key_timespent[user_key_timespent['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b*100, '%: ',user_key_timespent[user_key_timespent['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/662: print(user_personalities.shape[0], user_key_timespent.shape[0])\n",
      "43/663:\n",
      "print(pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author').user.unique().shape[0])\n",
      "\n",
      "pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')\n",
      "43/664:\n",
      "print(pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author').user.unique().shape[0])\n",
      "\n",
      "pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', 'minutes_spent']]\n",
      "43/665:\n",
      "user_task_time = pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', 'minutes_spent']]\n",
      "43/666: user_task_time.head()\n",
      "43/667: user_task_time.shape[0]\n",
      "43/668:\n",
      "print(user_task_time.sort_values('minutes_spent', ascending=True).head())\n",
      "print('------------')\n",
      "print(user_task_time.sort_values('minutes_spent', ascending=False).head())\n",
      "43/669:\n",
      "t=0.05\n",
      "b=0.2\n",
      "cut_val_t = round((int(user_task_time.shape[0]) * t))\n",
      "cut_val_b = round((int(user_task_time.shape[0]) * b))\n",
      "cut_val_top = user_task_time.sort_values('minutes_spent', ascending=False).head(cut_val_t).iloc[cut_val_t-1].minutes_spent\n",
      "cut_val_bottom = user_task_time.sort_values('minutes_spent', ascending=True).head(cut_val_b).iloc[cut_val_b-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, '% to ', str(100-100*t), '% : ', user_task_time[(user_key_timespent['minutes_spent']<cut_val_top) & (user_task_time['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-100*t), '%: ',user_task_time[user_task_time['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b*100, '%: ',user_task_time[user_task_time['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_task_time.shape[0])\n",
      "43/670:\n",
      "t=0.05\n",
      "b=0.2\n",
      "cut_val_t = round((int(user_task_time.shape[0]) * t))\n",
      "cut_val_b = round((int(user_task_time.shape[0]) * b))\n",
      "cut_val_top = user_task_time.sort_values('minutes_spent', ascending=False).head(cut_val_t).iloc[cut_val_t-1].minutes_spent\n",
      "cut_val_bottom = user_task_time.sort_values('minutes_spent', ascending=True).head(cut_val_b).iloc[cut_val_b-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, '% to ', str(100-100*t), '% : ', user_task_time[(user_task_time['minutes_spent']<cut_val_top) & (user_task_time['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-100*t), '%: ',user_task_time[user_task_time['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b*100, '%: ',user_task_time[user_task_time['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_task_time.shape[0])\n",
      "43/671:\n",
      "t=0.05\n",
      "b=0.05\n",
      "cut_val_t = round((int(user_task_time.shape[0]) * t))\n",
      "cut_val_b = round((int(user_task_time.shape[0]) * b))\n",
      "cut_val_top = user_task_time.sort_values('minutes_spent', ascending=False).head(cut_val_t).iloc[cut_val_t-1].minutes_spent\n",
      "cut_val_bottom = user_task_time.sort_values('minutes_spent', ascending=True).head(cut_val_b).iloc[cut_val_b-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, '% to ', str(100-100*t), '% : ', user_task_time[(user_task_time['minutes_spent']<cut_val_top) & (user_task_time['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-100*t), '%: ',user_task_time[user_task_time['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b*100, '%: ',user_task_time[user_task_time['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_task_time.shape[0])\n",
      "43/672:\n",
      "t=0.05\n",
      "b=0.20\n",
      "cut_val_t = round((int(user_task_time.shape[0]) * t))\n",
      "cut_val_b = round((int(user_task_time.shape[0]) * b))\n",
      "cut_val_top = user_task_time.sort_values('minutes_spent', ascending=False).head(cut_val_t).iloc[cut_val_t-1].minutes_spent\n",
      "cut_val_bottom = user_task_time.sort_values('minutes_spent', ascending=True).head(cut_val_b).iloc[cut_val_b-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', b*100, '% to ', str(100-100*t), '% : ', user_task_time[(user_task_time['minutes_spent']<cut_val_top) & (user_task_time['minutes_spent']>cut_val_bottom)].shape[0])\n",
      "print('nr of rows above ', str(100-100*t), '%: ',user_task_time[user_task_time['minutes_spent']>=cut_val_top].shape[0])\n",
      "print('nr of rows below, ', b*100, '%: ',user_task_time[user_task_time['minutes_spent']<=cut_val_bottom].shape[0])\n",
      "print('Total number of rows in dataset: ', user_task_time.shape[0])\n",
      "43/673:\n",
      "hist_with_perc(user_task_time['minutes_spent'], 15,'spent minutes by developers \\n in the whole dataset','minute','occurences count','gold'\n",
      "            , user_task_time[user_task_time['minutes_spent']>=cut_val_top]['minutes_spent'],15,('the longest '+str(t*100)+'% spent \\n minutes by developers')\n",
      "            , user_task_time[user_task_time['minutes_spent']<=cut_val_bottom]['minutes_spent'],15,('the shortest '+str(b*100)+'% \\n spent minutes by developers')\n",
      "            , user_task_time[(user_task_time['minutes_spent']<cut_val_top)&(user_task_time['minutes_spent']>cut_val_bottom)].minutes_spent,15,('the minutes spent between \\n '+str(b*100)+'% - '+str(100-t*100)+'% percentile'))\n",
      "43/674: print(user_personalities.shape[0], user_key_timespent.shape[0])\n",
      "43/675:\n",
      "issues[['fields.status.statusCategory.name', 'fields.status.name', 'project']].groupby(\n",
      "    ['fields.status.statusCategory.name', 'fields.status.name']).count()\n",
      "43/676:\n",
      "print(issues[['fields.status.statusCategory.name', 'fields.status.name', 'project']].groupby(\n",
      "    ['fields.status.statusCategory.name', 'fields.status.name']).count())\n",
      "43/677:\n",
      "issues[['fields.status.statusCategory.name', 'fields.status.name', 'project']].groupby(\n",
      "    ['fields.status.statusCategory.name', 'fields.status.name']).count()\n",
      "43/678:\n",
      "changelog[changelog['field']=='status'][['toString', 'author']].groupby(\n",
      "    'toString').count().sort_values('author', ascending=False)\n",
      "43/679: properly assign the status names to columns not to mix them\n",
      "43/680: users_personalities_tasks.head()\n",
      "43/681:\n",
      "users_personalities_tasks[(pd.isnull(users_personalities_tasks['done'])==False)\n",
      "                         & (pd.isnull(users_personalities_tasks['todo'])==False)\n",
      "                         & (pd.isnull(users_personalities_tasks['inprogress'])==False)].shape[0]\n",
      "43/682: changelog.head()\n",
      "43/683: <h2> Task Prioritization <h/2>\n",
      "43/684: changelog['field'].unique()\n",
      "43/685: changelog['field'].unique()..sort_values('field', ascending=False)\n",
      "43/686: changelog['field'].unique().sort_values('field', ascending=False)\n",
      "43/687: changelog['field'].sort_values('field', ascending=False).unique()\n",
      "43/688: changelog['field'].drop_duplicates().sort_values('field', ascending=False).unique()\n",
      "43/689: changelog['field'].drop_duplicates()\n",
      "43/690: changelog['field'].drop_duplicates().sort_values('field', ascending=False)\n",
      "43/691: changelog['field'].drop_duplicates().sort_values('field')\n",
      "43/692: changelog['field'].drop_duplicates()\n",
      "43/693: changelog['field'].drop_duplicates()\n",
      "43/694: changelog['field'].drop_duplicates().sort_values()\n",
      "43/695: changelog['field'].drop_duplicates().sort_values(1)\n",
      "43/696: changelog['field'].drop_duplicates().sort_values()\n",
      "43/697: changelog['field'].drop_duplicates()['field']\n",
      "43/698: changelog['field'].drop_duplicates()\n",
      "43/699: changelog.unique('field')\n",
      "43/700: changelog['field'].unique()\n",
      "43/701: changelog['field'].unique().sort\n",
      "43/702: changelog['field'].unique().sort()\n",
      "43/703: changelog['field'].unique().sort()\n",
      "43/704: changelog['field'].unique().sort_values()\n",
      "43/705: changelog['field'].unique().sort_values()\n",
      "43/706: changelog['field'].unique()\n",
      "43/707:\n",
      "changelog[changelog['field']=='priority'][['toString', 'author']].groupby(\n",
      "    'toString').count().sort_values('author', ascending=False)\n",
      "43/708:\n",
      "_high = ['High', 'Critical', 'Blocker']\n",
      "_medium = ['Medium', 'Major']\n",
      "_low = ['Low', 'Minor', 'None', 'Trivial', 'To be reviewed']\n",
      "43/709:\n",
      "log_high = changelog[(changelog['field']=='priority') & (changelog['toString'].isin(_high))]\n",
      "log_medium = changelog[(changelog['field']=='priority') & (changelog['toString'].isin(_medium))]\n",
      "log_low = changelog[(changelog['field']=='priority') & (changelog['toString'].isin(_low))]\n",
      "43/710: log_high.shape[0]\n",
      "43/711:\n",
      "log_high.shape[0]\n",
      "log_medium.shape[0]\n",
      "43/712:\n",
      "log_high.shape[0]\n",
      "log_low.shape[0]\n",
      "43/713:\n",
      "high_prioritization = log_high.groupby('author').agg({'created':'count'})\n",
      "high_prioritization.reset_index(level= [0], inplace=True)\n",
      "\n",
      "medium_prioritization = log_medium.groupby('author').agg({'created':'count'})\n",
      "medium_prioritization.reset_index(level= [0], inplace=True)\n",
      "\n",
      "low_prioritization = log_low.groupby('author').agg({'created':'count'})\n",
      "low_prioritization.reset_index(level= [0], inplace=True)\n",
      "43/714: print(high_prioritization.shape[0], medium_prioritization.shape[0], low_prioritization.shape[0])\n",
      "43/715:\n",
      "users_personalities_priorities = pd.merge(\n",
      "    pd.merge(\n",
      "        pd.merge(user_personalities, high_prioritization, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "            medium_prioritization, how = 'left', left_on = 'user', right_on = 'author'),\n",
      "                low_prioritization, how = 'left', left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'created', 'created_x', 'created_y']]\n",
      "43/716: users_personalities_priorities\n",
      "43/717: users_personalities_priorities.head()\n",
      "43/718:\n",
      "print(users_personalities_tasks.sort_values('user').head(3), '\\n---todo below')\n",
      "print(todo_actions.sort_values('author').head(3), '\\n--done below')\n",
      "print(done_actions.sort_values('author').head(3), '\\n--inprogress below')\n",
      "print(inprogress_actions.sort_values('author').head(3))\n",
      "43/719:\n",
      "print(users_personalities_priorities.sort_values('user').head(3), '\\n---high below')\n",
      "print(high_prioritization.sort_values('author').head(3), '\\n--medium below')\n",
      "print(medium_prioritization.sort_values('author').head(3), '\\n--low below')\n",
      "print(low_prioritization.sort_values('author').head(3))\n",
      "43/720:\n",
      "users_personalities_tasks = users_personalities_tasks.rename(\n",
      "    columns={'created':'low', 'created_x':'high', 'created_y':'medium'})\n",
      "43/721: users_personalities_tasks.shape[0]\n",
      "43/722: users_personalities_tasks.head()\n",
      "43/723:\n",
      "users_personalities_tasks = users_personalities_tasks.rename(\n",
      "    columns={'created':'done', 'created_x':'todo', 'created_y':'inprogress'})\n",
      "43/724: users_personalities_tasks.head()\n",
      "43/725:\n",
      "users_personalities_priorities = users_personalities_priorities.rename(\n",
      "    columns={'created':'low', 'created_x':'high', 'created_y':'medium'})\n",
      "43/726: users_personalities_priorities.head()\n",
      "43/727:\n",
      "\n",
      "users_personalities_priorities[(pd.isnull(users_personalities_priorities['low'])==False)\n",
      "                         & (pd.isnull(users_personalities_priorities['medium'])==False)\n",
      "                         & (pd.isnull(users_personalities_priorities['high'])==False)].shape[0]\n",
      "users_personalities_priorities[(pd.isnull(users_personalities_priorities['low'])==False)\n",
      "                         & (pd.isnull(users_personalities_priorities['medium'])==False)\n",
      "                         & (pd.isnull(users_personalities_priorities['high'])==False)].shape[0]\n",
      "43/728:\n",
      "print(users_personalities_priorities[(pd.isnull(users_personalities_priorities['low'])==False)\n",
      "                         & (pd.isnull(users_personalities_priorities['medium'])==False)\n",
      "                         & (pd.isnull(users_personalities_priorities['high'])==False)].shape[0])\n",
      "print(users_personalities_priorities[(pd.isnull(users_personalities_priorities['low'])==False)\n",
      "                         | (pd.isnull(users_personalities_priorities['medium'])==False)\n",
      "                         | (pd.isnull(users_personalities_priorities['high'])==False)].shape[0])\n",
      "43/729:\n",
      "print(users_personalities_tasks[(pd.isnull(users_personalities_tasks['done'])==False)\n",
      "                         & (pd.isnull(users_personalities_tasks['todo'])==False)\n",
      "                         & (pd.isnull(users_personalities_tasks['inprogress'])==False)].shape[0])\n",
      "\n",
      "print(users_personalities_tasks[(pd.isnull(users_personalities_tasks['done'])==False)\n",
      "                         | (pd.isnull(users_personalities_tasks['todo'])==False)\n",
      "                         | (pd.isnull(users_personalities_tasks['inprogress'])==False)].shape[0])\n",
      "43/730: user_key_timespent.head*\n",
      "43/731: user_key_timespent.head*\n",
      "43/732: user_key_timespent.head()\n",
      "43/733:\n",
      "_percentile=0.33\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * t))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', _top_percentile_rows_filter_value_minutes*100, '% to ',\n",
      "      str(100-100*_top_percentile_rows_filter_value_minutes), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/734:\n",
      "_percentile=0.33\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * t))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/735:\n",
      "_percentile=0.33\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * t))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "print(_top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/736:\n",
      "_percentile=0.33\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * t))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/737:\n",
      "_percentile=0.33\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/738:\n",
      "_percentile=0.33\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/739:\n",
      "_percentile=0.33333\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/740:\n",
      "_percentile=0.333333\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/741:\n",
      "_percentile=0.33333\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/742:\n",
      "_percentile=0.3333\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/743:\n",
      "_percentile=0.333\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/744:\n",
      "_percentile=0.3333\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/745: user_key_timespent.head()\n",
      "43/746:\n",
      "user_key_timespent['time_spending_category'] = np.nan\n",
      "\n",
      "user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)\n",
      "                  ].time_spending_category = 'medium'\n",
      "\n",
      "user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].time_spending_category='high'\n",
      "\n",
      "user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].time_spending_category='low'\n",
      "43/747: user_key_timespent\n",
      "43/748: user_key_timespent\n",
      "43/749:\n",
      "user_key_timespent['time_spending_category'] = np.nan\n",
      "\n",
      "user_key_timespent.loc[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)\n",
      "                  , 'time_spending_category'] = 'medium'\n",
      "\n",
      "user_key_timespent.loc[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='high'\n",
      "\n",
      "user_key_timespent.loc[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='low'\n",
      "43/750: user_key_timespent\n",
      "43/751: user_key_timespent.head()\n",
      "43/752: user_key_timespent.gruopby('time_spending_category').count()\n",
      "43/753: user_key_timespent.groupby('time_spending_category').count()\n",
      "43/754:\n",
      "user_key_timespent['time_spending_category'] = np.nan\n",
      "\n",
      "user_key_timespent.loc[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)\n",
      "                  , 'time_spending_category'] = 'medium'\n",
      "\n",
      "user_key_timespent.loc[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='high'\n",
      "\n",
      "user_key_timespent.loc[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='low'\n",
      "\n",
      "user_key_timespent.groupby('time_spending_category').count()\n",
      "43/755:\n",
      "_percentile=0.333333333333333\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/756:\n",
      "_percentile=0.333333333333333\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>=_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>=_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/757:\n",
      "_percentile=0.333333333333333\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>=_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/758:\n",
      "_percentile=0.33\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>=_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/759:\n",
      "_percentile=0.333\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>=_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/760:\n",
      "_percentile=0.3333\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>=_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/761:\n",
      "_percentile=0.3333\n",
      "_rownumber_within_percentile = round((int(user_key_timespent.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = user_key_timespent.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(user_key_timespent['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']>=_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', user_key_timespent.shape[0])\n",
      "43/762:\n",
      "user_key_timespent['time_spending_category'] = np.nan\n",
      "\n",
      "user_key_timespent.loc[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)\n",
      "                  , 'time_spending_category'] = 'medium'\n",
      "\n",
      "user_key_timespent.loc[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='high'\n",
      "\n",
      "user_key_timespent.loc[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='low'\n",
      "\n",
      "user_key_timespent.groupby('time_spending_category').count()\n",
      "43/763:\n",
      "user_key_timespent['time_spending_category'] = np.nan\n",
      "\n",
      "user_key_timespent.loc[(user_key_timespent['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( user_key_timespent['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)\n",
      "                  , 'time_spending_category'] = 'medium'\n",
      "\n",
      "user_key_timespent.loc[user_key_timespent['minutes_spent']>_top_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='high'\n",
      "\n",
      "user_key_timespent.loc[user_key_timespent['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='low'\n",
      "\n",
      "user_key_timespent.groupby('time_spending_category').count()\n",
      "43/764:\n",
      "user_key_timespent=changelog_from_inprogress.groupby(['key', 'project', 'author']).agg({'minutes_spent':'sum'})\n",
      "user_key_timespent.reset_index(level= [0,1,2], inplace=True)\n",
      "43/765: user_key_timespent.head(3)\n",
      "43/766:\n",
      "user_task_time = pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', 'minutes_spent']]\n",
      "43/767: user_task_time.shape[0]\n",
      "43/768: user_task_time.head()\n",
      "43/769: print(user_personalities.shape[0], user_key_timespent.shape[0])\n",
      "43/770: print(user_personalities.shape[0], user_task_time.shape[0])\n",
      "43/771: print(user_personalities.shape[0], user_key_timespent.shape[0])\n",
      "43/772: print(user_personalities.shape[0], user_task_time.shape[0])\n",
      "43/773: user_task_time.head()\n",
      "43/774:\n",
      "valid_users_times = pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', 'minutes_spent']]\n",
      "43/775: valid_users_times.head()\n",
      "43/776: print(user_personalities.shape[0], valid_users_times.shape[0])\n",
      "43/777: valid_users_times.head()\n",
      "43/778:\n",
      "_percentile=0.3333\n",
      "_rownumber_within_percentile = round((int(valid_users_times.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = valid_users_times.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = valid_users_times.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "#print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(valid_users_times['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( valid_users_times['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[valid_users_times['minutes_spent']>=_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[valid_users_times['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', valid_users_times.shape[0])\n",
      "43/779: print(user_personalities.shape[0], valid_users_times.shape[0])\n",
      "43/780:\n",
      "_percentile=0.3333\n",
      "_rownumber_within_percentile = round((int(valid_users_times.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = valid_users_times.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = valid_users_times.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      user_key_timespent[(valid_users_times['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( valid_users_times['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[valid_users_times['minutes_spent']>=_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[valid_users_times['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', valid_users_times.shape[0])\n",
      "43/781:\n",
      "_percentile=0.3333\n",
      "_rownumber_within_percentile = round((int(valid_users_times.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = valid_users_times.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = valid_users_times.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      valid_users_times[(valid_users_times['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( valid_users_times['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      user_key_timespent[valid_users_times['minutes_spent']>=_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      user_key_timespent[valid_users_times['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', valid_users_times.shape[0])\n",
      "43/782:\n",
      "_percentile=0.3333\n",
      "_rownumber_within_percentile = round((int(valid_users_times.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = valid_users_times.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = valid_users_times.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      valid_users_times[(valid_users_times['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( valid_users_times['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      valid_users_times[valid_users_times['minutes_spent']>=_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      valid_users_times[valid_users_times['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', valid_users_times.shape[0])\n",
      "43/783:\n",
      "valid_users_times['time_spending_category'] = np.nan\n",
      "\n",
      "valid_users_times.loc[(valid_users_times['minutes_spent']<=_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( valid_users_times['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)\n",
      "                  , 'time_spending_category'] = 'medium'\n",
      "\n",
      "valid_users_times.loc[valid_users_times['minutes_spent']>_top_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='high'\n",
      "\n",
      "valid_users_times.loc[valid_users_times['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='low'\n",
      "\n",
      "valid_users_times.groupby('time_spending_category').count()\n",
      "43/784:\n",
      "valid_users_times['time_spending_category'] = np.nan\n",
      "\n",
      "valid_users_times.loc[(valid_users_times['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( valid_users_times['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)\n",
      "                  , 'time_spending_category'] = 'medium'\n",
      "\n",
      "valid_users_times.loc[valid_users_times['minutes_spent']>=_top_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='high'\n",
      "\n",
      "valid_users_times.loc[valid_users_times['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='low'\n",
      "\n",
      "valid_users_times.groupby('time_spending_category').count()\n",
      "43/785: valid_users_times.head()\n",
      "43/786:\n",
      "valid_users_times['time_spending_category'] = np.nan\n",
      "\n",
      "valid_users_times.loc[(valid_users_times['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( valid_users_times['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)\n",
      "                  , 'time_spending_category'] = 'medium'\n",
      "\n",
      "valid_users_times.loc[valid_users_times['minutes_spent']>=_top_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='high'\n",
      "\n",
      "valid_users_times.loc[valid_users_times['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='low'\n",
      "\n",
      "valid_users_times[['time_spending_category', 'user']].groupby('time_spending_category').count()\n",
      "43/787: valid_users_times.head()\n",
      "43/788: valid_users_times.groupby('user', 'time_spending_category').agg('key':'count')\n",
      "43/789: valid_users_times.groupby('user', 'time_spending_category').agg({'key':'count'})\n",
      "43/790: valid_users_times.groupby('user', 'time_spending_category').agg({'key':'count'})\n",
      "43/791: valid_users_times.groupby('user', 'time_spending_category').agg({'key':'count'})\n",
      "43/792: valid_users_times.head()\n",
      "43/793: valid_users_times.groupby(['user', 'time_spending_category']).agg({'key':'count'})\n",
      "43/794: valid_users_times.groupby(['user', 'time_spending_category']).agg({'key':'count'}).head()\n",
      "43/795:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_times\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = medium.user\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/796:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_times\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = highum.user\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/797:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_times\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "43/798:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_times\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "ps.sqldf(q1, locals()).head()\n",
      "43/799:\n",
      "import pandasql as ps\n",
      "\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_times\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_time_cat = ps.sqldf(q1, locals()).head()\n",
      "43/800: valid_user_time_cat.head()\n",
      "43/801:\n",
      "import pandasql as ps\n",
      "\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_times\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_time_cat = ps.sqldf(q1, locals()).head()\n",
      "print(valid_user_time_cat.shape[0])\n",
      "valid_user_time_cat.head()\n",
      "43/802:\n",
      "import pandasql as ps\n",
      "\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_times\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_time_cat = ps.sqldf(q1, locals())\n",
      "print(valid_user_time_cat.shape[0])\n",
      "valid_user_time_cat.head()\n",
      "43/803:\n",
      "print(valid_user_time_cat[(pd.isnull(valid_user_time_cat['low_timespent_tasks'])==False)\n",
      "                         & (pd.isnull(valid_user_time_cat['medium_timespent_tasks'])==False)\n",
      "                         & (pd.isnull(valid_user_time_cat['high_timespent_tasks'])==False)].shape[0])\n",
      "print(valid_user_time_cat[(pd.isnull(valid_user_time_cat['low_timespent_tasks'])==False)\n",
      "                         | (pd.isnull(valid_user_time_cat['medium_timespent_tasks'])==False)\n",
      "                         | (pd.isnull(valid_user_time_cat['high_timespent_tasks'])==False)].shape[0])\n",
      "43/804:\n",
      "print(valid_user_time_cat[(pd.isnull(valid_user_time_cat['low_timespent_tasks'])==False)\n",
      "                         & (pd.isnull(valid_user_time_cat['medium_timespent_tasks'])==False)\n",
      "                         & (pd.isnull(valid_user_time_cat['high_timespent_tasks'])==False)].shape[0])\n",
      "\n",
      "print(valid_user_time_cat[(pd.isnull(valid_user_time_cat['low_timespent_tasks'])==False)\n",
      "                         | (pd.isnull(valid_user_time_cat['medium_timespent_tasks'])==False)\n",
      "                         | (pd.isnull(valid_user_time_cat['high_timespent_tasks'])==False)].shape[0])\n",
      "43/805: issues.head()\n",
      "43/806: issues.groupby('project', 'storypoints').count()\n",
      "43/807: issues.groupby(['project', 'storypoints']).count()\n",
      "43/808:\n",
      "issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'])\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "        g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "43/809:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'])\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "43/810:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(16,6)})\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "43/811:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(16,6)})\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "43/812:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(16,6)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "43/813:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(18,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "43/814:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(18,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Reds', fmt='g')\n",
      "43/815:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Reds', fmt='g')\n",
      "43/816:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='YlGn', fmt='g')\n",
      "43/817:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='GnYl', fmt='g')\n",
      "43/818:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='YlGn', fmt='g')\n",
      "43/819:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True,  fmt='g')\n",
      "43/820:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "43/821:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/822:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique()\n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.distplot(issues[proj]['storypoints'])\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "43/823:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.distplot(issues[proj]['storypoints'])\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "43/824:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = issues[proj]['storypoints'].hist()\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "43/825:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.distplot(issues[issues['projecct']==proj]['storypoints'])\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "43/826:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.distplot(issues[issues['project']==proj]['storypoints'])\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "43/827:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.distplot(issues[issues['project']==proj]['storypoints'])\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "    \n",
      "    \n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 4, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange',]\n",
      "projects = issues['project'].unique()\n",
      "\n",
      "for i, (axx, proj) in enumerate(zip(axes.flatten(), projects)):\n",
      "    x = issues[issues['project']==proj]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "    axx.set_title(proj)\n",
      "\n",
      "plt.suptitle('Histogram of all projects storypoints', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/828:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 4, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange',]\n",
      "projects = issues['project'].unique()\n",
      "\n",
      "for i, (axx, proj) in enumerate(zip(axes.flatten(), projects)):\n",
      "    x = issues[issues['project']==proj]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "    axx.set_title(proj)\n",
      "\n",
      "plt.suptitle('Histogram of all projects storypoints', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/829:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 4, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange',]\n",
      "projects = issues['project'].unique()\n",
      "\n",
      "for i, (axx, proj) in enumerate(zip(axes.flatten(), projects)):\n",
      "    x = issues[issues['project']==proj]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=proj, color=colors[i])\n",
      "    axx.set_title(proj)\n",
      "\n",
      "plt.suptitle('Histogram of all projects storypoints', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/830:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 4, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange',]\n",
      "projects = issues['project'].unique()\n",
      "\n",
      "for i, (axx, proj) in enumerate(zip(axes.flatten(), projects)):\n",
      "    print(proj)\n",
      "    x = issues[issues['project']==proj]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=proj, color=colors[i])\n",
      "    axx.set_title(proj)\n",
      "\n",
      "plt.suptitle('Histogram of all projects storypoints', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/831:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 4, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange',]\n",
      "projects = issues['project'].unique()\n",
      "\n",
      "for i, (axx, proj) in enumerate(zip(axes.flatten(), projects)):\n",
      "    x = issues[issues['project']==proj]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=proj, color=colors[i])\n",
      "    axx.set_title(proj)\n",
      "\n",
      "plt.suptitle('Histogram of all projects storypoints', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/832:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 4, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange',]\n",
      "projects = issues['project'].unique()\n",
      "\n",
      "for i, (axx, proj) in enumerate(zip(axes.flatten(), projects)):\n",
      "    x = issues[issues['project']==proj]\n",
      "    axx.hist(x, alpha=0.8, bins=25, color=colors[i])\n",
      "    axx.set_title(proj)\n",
      "\n",
      "plt.suptitle('Histogram of all projects storypoints', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/833:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 4, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange',]\n",
      "projects = issues['project'].unique()\n",
      "\n",
      "for i, (axx, proj) in enumerate(zip(axes.flatten(), projects)):\n",
      "    x = issues[issues['project']==proj]\n",
      "    axx.hist(x, alpha=0.8, color=colors[i])\n",
      "    axx.set_title(proj)\n",
      "\n",
      "plt.suptitle('Histogram of all projects storypoints', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/834:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 4, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange',]\n",
      "projects = issues['project'].unique()\n",
      "\n",
      "for i, (axx, proj) in enumerate(zip(axes.flatten(), projects)):\n",
      "    x = issues[issues['project']==proj]\n",
      "    x.hist(alpha=0.8, color=colors[i])\n",
      "    axx.set_title(proj)\n",
      "\n",
      "plt.suptitle('Histogram of all projects storypoints', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/835:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 4, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange',]\n",
      "projects = issues['project'].unique()\n",
      "\n",
      "for i, (axx, proj) in enumerate(zip(axes.flatten(), projects)):\n",
      "    x = issues[issues['project']==proj]\n",
      "    x.hist(alpha=0.8, color=colors[i])\n",
      "\n",
      "plt.suptitle('Histogram of all projects storypoints', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/836:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 4, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange',]\n",
      "projects = issues['project'].unique()\n",
      "\n",
      "for i, (axx, proj) in enumerate(zip(axes.flatten(), projects)):\n",
      "    x = issues[issues['project']==proj]\n",
      "    x.hist()\n",
      "\n",
      "plt.suptitle('Histogram of all projects storypoints', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "43/837:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.distplot(issues[issues['project']==proj]['storypoints'])\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "    #    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "43/838:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 0\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.distplot(issues[issues['project']==proj]['storypoints'])\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "#    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "43/839:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.distplot(issues[issues['project']==proj]['storypoints'])\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "#    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "43/840:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = plt.hist(issues[issues['project']==proj]['storypoints'])\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "#    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "43/841:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = plt.hist(issues[issues['project']==proj]['storypoints'])\n",
      "   # g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "#    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "43/842:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = plt.hist(issues[issues['project']==proj]['storypoints'], alpha=0.8, bins=25)\n",
      "   # g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "#    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "43/843:\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(2, 4, n)\n",
      "    ax.set_title(proj)\n",
      "    g = plt.hist(issues[issues['project']==proj]['storypoints'], alpha=0.8, bins=25)\n",
      "   # g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "#    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "43/844:\n",
      "figure = plt.figure(figsize=(18,8))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(2, 4, n)\n",
      "    ax.set_title(proj)\n",
      "    g = plt.hist(issues[issues['project']==proj]['storypoints'], alpha=0.8, bins=25)\n",
      "   # g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "#    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "43/845:\n",
      "figure = plt.figure(figsize=(18,6))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(2, 4, n)\n",
      "    ax.set_title(proj)\n",
      "    g = plt.hist(issues[issues['project']==proj]['storypoints'], alpha=0.8, bins=25)\n",
      "   # g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "#    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "43/846:\n",
      "figure = plt.figure(figsize=(18,6))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(2, 4, n)\n",
      "    ax.set_title(proj)\n",
      "    ax.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "    g = plt.hist(issues[issues['project']==proj]['storypoints'], alpha=0.8, bins=25)\n",
      "   # g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "#    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "43/847:\n",
      "figure = plt.figure(figsize=(18,6))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(2, 4, n)\n",
      "    ax.set_title(proj)\n",
      "    plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "    g = plt.hist(issues[issues['project']==proj]['storypoints'], alpha=0.8, bins=25)\n",
      "   # g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "#    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "43/848:\n",
      "figure = plt.figure(figsize=(18,6))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    ax = figure.add_subplot(2, 4, n)\n",
      "    ax.set_title(proj)\n",
      "    plt.subplots_adjust(bottom=0.15, hspace=0.4)\n",
      "    g = plt.hist(issues[issues['project']==proj]['storypoints'], alpha=0.8, bins=25)\n",
      "   # g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "#    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "43/849: issues.head()\n",
      "43/850:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in issues['project'].unique():\n",
      "    data = issues[issues['project']==proj]\n",
      "    proj_story_tab = pd.crosstab(data['fields.issuetype.name']\n",
      "                                 , data['storypoints']\n",
      "                                 , values = data['key']\n",
      "                                 , aggfunc='count') \n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "        \n",
      "        \n",
      "#proj_story_tab = pd.crosstab(issues['fields.issuetype.name'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "#hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "#g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "#g = sns.heatmap(proj_story_tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/851:\n",
      "proj_story_tab = pd.crosstab(issues['project'], issues['storypoints'], values = issues['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/852: changelog.head()\n",
      "43/853: changelog['field'].unique()\n",
      "43/854: changelog[changelog['field']=='Points']\n",
      "43/855: changelog[changelog['field']=='Points'].shape[,]\n",
      "43/856: changelog[changelog['field']=='Points'].shape[]\n",
      "43/857: changelog[changelog['field']=='Points'].shape[0]\n",
      "43/858: changelog.groupby('field').agg('key':'count')\n",
      "43/859: changelog.groupby('field').agg({'key':'count'})\n",
      "43/860: changelog.groupby('field').agg({'key':'count'}).sort_values('field')\n",
      "43/861:\n",
      "changelog.groupby('field').agg({'key':'count'}).sort_values('field').to_csv('fields.csv')\n",
      "'Actual Story Points'\n",
      "'Customer Value Points'\n",
      "'Effort points'\n",
      "'timeoriginalestimate'\n",
      "'timeestimate'\n",
      "43/862:\n",
      "story_fields = ['Actual Story Points'\n",
      "                ,'Customer Value Points'\n",
      "                ,'Effort points'\n",
      "                ,'timeoriginalestimate'\n",
      "                ,'timeestimate'\n",
      "                ,'QA Story Points'\n",
      "                ,'Story Points']\n",
      "\n",
      "changelog[changelog['field'].isin([])].groupby('field').agg({'key':'count'}).sort_values('field')\n",
      "43/863:\n",
      "story_fields = ['Actual Story Points'\n",
      "                ,'Customer Value Points'\n",
      "                ,'Effort points'\n",
      "                ,'timeoriginalestimate'\n",
      "                ,'timeestimate'\n",
      "                ,'QA Story Points'\n",
      "                ,'Story Points']\n",
      "\n",
      "changelog[changelog['field'].isin([story_fields])].groupby('field').agg({'key':'count'}).sort_values('field')\n",
      "43/864:\n",
      "story_fields = ['Actual Story Points'\n",
      "                ,'Customer Value Points'\n",
      "                ,'Effort points'\n",
      "                ,'timeoriginalestimate'\n",
      "                ,'timeestimate'\n",
      "                ,'QA Story Points'\n",
      "                ,'Story Points']\n",
      "\n",
      "changelog[changelog['field'].isin([story_fields])].groupby('field').agg({'key':'count'}).sort_values('field')\n",
      "43/865:\n",
      "story_fields = ['Actual Story Points'\n",
      "                ,'Customer Value Points'\n",
      "                ,'Effort points'\n",
      "                ,'timeoriginalestimate'\n",
      "                ,'timeestimate'\n",
      "                ,'QA Story Points'\n",
      "                ,'Story Points']\n",
      "\n",
      "changelog[changelog['field'].isin(story_fields).groupby('field').agg({'key':'count'}).sort_values('field')\n",
      "43/866:\n",
      "story_fields = ['Actual Story Points'\n",
      "                ,'Customer Value Points'\n",
      "                ,'Effort points'\n",
      "                ,'timeoriginalestimate'\n",
      "                ,'timeestimate'\n",
      "                ,'QA Story Points'\n",
      "                ,'Story Points']\n",
      "\n",
      "changelog[changelog['field'].isin(story_fields)].groupby('field').agg({'key':'count'}).sort_values('field')\n",
      "43/867:\n",
      "story_fields = ['Actual Story Points'\n",
      "                ,'Customer Value Points'\n",
      "                ,'Effort points'\n",
      "                ,'timeoriginalestimate'\n",
      "                ,'timeestimate'\n",
      "                ,'QA Story Points'\n",
      "                ,'Story Points']\n",
      "\n",
      "changelog[changelog['field'].isin(story_fields)].groupby('field').agg({'key':'count'}).sort_values('key')\n",
      "43/868:\n",
      "story_fields = ['Actual Story Points'\n",
      "                ,'Customer Value Points'\n",
      "                ,'Effort points'\n",
      "                ,'timeoriginalestimate'\n",
      "                ,'timeestimate'\n",
      "                ,'QA Story Points'\n",
      "                ,'Story Points']\n",
      "\n",
      "changelog[changelog['field'].isin(story_fields)].groupby('field').agg({'key':'count'}).sort_values('key', ascending=False)\n",
      "43/869:\n",
      "story_fields = ['Actual Story Points'\n",
      "                ,'Customer Value Points'\n",
      "                ,'Effort points'\n",
      "                ,'timeoriginalestimate'\n",
      "                ,'timeestimate'\n",
      "                ,'QA Story Points'\n",
      "                ,'Story Points']\n",
      "\n",
      "print(changelog[changelog['field'].isin(story_fields)].groupby('field').agg({'key':'count'}).sort_values('key',ascending=False))\n",
      "43/870:\n",
      "story_fields = ['Actual Story Points'\n",
      "                ,'Customer Value Points'\n",
      "                ,'Effort points'\n",
      "                ,'timeoriginalestimate'\n",
      "                ,'timeestimate'\n",
      "                ,'QA Story Points'\n",
      "                ,'Story Points']\n",
      "\n",
      "print(changelog[changelog['field'].isin(story_fields)].groupby('field').agg({'key':'count'}).sort_values('key',ascending=False))\n",
      "print(issues.shape[0])\n",
      "43/871: issues[['key', 'storypoints']].unique().head()\n",
      "43/872: issues[['key', 'storypoints']].unique()\n",
      "43/873: issues[['key', 'storypoints']]\n",
      "43/874: issues[['key', 'storypoints']].drop_duplicates()\n",
      "43/875: issues[['key', 'storypoints']].drop_duplicates().shape[0]\n",
      "43/876:\n",
      "print(issues[['key', 'storypoints']].drop_duplicates().shape[0])\n",
      "print(issues.shape[0])\n",
      "43/877:\n",
      "print(issues[['key', 'storypoints']].drop_duplicates().shape[0])\n",
      "print(issues.shape[0])\n",
      "print(issues['key'].drop_duplicates().shape[0])\n",
      "43/878:\n",
      "print(issues[['key', 'storypoints']].drop_duplicates().shape[0])\n",
      "print(issues['key'].drop_duplicates().shape[0])\n",
      "print(issues.shape[0])\n",
      "43/879:\n",
      "unique_keys = issues[['key', 'project', 'storypoints', 'fields.issuetype.name']].drop_duplicates()\n",
      "proj_story_tab = pd.crosstab(unique_keys['project'], unique_keys['storypoints'], values = unique_keys['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/880:\n",
      "print(issues[['key', 'storypoints']].drop_duplicates().shape[0])\n",
      "print(issues['key'].drop_duplicates().shape[0])\n",
      "print(issues.shape[0])\n",
      "print(unique_keys.shape[0])\n",
      "43/881:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in unique_keys['project'].unique():\n",
      "    data = unique_keys[unique_keys['project']==proj]\n",
      "    proj_story_tab = pd.crosstab(data['fields.issuetype.name']\n",
      "                                 , data['storypoints']\n",
      "                                 , values = data['key']\n",
      "                                 , aggfunc='count') \n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "43/882:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in unique_keys['project'].unique():\n",
      "    data = unique_keys[unique_keys['project']==proj]\n",
      "    proj_story_tab = pd.crosstab(data['fields.issuetype.name']\n",
      "                                 , data['storypoints']\n",
      "                                 , values = data['key']\n",
      "                                 , aggfunc='count') \n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "    g.set_yticklabels(g.get_yticklabels())\n",
      "    n = n + 1\n",
      "43/883:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "figure = plt.figure(figsize=(20,24))\n",
      "n = 1\n",
      "for proj in unique_keys['project'].unique():\n",
      "    data = issues[unique_keys['project']==proj]\n",
      "    proj_story_tab = pd.crosstab(data['fields.issuetype.name']\n",
      "                                 , data['storypoints']\n",
      "                                 , values = data['key']\n",
      "                                 , aggfunc='count') \n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=90)\n",
      "    n = n + 1\n",
      "43/884:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "figure = plt.figure(figsize=(20,20))\n",
      "n = 1\n",
      "for proj in unique_keys['project'].unique():\n",
      "    data = issues[unique_keys['project']==proj]\n",
      "    proj_story_tab = pd.crosstab(data['fields.issuetype.name']\n",
      "                                 , data['storypoints']\n",
      "                                 , values = data['key']\n",
      "                                 , aggfunc='count') \n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=90)\n",
      "    n = n + 1\n",
      "43/885:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "figure = plt.figure(figsize=(20,20))\n",
      "n = 1\n",
      "for proj in unique_keys['project'].unique():\n",
      "    data = issues[unique_keys['project']==proj]\n",
      "    proj_story_tab = pd.crosstab(data['fields.issuetype.name']\n",
      "                                 , data['storypoints']\n",
      "                                 , values = data['key']\n",
      "                                 , aggfunc='count') \n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=89)\n",
      "    n = n + 1\n",
      "43/886:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "figure = plt.figure(figsize=(20,20))\n",
      "n = 1\n",
      "for proj in unique_keys['project'].unique():\n",
      "    data = issues[unique_keys['project']==proj]\n",
      "    proj_story_tab = pd.crosstab(data['fields.issuetype.name']\n",
      "                                 , data['storypoints']\n",
      "                                 , values = data['key']\n",
      "                                 , aggfunc='count') \n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "43/887:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in unique_keys['project'].unique():\n",
      "    data = issues[unique_keys['project']==proj]\n",
      "    proj_story_tab = pd.crosstab(data['fields.issuetype.name']\n",
      "                                 , data['storypoints']\n",
      "                                 , values = data['key']\n",
      "                                 , aggfunc='count') \n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "    n = n + 1\n",
      "43/888:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "figure = plt.figure(figsize=(18,24))\n",
      "n = 1\n",
      "for proj in unique_keys['project'].unique():\n",
      "    data = unique_keys[unique_keys['project']==proj]\n",
      "    proj_story_tab = pd.crosstab(data['fields.issuetype.name']\n",
      "                                 , data['storypoints']\n",
      "                                 , values = data['key']\n",
      "                                 , aggfunc='count') \n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=90)\n",
      "    n = n + 1\n",
      "43/889:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "figure = plt.figure(figsize=(18,24))\n",
      "n = 1\n",
      "for proj in unique_keys['project'].unique():\n",
      "    data = unique_keys[unique_keys['project']==proj]\n",
      "    proj_story_tab = pd.crosstab(data['fields.issuetype.name']\n",
      "                                 , data['storypoints']\n",
      "                                 , values = data['key']\n",
      "                                 , aggfunc='count') \n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=30)\n",
      "    n = n + 1\n",
      "43/890:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "figure = plt.figure(figsize=(18,18))\n",
      "n = 1\n",
      "for proj in unique_keys['project'].unique():\n",
      "    data = unique_keys[unique_keys['project']==proj]\n",
      "    proj_story_tab = pd.crosstab(data['fields.issuetype.name']\n",
      "                                 , data['storypoints']\n",
      "                                 , values = data['key']\n",
      "                                 , aggfunc='count') \n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=30)\n",
      "    n = n + 1\n",
      "43/891:\n",
      "#issues.groupby(['project', 'storypoints']).count()\n",
      "\n",
      "figure = plt.figure(figsize=(18,20))\n",
      "n = 1\n",
      "for proj in unique_keys['project'].unique():\n",
      "    data = unique_keys[unique_keys['project']==proj]\n",
      "    proj_story_tab = pd.crosstab(data['fields.issuetype.name']\n",
      "                                 , data['storypoints']\n",
      "                                 , values = data['key']\n",
      "                                 , aggfunc='count') \n",
      "    ax = figure.add_subplot(4, 2, n)\n",
      "    ax.set_title(proj)\n",
      "    g = sns.heatmap(proj_story_tab, annot=True, cmap='Purples', fmt='g')\n",
      "    g.set_yticklabels(g.get_yticklabels(), rotation=30)\n",
      "    n = n + 1\n",
      "43/892:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')]\n",
      "log_keys.head()\n",
      "43/893:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')]\n",
      "print(log_keys['key'].drop_duplicates().shape[0])\n",
      "print(log_keys['key'].shape[0])\n",
      "43/894:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')]\n",
      "print(log_keys['key'].drop_duplicates().shape[0])\n",
      "print(log_keys['key'].shape[0])\n",
      "log_keys.head()\n",
      "43/895:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')].reset_index()\n",
      "print(log_keys['key'].drop_duplicates().shape[0])\n",
      "print(log_keys['key'].shape[0])\n",
      "log_keys.head()\n",
      "43/896:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')].reset_index()\n",
      "print(log_keys['key'].drop_duplicates().shape[0])\n",
      "print(log_keys['key'].shape[0])\n",
      "log_keys.head()\n",
      "\n",
      "for i in range(0, log_keys.shape[0]):\n",
      "    log_keys.loc[i]\n",
      "43/897:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')].reset_index()\n",
      "print(log_keys['key'].drop_duplicates().shape[0])\n",
      "print(log_keys['key'].shape[0])\n",
      "log_keys.head()\n",
      "\n",
      "for i in range(0, log_keys.shape[0]):\n",
      "    log_keys.loc[i]\n",
      "print(log_keys.loc[i])\n",
      "43/898:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')].reset_index()\n",
      "print(log_keys['key'].drop_duplicates().shape[0])\n",
      "print(log_keys['key'].shape[0])\n",
      "log_keys.head()\n",
      "\n",
      "for i in range(0, log_keys.shape[0]):\n",
      "    log_keys.loc[i]\n",
      "print(log_keys.loc[i, 'author'])\n",
      "43/899:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')].reset_index()\n",
      "print(log_keys['key'].drop_duplicates().shape[0])\n",
      "print(log_keys['key'].shape[0])\n",
      "log_keys.head()\n",
      "43/900: changelog[(changelog['field']=='issuetype')].head()\n",
      "43/901:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')].reset_index()\n",
      "print(log_keys['key'].drop_duplicates().shape[0])\n",
      "print(log_keys['key'].shape[0])\n",
      "log_keys.head()\n",
      "log_keys['issuetype']=np.nan\n",
      "\n",
      "for i in range(0, log_keys.shape[0]):\n",
      "    _key = log_keys.loc[i, 'key']\n",
      "    _created = log_keys.loc[i, 'created']\n",
      "    \n",
      "    issuetype_log = changelog[(changelog['field']=='issuetype')&\n",
      "              (changelog['key']==_key)&\n",
      "              (changelog['created']<=_created)].sort_values('created', ascending=False).head(1).reset_index()\n",
      "    _issuetype_log_value = issuetype_log.loc[0, 'toString']\n",
      "    _issuetype_issues_value = issues[issues['key']==_key].head(1).reset_index().loc[0, 'fields.issuetype.name']\n",
      "    \n",
      "    if pd.isnull(_issuetype_log_value)==False:\n",
      "        _issuetype = _issuetype_log_value\n",
      "    else:\n",
      "        _issuetype = _issuetype_issues_value\n",
      "    log_keys[i, 'issuetype']=_issuetype\n",
      "43/902:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')].reset_index()\n",
      "print(log_keys['key'].drop_duplicates().shape[0])\n",
      "print(log_keys['key'].shape[0])\n",
      "log_keys.head()\n",
      "log_keys['issuetype']=np.nan\n",
      "\n",
      "for i in range(0, log_keys.shape[0]):\n",
      "    _key = log_keys.loc[i, 'key']\n",
      "    _created = log_keys.loc[i, 'created']\n",
      "    \n",
      "    issuetype_log = changelog[(changelog['field']=='issuetype')&\n",
      "              (changelog['key']==_key)&\n",
      "              (changelog['created']<=_created)].sort_values('created', ascending=False).head(1).reset_index()\n",
      "    _issuetype_log_value = issuetype_log.loc[1, 'toString']\n",
      "    _issuetype_issues_value = issues[issues['key']==_key].head(1).reset_index().loc[0, 'fields.issuetype.name']\n",
      "    \n",
      "    if pd.isnull(_issuetype_log_value)==False:\n",
      "        _issuetype = _issuetype_log_value\n",
      "    else:\n",
      "        _issuetype = _issuetype_issues_value\n",
      "    log_keys[i, 'issuetype']=_issuetype\n",
      "43/903:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')].reset_index()\n",
      "print(log_keys['key'].drop_duplicates().shape[0])\n",
      "print(log_keys['key'].shape[0])\n",
      "log_keys.head()\n",
      "log_keys['issuetype']=np.nan\n",
      "\n",
      "for i in range(0, log_keys.shape[0]):\n",
      "    _key = log_keys.loc[i, 'key']\n",
      "    _created = log_keys.loc[i, 'created']\n",
      "    \n",
      "    issuetype_log = changelog[(changelog['field']=='issuetype')&\n",
      "              (changelog['key']==_key)&\n",
      "              (changelog['created']<=_created)].sort_values('created', ascending=False).head(1).reset_index()\n",
      "    _issuetype_log_value = issuetype_log.loc[0, 'toString']\n",
      "    _issuetype_issues_value = issues[issues['key']==_key].head(1).reset_index().loc[0, 'fields.issuetype.name']\n",
      "    \n",
      "    if pd.isnull(_issuetype_log_value)==False:\n",
      "        _issuetype = _issuetype_log_value\n",
      "    else:\n",
      "        _issuetype = _issuetype_issues_value\n",
      "    log_keys[i, 'issuetype']=_issuetype\n",
      "43/904: issuetype_log\n",
      "43/905: issuetype_log.shape[0]\n",
      "43/906:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')].reset_index()\n",
      "print(log_keys['key'].drop_duplicates().shape[0])\n",
      "print(log_keys['key'].shape[0])\n",
      "log_keys.head()\n",
      "log_keys['issuetype']=np.nan\n",
      "\n",
      "for i in range(0, log_keys.shape[0]):\n",
      "    _key = log_keys.loc[i, 'key']\n",
      "    _created = log_keys.loc[i, 'created']\n",
      "    \n",
      "    issuetype_log = changelog[(changelog['field']=='issuetype')&\n",
      "              (changelog['key']==_key)&\n",
      "              (changelog['created']<=_created)].sort_values('created', ascending=False).head(1).reset_index()\n",
      "    log_rows = issuetype_log.shape[0]\n",
      "    if log_rows > 0:\n",
      "        _issuetype = issuetype_log.loc[0, 'toString']\n",
      "    else:\n",
      "        _issuetype = issues[issues['key']==_key].head(1).reset_index().loc[0, 'fields.issuetype.name']\n",
      "    log_keys[i, 'issuetype']=_issuetype\n",
      "43/907: log_keys.head()\n",
      "43/908:\n",
      "log_keys = changelog[(changelog['field']=='Story Points')].reset_index()\n",
      "print(log_keys['key'].drop_duplicates().shape[0])\n",
      "print(log_keys['key'].shape[0])\n",
      "log_keys.head()\n",
      "log_keys['issuetype']=np.nan\n",
      "\n",
      "for i in range(0, log_keys.shape[0]):\n",
      "    _key = log_keys.loc[i, 'key']\n",
      "    _created = log_keys.loc[i, 'created']\n",
      "    \n",
      "    issuetype_log = changelog[(changelog['field']=='issuetype')&\n",
      "              (changelog['key']==_key)&\n",
      "              (changelog['created']<=_created)].sort_values('created', ascending=False).head(1).reset_index()\n",
      "    log_rows = issuetype_log.shape[0]\n",
      "    if log_rows > 0:\n",
      "        _issuetype = issuetype_log.loc[0, 'toString']\n",
      "    else:\n",
      "        _issuetype = issues[issues['key']==_key].head(1).reset_index().loc[0, 'fields.issuetype.name']\n",
      "    log_keys.loc[i, 'issuetype']=_issuetype\n",
      "43/909: log_keys.head()\n",
      "43/910: todo_actions.head()\n",
      "43/911: log_todo.head()\n",
      "43/912: users_personalities_priorities.head()\n",
      "43/913: #issues[['fields.status.statusCategory.name', 'fields.status.name', 'project']].groupby(['fields.status.statusCategory.name', 'fields.status.name']).count()\n",
      "43/914:\n",
      "log_state = changelog[(changelog['field']=='status')]\n",
      "log_todo = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_todo))]\n",
      "log_inprogress = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_inprogress))]\n",
      "log_done = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_done))]\n",
      "43/915:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state = log_state.groupby(['author', 'toString']).agg({'created':'count'})\n",
      "log_state.reset_index(level= [0,1], inplace=True)\n",
      "43/916:\n",
      "log_state = changelog[(changelog['field']=='status')]\n",
      "log_todo = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_todo))]\n",
      "log_inprogress = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_inprogress))]\n",
      "log_done = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_done))]\n",
      "43/917:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "state_actions = log_state.groupby(['author', 'toString']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/918: print(todo_actions.shape[0], inprogress_actions.shape[0], done_actions.shape[0], state_actions.shape[0])\n",
      "43/919: todo_actions.head()\n",
      "43/920:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state']=log_state['toString'].apply(lambda x: if x.isin(_todo) 'todo' else 'done')\n",
      "state_actions = log_state.groupby(['author', 'toString']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/921:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state']=log_state['toString'].apply(lambda x: if x.isin(_todo) then 'todo' else 'done')\n",
      "state_actions = log_state.groupby(['author', 'toString']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/922:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state']=log_state['toString'].apply(lambda x: if x.isin(_todo), 'todo' else 'done')\n",
      "state_actions = log_state.groupby(['author', 'toString']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/923:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state']=log_state['toString'].apply(lambda x: if True x.isin(_todo) 'todo' else 'done')\n",
      "state_actions = log_state.groupby(['author', 'toString']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/924:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state']=log_state['toString'].apply(lambda x:  'todo' if True x.isin(_todo) else 'done')\n",
      "state_actions = log_state.groupby(['author', 'toString']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/925:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state']=log_state['toString'].apply(lambda x:  'todo' if x.isin(_todo) else 'done')\n",
      "state_actions = log_state.groupby(['author', 'toString']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/926:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state']=log_state['toString'].apply(lambda x:  'todo' if x  in _todo else 'done')\n",
      "state_actions = log_state.groupby(['author', 'toString']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/927:\n",
      "log_state = changelog[(changelog['field']=='status')]\n",
      "log_todo = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_todo))]\n",
      "log_inprogress = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_inprogress))]\n",
      "log_done = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_done))]\n",
      "43/928:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state'] = np.nan\n",
      "log_state[log_state['toString'].isin(_todo), 'state']='todo'\n",
      "log_state[log_state['toString'].isin(_inprogress), 'state']='inprogress'\n",
      "log_state[log_state['toString'].isin(_done), 'state']='done'\n",
      "\n",
      "state_actions = log_state.groupby(['author', 'state']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/929:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state'] = np.nan\n",
      "log_state.loc[log_state['toString'].isin(_todo), 'state']='todo'\n",
      "log_state.loc[log_state['toString'].isin(_inprogress), 'state']='inprogress'\n",
      "log_state.loc[log_state['toString'].isin(_done), 'state']='done'\n",
      "\n",
      "state_actions = log_state.groupby(['author', 'state']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/930:\n",
      "log_state = changelog[(changelog['field']=='status')]\n",
      "log_todo = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_todo))]\n",
      "log_inprogress = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_inprogress))]\n",
      "log_done = changelog[(changelog['field']=='status') & (changelog['toString'].isin(_done))]\n",
      "43/931:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state'] = np.nan\n",
      "log_state.loc[log_state['toString'].isin(_todo), 'state']='todo'\n",
      "log_state.loc[log_state['toString'].isin(_inprogress), 'state']='inprogress'\n",
      "log_state.loc[log_state['toString'].isin(_done), 'state']='done'\n",
      "\n",
      "state_actions = log_state.groupby(['author', 'state']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/932:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state'] = np.nan\n",
      "log_state.loc[log_state.toString.isin(_todo), 'state']='todo'\n",
      "log_state.loc[log_state.toString.isin(_inprogress), 'state']='inprogress'\n",
      "log_state.loc[log_state.toString.isin(_done), 'state']='done'\n",
      "\n",
      "state_actions = log_state.groupby(['author', 'state']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/933:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state'] = np.nan\n",
      "log_state.loc[log_state.toString.isin(_todo)==True, 'state']='todo'\n",
      "log_state.loc[log_state.toString.isin(_inprogress)==True, 'state']='inprogress'\n",
      "log_state.loc[log_state.toString.isin(_done)==True, 'state']='done'\n",
      "\n",
      "state_actions = log_state.groupby(['author', 'state']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/934:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state'] = np.nan\n",
      "log_state.loc[(log_state.toString in _todo)==True, 'state']='todo'\n",
      "log_state.loc[log_state.toString.isin(_inprogress)==True, 'state']='inprogress'\n",
      "log_state.loc[log_state.toString.isin(_done)==True, 'state']='done'\n",
      "\n",
      "state_actions = log_state.groupby(['author', 'state']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/935:\n",
      "todo_actions = log_todo.groupby('author').agg({'created':'count'})\n",
      "todo_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "inprogress_actions = log_inprogress.groupby('author').agg({'created':'count'})\n",
      "inprogress_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "done_actions = log_done.groupby('author').agg({'created':'count'})\n",
      "done_actions.reset_index(level= [0], inplace=True)\n",
      "\n",
      "log_state['state'] = np.nan\n",
      "log_state.loc[(log_state.toString.isin(_todo)), 'state']='todo'\n",
      "log_state.loc[(log_state.toString.isin(_inprogress)), 'state']='inprogress'\n",
      "log_state.loc[(log_state.toString.isin(_done)), 'state']='done'\n",
      "\n",
      "state_actions = log_state.groupby(['author', 'state']).agg({'created':'count'})\n",
      "state_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/936: print(todo_actions.shape[0], inprogress_actions.shape[0], done_actions.shape[0], state_actions.shape[0])\n",
      "43/937: state_actions.head()\n",
      "43/938: log_state.head()\n",
      "43/939: todo_actions.head()\n",
      "43/940: print(state_actions.shape[0])\n",
      "43/941:\n",
      "print(state_actions.shape[0])\n",
      "log_state.head()\n",
      "43/942: state_actions.head()\n",
      "43/943:\n",
      "state_actions.shape[0]\n",
      "state_actions.head()\n",
      "43/944:\n",
      "print(state_actions.shape[0])\n",
      "state_actions.head()\n",
      "43/945:\n",
      "print(log_state.shape[0])\n",
      "log_state.head()\n",
      "43/946:\n",
      "print(state_actions.shape[0])\n",
      "state_actions.head()\n",
      "43/947:\n",
      "valid_users_states = pd.merge(user_personalities, log_state, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', 'minutes_spent']]\n",
      "43/948:\n",
      "valid_users_states = pd.merge(user_personalities, log_state, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', 'state']]\n",
      "43/949: valid_users_states.head()\n",
      "43/950:\n",
      "import pandasql as ps\n",
      "\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_times\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_timespent_tasks\n",
      "    FROM valid_users_times AS F\n",
      "    WHERE time_spending_category = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_time_agg = ps.sqldf(q1, locals())\n",
      "print(valid_user_time_agg.shape[0])\n",
      "valid_user_time_agg.head()\n",
      "43/951:\n",
      "print(valid_user_time_agg[(pd.isnull(valid_user_time_agg['low_timespent_tasks'])==False)\n",
      "                         & (pd.isnull(valid_user_time_agg['medium_timespent_tasks'])==False)\n",
      "                         & (pd.isnull(valid_user_time_agg['high_timespent_tasks'])==False)].shape[0])\n",
      "\n",
      "print(valid_user_time_agg[(pd.isnull(valid_user_time_agg['low_timespent_tasks'])==False)\n",
      "                         | (pd.isnull(valid_user_time_agg['medium_timespent_tasks'])==False)\n",
      "                         | (pd.isnull(valid_user_time_agg['high_timespent_tasks'])==False)].shape[0])\n",
      "43/952:\n",
      "valid_users_states_agg = pd.merge(\n",
      "    pd.merge(\n",
      "        pd.merge(user_personalities, valid_users_states[valid_users_states['state']=='todo'], how = 'left', left_on = 'user', right_on = 'author'),\n",
      "            valid_users_states[valid_users_states['state']=='inprogress'], how = 'left', left_on = 'user', right_on = 'author'),\n",
      "                valid_users_states[valid_users_states['state']=='done'], how = 'left', left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'created', 'created_x', 'created_y']]\n",
      "43/953:\n",
      "valid_users_states_agg = pd.merge(\n",
      "    pd.merge(\n",
      "        pd.merge(user_personalities, valid_users_states[valid_users_states['state']=='todo'], how = 'left', left_on = 'user', right_on = 'user'),\n",
      "            valid_users_states[valid_users_states['state']=='inprogress'], how = 'left', left_on = 'user', right_on = 'user'),\n",
      "                valid_users_states[valid_users_states['state']=='done'], how = 'left', left_on = 'user', right_on = 'user')[['user', 'emailAddress', 'created', 'created_x', 'created_y']]\n",
      "43/954:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_states\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS todo_category_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'todo'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS inprogress_category_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'inprogress'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS done_category_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'done'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_states_agg = ps.sqldf(q1, locals())\n",
      "print(valid_user_states_agg.shape[0])\n",
      "valid_user_states_agg.head()\n",
      "43/955:\n",
      "q1 = \"\"\"\n",
      "Select U.user, todo_tasks, inprogress_tasks, done_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_states\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS todo_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'todo'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS inprogress_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'inprogress'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS done_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'done'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_states_agg = ps.sqldf(q1, locals())\n",
      "print(valid_user_states_agg.shape[0])\n",
      "valid_user_states_agg.head()\n",
      "43/956: changelog[(changelog['field']=='status') & changelog['author']=='grussell']\n",
      "43/957: changelog[(changelog['field']=='status')]\n",
      "43/958: changelog[(changelog['field']=='status') & (changelog['author']=='grussell')]\n",
      "43/959: changelog[(changelog['field']=='status') & (changelog['author']=='grussell')].head()\n",
      "43/960: valid_users_states.shape[0]\n",
      "43/961: valid_users_states.drop_duplicates().shape[0]\n",
      "43/962:\n",
      "print(valid_users_states.shape[0], valid_users_states.drop_suplicates().shape[0])\n",
      "valid_users_states.head()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/963:\n",
      "print(valid_users_states.shape[0], valid_users_states.drop_duplicates().shape[0])\n",
      "valid_users_states.head()\n",
      "43/964:\n",
      "print(valid_users_states.shape[0], \n",
      "      valid_users_states.drop_duplicates().shape[0],\n",
      "      valid_users_states['user', 'key'].drop_duplicates().shape[0])\n",
      "valid_users_states.head()\n",
      "43/965:\n",
      "print(valid_users_states.shape[0], \n",
      "      valid_users_states.drop_duplicates().shape[0],\n",
      "      valid_users_states[['user', 'key']].drop_duplicates().shape[0])\n",
      "valid_users_states.head()\n",
      "43/966: valid_users_states = valid_users_states.drop_duplicates()\n",
      "43/967:\n",
      "q1 = \"\"\"\n",
      "Select U.user, todo_tasks, inprogress_tasks, done_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_states\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS todo_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'todo'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS inprogress_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'inprogress'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS done_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'done'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_states_agg = ps.sqldf(q1, locals())\n",
      "print(valid_user_states_agg.shape[0])\n",
      "valid_user_states_agg.head()\n",
      "43/968: changelog[(changelog['field']=='status') & (changelog['author']=='grussell')]['key'].unique()\n",
      "43/969: changelog[(changelog['field']=='status') & (changelog['author']=='grussell')]['key'].drop_duplicates()\n",
      "43/970: changelog[(changelog['field']=='status') & (changelog['author']=='grussell')]['key'].drop_duplicates().shape()\n",
      "43/971: changelog[(changelog['field']=='status') & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0]\n",
      "43/972:\n",
      "q1 = \"\"\"\n",
      "Select U.user, todo_tasks, inprogress_tasks, done_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_states\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS todo_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'todo'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS inprogress_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'inprogress'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS done_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'done'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_states_agg = ps.sqldf(q1, locals())\n",
      "print(valid_user_states_agg.shape[0], changelog[(changelog['field']=='status') & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "valid_user_states_agg.head()\n",
      "43/973:\n",
      "print(valid_user_states_agg[(pd.isnull(valid_user_states_agg['done_tasks'])==False)\n",
      "                         & (pd.isnull(valid_user_states_agg['todo_tasks'])==False)\n",
      "                         & (pd.isnull(valid_user_states_agg['inprogress_tasks'])==False)].shape[0])\n",
      "\n",
      "print(valid_user_states_agg[(pd.isnull(valid_user_states_agg['done_tasks'])==False)\n",
      "                         | (pd.isnull(valid_user_states_agg['todo_tasks'])==False)\n",
      "                         | (pd.isnull(valid_user_states_agg['inprogress_tasks'])==False)].shape[0])\n",
      "43/974:\n",
      "_high = ['High', 'Critical', 'Blocker']\n",
      "_medium = ['Medium', 'Major']\n",
      "_low = ['Low', 'Minor', 'None', 'Trivial', 'To be reviewed']\n",
      "43/975:\n",
      "log_priorities = changelog[(changelog['field']=='priority')]\n",
      "\n",
      "log_high = changelog[(changelog['field']=='priority') & (changelog['toString'].isin(_high))]\n",
      "log_medium = changelog[(changelog['field']=='priority') & (changelog['toString'].isin(_medium))]\n",
      "log_low = changelog[(changelog['field']=='priority') & (changelog['toString'].isin(_low))]\n",
      "43/976:\n",
      "log_priorities['priority'] = np.nan\n",
      "log_priorities.loc[(log_priorities.toString.isin(_high)), 'priority']='high'\n",
      "log_priorities.loc[(log_priorities.toString.isin(_medium)), 'priority']='medium'\n",
      "log_priorities.loc[(log_priorities.toString.isin(_low)), 'priority']='low'\n",
      "\n",
      "log_priorities_actions = log_priorities.groupby(['author', 'priority']).agg({'created':'count'})\n",
      "log_priorities_actions.reset_index(level= [0,1], inplace=True)\n",
      "43/977: print(log_priorities_actions.shape[0],log_priorities.shape[0])\n",
      "43/978: log_priorities_actions.head()\n",
      "43/979: log_priorities.head()\n",
      "43/980: print(log_priorities_actions.shape[0],log_priorities.shape[0], log_priorities.drop_duplicates().shape[0])\n",
      "43/981: print(log_priorities_actions.shape[0],log_priorities.shape[0], log_priorities['author', 'key'].drop_duplicates().shape[0])\n",
      "43/982: print(log_priorities_actions.shape[0],log_priorities.shape[0], log_priorities[['author', 'key']].drop_duplicates().shape[0])\n",
      "43/983:\n",
      "valid_users_priorities = pd.merge(user_personalities, log_priorities, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', 'priority']]\n",
      "43/984:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_tasks, medium_tasks, high_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_priorities\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_tasks\n",
      "    FROM valid_users_priorities AS F\n",
      "    WHERE priority = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_tasks\n",
      "    FROM valid_users_priorities AS F\n",
      "    WHERE priority = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_tasks\n",
      "    FROM valid_users_priorities AS F\n",
      "    WHERE priority = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_states_agg = ps.sqldf(q1, locals())\n",
      "print(valid_user_states_agg.shape[0], \n",
      "      changelog[(changelog['field']=='status') & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "valid_user_states_agg.head()\n",
      "43/985:\n",
      "log_priorities['priority'] = np.nan\n",
      "log_priorities.loc[(log_priorities.toString.isin(_high)), 'priority']='high'\n",
      "log_priorities.loc[(log_priorities.toString.isin(_medium)), 'priority']='medium'\n",
      "log_priorities.loc[(log_priorities.toString.isin(_low)), 'priority']='low'\n",
      "43/986:\n",
      "valid_users_priorities = pd.merge(user_personalities, log_priorities, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', 'priority']]\n",
      "43/987: print(log_priorities_actions.shape[0],log_priorities.shape[0], log_priorities[['author', 'key']].drop_duplicates().shape[0])\n",
      "43/988:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_tasks, medium_tasks, high_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_priorities\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_tasks\n",
      "    FROM valid_users_priorities AS F\n",
      "    WHERE priority = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_tasks\n",
      "    FROM valid_users_priorities AS F\n",
      "    WHERE priority = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_tasks\n",
      "    FROM valid_users_priorities AS F\n",
      "    WHERE priority = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_states_agg = ps.sqldf(q1, locals())\n",
      "print(valid_user_states_agg.shape[0], \n",
      "      changelog[(changelog['field']=='status') & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "valid_user_states_agg.head()\n",
      "43/989:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_tasks, medium_tasks, high_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_priorities\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_tasks\n",
      "    FROM valid_users_priorities AS F\n",
      "    WHERE priority = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_tasks\n",
      "    FROM valid_users_priorities AS F\n",
      "    WHERE priority = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_tasks\n",
      "    FROM valid_users_priorities AS F\n",
      "    WHERE priority = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_users_priorities_agg = ps.sqldf(q1, locals())\n",
      "print(valid_users_priorities_agg.shape[0], \n",
      "      changelog[(changelog['field']=='priority') & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "valid_users_priorities_agg.head()\n",
      "43/990:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_tasks, medium_tasks, high_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_priorities\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_tasks\n",
      "    FROM valid_users_priorities AS F\n",
      "    WHERE priority = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_tasks\n",
      "    FROM valid_users_priorities AS F\n",
      "    WHERE priority = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_tasks\n",
      "    FROM valid_users_priorities AS F\n",
      "    WHERE priority = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_users_priorities_agg = ps.sqldf(q1, locals())\n",
      "print(valid_users_priorities_agg.shape[0], \n",
      "      changelog[(changelog['field']=='priority') & (changelog['author']=='mark.pollack')]['key'].drop_duplicates().shape[0])\n",
      "valid_users_priorities_agg.head()\n",
      "43/991:\n",
      "q1 = \"\"\"\n",
      "Select U.user, todo_tasks, inprogress_tasks, done_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_states\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS todo_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'todo'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS inprogress_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'inprogress'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS done_tasks\n",
      "    FROM valid_users_states AS F\n",
      "    WHERE state = 'done'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_states_agg = ps.sqldf(q1, locals())\n",
      "print(valid_user_states_agg.shape[0], \n",
      "      changelog[(changelog['field']=='status') & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "valid_user_states_agg.head()\n",
      "43/992:\n",
      "print(valid_users_priorities_agg[(pd.isnull(valid_users_priorities_agg['high_tasks'])==False)\n",
      "                         & (pd.isnull(valid_users_priorities_agg['medium_tasks'])==False)\n",
      "                         & (pd.isnull(valid_users_priorities_agg['low_tasks'])==False)].shape[0])\n",
      "\n",
      "print(valid_users_priorities_agg[(pd.isnull(valid_users_priorities_agg['high_tasks'])==False)\n",
      "                         | (pd.isnull(valid_users_priorities_agg['medium_tasks'])==False)\n",
      "                         | (pd.isnull(valid_users_priorities_agg['low_tasks'])==False)].shape[0])\n",
      "43/993:\n",
      "print(issues[['key', 'storypoints']].drop_duplicates().shape[0])\n",
      "print(issues['key'].drop_duplicates().shape[0])\n",
      "print(issues.shape[0])\n",
      "43/994: issuetype_log.shape[0]\n",
      "43/995: changelog[changelog['field'].isin(story_fields)].shape[0]\n",
      "43/996:\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "pd.merge(story_log, issues, how='inner', left_on = 'author', right_on='user')\n",
      "43/997:\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "pd.merge(story_log, issues, how='inner', left_on = 'author', right_on='author')\n",
      "43/998:\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "pd.merge(story_log, issues, how='inner', left_on = 'key', right_on='key')\n",
      "43/999:\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "pd.merge(story_log, issues[['key', 'fields.issuetype.name']], how='inner', left_on = 'key', right_on='key')\n",
      "43/1000:\n",
      "print(issues[['key', 'fields.issuetype.name']].drop_duplicates(),\n",
      "     issues[['key']].drop_duplicates())\n",
      "\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "pd.merge(story_log, issues[['key', 'fields.issuetype.name']].drop_duplicates(), how='inner', left_on = 'key', right_on='key')\n",
      "43/1001:\n",
      "print(issues[['key', 'fields.issuetype.name']].drop_duplicates(),\n",
      "     issues[['key']].drop_duplicates())\n",
      "\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "#pd.merge(story_log, issues[['key', 'fields.issuetype.name']].drop_duplicates(), how='inner', left_on = 'key', right_on='key')\n",
      "43/1002:\n",
      "print(issues[['key', 'fields.issuetype.name']].drop_duplicates().shape[0],\n",
      "     issues[['key']].drop_duplicates().shape[0])\n",
      "\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "#pd.merge(story_log, issues[['key', 'fields.issuetype.name']].drop_duplicates(), how='inner', left_on = 'key', right_on='key')\n",
      "43/1003: story_log.columns\n",
      "43/1004:\n",
      "print(issues[['key', 'fields.issuetype.name']].drop_duplicates().shape[0],\n",
      "     issues[['key']].drop_duplicates().shape[0])\n",
      "\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "story_log = pd.merge(story_log, issues[['key', 'fields.issuetype.name']].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "       'project', 'to', 'toString', 'fields.issuetype.name']\n",
      "43/1005:\n",
      "print(issues[['key', 'fields.issuetype.name']].drop_duplicates().shape[0],\n",
      "     issues[['key']].drop_duplicates().shape[0])\n",
      "\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "story_log = pd.merge(story_log, issues[['key', 'fields.issuetype.name']].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "       'project', 'to', 'toString', 'fields.issuetype.name']]\n",
      "43/1006: story_log['fields.issuetype.name'].unique()\n",
      "43/1007:\n",
      "print(issues[['key', 'fields.issuetype.name']].drop_duplicates().shape[0],\n",
      "     issues[['key']].drop_duplicates().shape[0])\n",
      "\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "story_log = pd.merge(story_log, issues[['key', 'fields.issuetype.name']].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "       'project', 'to', 'toString', 'fields.issuetype.name']]\n",
      "\n",
      "story_log = story_log[story_log['fields.issuetype.name']!='Epic']\n",
      "43/1008: story_log['fields.issuetype.name'].unique()\n",
      "43/1009: story_log.head()\n",
      "43/1010:\n",
      "print(issues[['key', 'fields.issuetype.name']].drop_duplicates().shape[0],\n",
      "     issues[['key']].drop_duplicates().shape[0])\n",
      "\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "story_log = pd.merge(story_log, issues[['key', 'fields.issuetype.name']].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "       'project', 'to', 'toString', 'fields.issuetype.name']]\n",
      "\n",
      "story_log = story_log[story_log['fields.issuetype.name']!='Epic']\n",
      "story_log.head()\n",
      "43/1011:\n",
      "print(issues[['key', 'fields.issuetype.name']].drop_duplicates().shape[0],\n",
      "     issues[['key']].drop_duplicates().shape[0])\n",
      "\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "story_log = pd.merge(story_log, issues[['key', 'fields.issuetype.name']].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "       'project', 'to', 'toString', 'fields.issuetype.name']]\n",
      "\n",
      "story_log = story_log[story_log['fields.issuetype.name']!='Epic']\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "\n",
      "print(story_log[story_log['toString'].isin(story_points).shape[0],story_log.shape[0]) \n",
      "#story_log.head()\n",
      "43/1012:\n",
      "print(issues[['key', 'fields.issuetype.name']].drop_duplicates().shape[0],\n",
      "     issues[['key']].drop_duplicates().shape[0])\n",
      "\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "story_log = pd.merge(story_log, issues[['key', 'fields.issuetype.name']].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "       'project', 'to', 'toString', 'fields.issuetype.name']]\n",
      "\n",
      "story_log = story_log[story_log['fields.issuetype.name']!='Epic']\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "\n",
      "print(story_log[story_log['toString'].isin(story_points)].shape[0], story_log.shape[0]) \n",
      "#story_log.head()\n",
      "43/1013:\n",
      "print(issues[['key', 'fields.issuetype.name']].drop_duplicates().shape[0],\n",
      "     issues[['key']].drop_duplicates().shape[0])\n",
      "\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "story_log = pd.merge(story_log, issues[['key', 'fields.issuetype.name']].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "       'project', 'to', 'toString', 'fields.issuetype.name']]\n",
      "\n",
      "story_log = story_log[story_log['fields.issuetype.name']!='Epic']\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "\n",
      "print(story_log[story_log['toString'].isin(story_points)].shape[0], story_log.shape[0]) \n",
      "story_log = story_log[story_log['toString'].isin(story_points)]\n",
      "#story_log.head()\n",
      "43/1014: story_log.shape[]\n",
      "43/1015: story_log.shape[0]\n",
      "43/1016:\n",
      "print(issues[['key', 'fields.issuetype.name']].drop_duplicates().shape[0],\n",
      "     issues[['key']].drop_duplicates().shape[0])\n",
      "\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "story_log = pd.merge(story_log, issues[['key', 'fields.issuetype.name']].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "       'project', 'to', 'toString', 'fields.issuetype.name']]\n",
      "\n",
      "story_log = story_log[story_log['fields.issuetype.name']!='Epic']\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "\n",
      "print(story_log[story_log['toString'].isin(story_points)].shape[0], story_log.shape[0]) \n",
      "story_log = story_log[story_log['toString'].isin(story_points)]\n",
      "story_log.head()\n",
      "43/1017:\n",
      "log_state['state'] = np.nan\n",
      "log_state.loc[(log_state.toString.isin(_todo)), 'state']='todo'\n",
      "log_state.loc[(log_state.toString.isin(_inprogress)), 'state']='inprogress'\n",
      "log_state.loc[(log_state.toString.isin(_done)), 'state']='done'\n",
      "43/1018:\n",
      "print(log_state.shape[0])\n",
      "log_state.head()\n",
      "43/1019:\n",
      "valid_users_states = pd.merge(user_personalities, log_state, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', 'state']]\n",
      "43/1020:\n",
      "print(valid_users_states.shape[0], \n",
      "      valid_users_states.drop_duplicates().shape[0],\n",
      "      valid_users_states[['user', 'key']].drop_duplicates().shape[0])\n",
      "valid_users_states.head()\n",
      "43/1021:\n",
      "e_high = ['0.5', '1', '2', '3']\n",
      "e_medium = ['5', '8', '13']\n",
      "e_low = ['20', '40', '100']\n",
      "\n",
      "story_log['points'] = np.nan\n",
      "story_log.loc[(story_log.toString.isin(e_high)), 'points']='high'\n",
      "story_log.loc[(story_log.toString.isin(e_medium)), 'points']='medium'\n",
      "story_log.loc[(story_log.toString.isin(e_low)), 'points']='low'\n",
      "43/1022: story_log.head()\n",
      "43/1023:\n",
      "e_low = ['0.5', '1', '2', '3']\n",
      "e_medium = ['5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "story_log['points'] = np.nan\n",
      "story_log.loc[(story_log.toString.isin(e_high)), 'points']='high'\n",
      "story_log.loc[(story_log.toString.isin(e_medium)), 'points']='medium'\n",
      "story_log.loc[(story_log.toString.isin(e_low)), 'points']='low'\n",
      "43/1024: story_log.head()\n",
      "43/1025:\n",
      "e_low = ['0.5', '1', '2', '3']\n",
      "e_medium = ['5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "story_log['points'] = np.nan\n",
      "story_log.loc[(story_log.toString.isin(e_high)), 'points']='high'\n",
      "story_log.loc[(story_log.toString.isin(e_medium)), 'points']='medium'\n",
      "story_log.loc[(story_log.toString.isin(e_low)), 'points']='low'\n",
      "story_log.head()\n",
      "43/1026:\n",
      "valid_users_estimation = pd.merge(user_personalities, story_log, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', 'points']]\n",
      "43/1027: valid_users_estimation.head()\n",
      "43/1028: print(valid_users_estimation.shape[0],valid_users_estimation[['user', 'key']].drop_duplicates().shape[0])\n",
      "43/1029:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_tasks, medium_tasks, high_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_estimation\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_users_estimation_agg = ps.sqldf(q1, locals())\n",
      "print(valid_users_estimation_agg.shape[0], \n",
      "      changelog[(changelog['field']=='priority') & (changelog['author']=='mark.pollack')]['key'].drop_duplicates().shape[0])\n",
      "valid_users_estimation_agg.head()\n",
      "43/1030:\n",
      "story_fields = ['Actual Story Points', 'Story Points']\n",
      "\n",
      "print(issues[['key', 'fields.issuetype.name']].drop_duplicates().shape[0],\n",
      "     issues[['key']].drop_duplicates().shape[0])\n",
      "\n",
      "story_log = changelog[changelog['field'].isin(story_fields)]\n",
      "story_log = pd.merge(story_log, issues[['key', 'fields.issuetype.name']].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "       'project', 'to', 'toString', 'fields.issuetype.name']]\n",
      "\n",
      "story_log = story_log[story_log['fields.issuetype.name']!='Epic']\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "\n",
      "print(story_log[story_log['toString'].isin(story_points)].shape[0], story_log.shape[0]) \n",
      "story_log = story_log[story_log['toString'].isin(story_points)]\n",
      "story_log.head()\n",
      "43/1031:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_tasks, medium_tasks, high_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_estimation\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_users_estimation_agg = ps.sqldf(q1, locals())\n",
      "print(valid_users_estimation_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='mark.pollack')]['key'].drop_duplicates().shape[0])\n",
      "valid_users_estimation_agg.head()\n",
      "43/1032:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_tasks, medium_tasks, high_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_estimation\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_users_estimation_agg = ps.sqldf(q1, locals())\n",
      "print(valid_users_estimation_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "valid_users_estimation_agg.head()\n",
      "43/1033:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_tasks, medium_tasks, high_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_estimation\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_users_estimation_agg = ps.sqldf(q1, locals())\n",
      "print(valid_users_estimation_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "valid_users_estimation_agg.head()\n",
      "43/1034:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_tasks, medium_tasks, high_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_estimation\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_users_estimation_agg = ps.sqldf(q1, locals())\n",
      "print(valid_users_estimation_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "valid_users_estimation_agg.head()\n",
      "43/1035:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_tasks, medium_tasks, high_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_estimation\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_users_estimation_agg = ps.sqldf(q1, locals())\n",
      "print(valid_users_estimation_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='dturanski')]['key'].drop_duplicates().shape[0])\n",
      "valid_users_estimation_agg.head()\n",
      "43/1036:\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_tasks, medium_tasks, high_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user\n",
      "    From valid_users_estimation\n",
      "    ) AS U\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS low_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'low'\n",
      "    Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS medium_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'medium'\n",
      "    Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join\n",
      "    (\n",
      "    SELECT user, count(*) AS high_tasks\n",
      "    FROM valid_users_estimation AS F\n",
      "    WHERE points = 'high'\n",
      "    Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_users_estimation_agg = ps.sqldf(q1, locals())\n",
      "print(valid_users_estimation_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "valid_users_estimation_agg.head()\n",
      "43/1037:\n",
      "print(valid_users_priorities_agg[(pd.isnull(valid_users_priorities_agg['high_tasks'])==False)\n",
      "                         & (pd.isnull(valid_users_priorities_agg['medium_tasks'])==False)\n",
      "                         & (pd.isnull(valid_users_priorities_agg['low_tasks'])==False)].shape[0])\n",
      "\n",
      "print(valid_users_priorities_agg[(pd.isnull(valid_users_priorities_agg['high_tasks'])==False)\n",
      "                         | (pd.isnull(valid_users_priorities_agg['medium_tasks'])==False)\n",
      "                         | (pd.isnull(valid_users_priorities_agg['low_tasks'])==False)].shape[0])\n",
      "43/1038:\n",
      "print(valid_users_estimation_agg[(pd.isnull(valid_users_estimation_agg['high_tasks'])==False)\n",
      "                         & (pd.isnull(valid_users_estimation_agg['medium_tasks'])==False)\n",
      "                         & (pd.isnull(valid_users_estimation_agg['low_tasks'])==False)].shape[0])\n",
      "\n",
      "print(valid_users_estimation_agg[(pd.isnull(valid_users_estimation_agg['high_tasks'])==False)\n",
      "                         | (pd.isnull(valid_users_estimation_agg['medium_tasks'])==False)\n",
      "                         | (pd.isnull(valid_users_estimation_agg['low_tasks'])==False)].shape[0])\n",
      "43/1039:\n",
      "txt = 'oooooo'\n",
      "k = \"\"\"asdasdas\"\"\"\n",
      "\n",
      "k\n",
      "43/1040:\n",
      "txt = 'oooooo'\n",
      "k = \"\"\"asdas\"\"\" +txt + \"\"\"das\"\"\"\n",
      "\n",
      "k\n",
      "43/1041:\n",
      "def categorical_metric(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    log_dt = changelog[(changelog['field']==field)]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(field, cat1_label, cat2_label, cat3_label)\n",
      "    valid_users_metrics = categorical_metric()\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\" FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1042:\n",
      "def categorical_metric(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    log_dt = changelog[(changelog['field']==field)]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = categorical_metric()\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\" FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1043:\n",
      "def categorical_metric(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    log_dt = changelog[(changelog['field']==field)]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = categorical_metric()\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\" FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1044:\n",
      "#(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label)\n",
      "\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "categorical_metric('status','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "43/1045:\n",
      "#(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label)\n",
      "\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "43/1046:\n",
      "def categorical_metric(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    log_dt = changelog[(changelog['field']==field)]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\" FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1047:\n",
      "#(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label)\n",
      "\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'todo', 'inprogress', 'done')\n",
      "43/1048:\n",
      "def categorical_metric(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    log_dt = changelog[(changelog['field']==field)]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\" FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1049:\n",
      "#(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label)\n",
      "\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'status','todo', 'inprogress', 'done')\n",
      "43/1050:\n",
      "def categorical_metric(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    log_dt = changelog[(changelog['field']==field)]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\" FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1051:\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'status','todo', 'inprogress', 'done')\n",
      "43/1052:\n",
      "print(log_state.shape[0])\n",
      "log_state.head()\n",
      "43/1053:\n",
      "print(states_df.shape[0])\n",
      "states_df.head()\n",
      "43/1054:\n",
      "print(states_df.shape[0], \n",
      "      states_df.drop_duplicates().shape[0],\n",
      "      states_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "states_df.head()\n",
      "43/1055:\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
      "                         & (pd.isnull(states_df_agg['todo'])==False)\n",
      "                         & (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
      "\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
      "                         | (pd.isnull(states_df_agg['todo'])==False)\n",
      "                         | (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
      "43/1056:\n",
      "statuses = changelog[changelog['field']=='status'][['toString', 'author']].groupby(\n",
      "    'toString').count().sort_values('author', ascending=False)\n",
      "statuses.reset_index(level= [0,1], inplace=True)\n",
      "43/1057:\n",
      "statuses = changelog[changelog['field']=='status'][['toString', 'author']].groupby(\n",
      "    'toString').count().sort_values('author', ascending=False)\n",
      "statuses.reset_index(level= [0], inplace=True)\n",
      "43/1058:\n",
      "statuses = changelog[changelog['field']=='status'][['toString', 'author']].groupby(\n",
      "    'toString').count().sort_values('author', ascending=False)\n",
      "statuses.reset_index(level= [0], inplace=True)\n",
      "statuses.head()\n",
      "43/1059:\n",
      "_high = ['High', 'Critical', 'Blocker']\n",
      "_medium = ['Medium', 'Major']\n",
      "_low = ['Low', 'Minor', 'None', 'Trivial', 'To be reviewed']\n",
      "\n",
      "priorities_df = categorical_metric('priority','','','',_high,'high',_medium,'medium',_low,'low')\n",
      "priorities_df_agg = categorical_metric_agg(states_df, 'priority','high', 'medium', 'low')\n",
      "43/1060:\n",
      "_high = ['High', 'Critical', 'Blocker']\n",
      "_medium = ['Medium', 'Major']\n",
      "_low = ['Low', 'Minor', 'None', 'Trivial', 'To be reviewed']\n",
      "\n",
      "priorities_df = categorical_metric('priority','','','',_high,'high',_medium,'medium',_low,'low')\n",
      "priorities_df.head()\n",
      "#priorities_df_agg = categorical_metric_agg(states_df, 'priority','high', 'medium', 'low')\n",
      "43/1061:\n",
      "_high = ['High', 'Critical', 'Blocker']\n",
      "_medium = ['Medium', 'Major']\n",
      "_low = ['Low', 'Minor', 'None', 'Trivial', 'To be reviewed']\n",
      "\n",
      "priorities_df = categorical_metric('priority','','','',_high,'high',_medium,'medium',_low,'low')\n",
      "priorities_df_agg = categorical_metric_agg(priorities_df, 'priority','high', 'medium', 'low')\n",
      "43/1062: print(priorities_df.shape[0],priorities_df.shape[0], priorities_df[['author', 'key']].drop_duplicates().shape[0])\n",
      "43/1063: print(log_priorities_actions.shape[0],log_priorities.shape[0], log_priorities[['author', 'key']].drop_duplicates().shape[0])\n",
      "43/1064:\n",
      "print(priorities_df_agg.shape[0], \n",
      "      changelog[(changelog['field']=='priority') & (changelog['author']=='mark.pollack')]['key'].drop_duplicates().shape[0])\n",
      "priorities_df_agg.head()\n",
      "43/1065:\n",
      "print(priorities_df_agg.shape[0], \n",
      "      changelog[(changelog['field']=='priority') & (changelog['author']=='mark.pollack')]['key'].drop_duplicates().shape[0])\n",
      "priorities_df_agg.head()\n",
      "43/1066:\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "43/1067:\n",
      "\n",
      "\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "story_fields = 'Story Points'\n",
      "e_low = ['0.5', '1', '2', '3']\n",
      "e_medium = ['5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "field_in_issues, field_exclusion_filter, values_filter\n",
      "\n",
      "estimates_df = categorical_metric(story_fields,'fields.issuetype.name','Epic',story_points,e_high,'high',e_medium,'medium',e_low,'low')\n",
      "estimates_df_agg = categorical_metric_agg(estimates_df, story_fields,'high', 'medium', 'low')\n",
      "43/1068:\n",
      "\n",
      "\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "story_fields = 'Story Points'\n",
      "e_low = ['0.5', '1', '2', '3']\n",
      "e_medium = ['5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "\n",
      "estimates_df = categorical_metric(story_fields,'fields.issuetype.name','Epic',story_points,e_high,'high',e_medium,'medium',e_low,'low')\n",
      "estimates_df_agg = categorical_metric_agg(estimates_df, story_fields,'high', 'medium', 'low')\n",
      "43/1069:\n",
      "def categorical_metric(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    log_dt = changelog[(changelog['field']==field)]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\" FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1070:\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'status','todo', 'inprogress', 'done')\n",
      "43/1071:\n",
      "print(states_df.shape[0], \n",
      "      states_df.drop_duplicates().shape[0],\n",
      "      states_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "states_df.head()\n",
      "43/1072:\n",
      "#field_in_issues, field_exclusion_filter, values_filter\n",
      "\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "story_fields = 'Story Points'\n",
      "e_low = ['0.5', '1', '2', '3']\n",
      "e_medium = ['5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "\n",
      "estimates_df = categorical_metric(story_fields,'fields.issuetype.name','Epic',story_points,e_high,'high',e_medium,'medium',e_low,'low')\n",
      "estimates_df_agg = categorical_metric_agg(estimates_df, story_fields,'high', 'medium', 'low')\n",
      "43/1073:\n",
      "def categorical_metric(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    log_dt = changelog[(changelog['field']==field)]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field_in_issues]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\" FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1074:\n",
      "statuses = changelog[changelog['field']=='status'][['toString', 'author']].groupby(\n",
      "    'toString').count().sort_values('author', ascending=False)\n",
      "statuses.reset_index(level= [0], inplace=True)\n",
      "statuses.head()\n",
      "43/1075:\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'status','todo', 'inprogress', 'done')\n",
      "43/1076:\n",
      "print(states_df.shape[0], \n",
      "      states_df.drop_duplicates().shape[0],\n",
      "      states_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "states_df.head()\n",
      "43/1077:\n",
      "#field_in_issues, field_exclusion_filter, values_filter\n",
      "\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "story_fields = 'Story Points'\n",
      "e_low = ['0.5', '1', '2', '3']\n",
      "e_medium = ['5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "\n",
      "estimates_df = categorical_metric(story_fields,'fields.issuetype.name','Epic',story_points,e_high,'high',e_medium,'medium',e_low,'low')\n",
      "estimates_df_agg = categorical_metric_agg(estimates_df, story_fields,'high', 'medium', 'low')\n",
      "43/1078:\n",
      "#field_in_issues, field_exclusion_filter, values_filter\n",
      "\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "story_fields = 'Story Points'\n",
      "e_low = ['0.5', '1', '2', '3']\n",
      "e_medium = ['5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "\n",
      "estimates_df = categorical_metric(story_fields,'fields.issuetype.name','Epic',story_points,e_high,'high',e_medium,'medium',e_low,'low')\n",
      "#estimates_df_agg = categorical_metric_agg(estimates_df, story_fields,'high', 'medium', 'low')\n",
      "estimates_df.head()\n",
      "43/1079:\n",
      "#field_in_issues, field_exclusion_filter, values_filter\n",
      "\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "story_fields = 'Story Points'\n",
      "e_low = ['0.5', '1', '2', '3']\n",
      "e_medium = ['5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "\n",
      "estimates_df = categorical_metric(story_fields,'fields.issuetype.name','Epic',story_points,e_high,'high',e_medium,'medium',e_low,'low')\n",
      "estimates_df_agg = categorical_metric_agg(estimates_df, '`'+story_fields+'`','high', 'medium', 'low')\n",
      "43/1080: estimates_df_agg.head()\n",
      "43/1081: estimates_df.head()\n",
      "43/1082: print(estimates_df.shape[0],estimates_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "43/1083:\n",
      "print(estimates_df_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "estimates_df_agg.head()\n",
      "43/1084:\n",
      "print(estimates_df_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin([story_fields])) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "estimates_df_agg.head()\n",
      "43/1085:\n",
      "print(estimates_df_agg[(pd.isnull(estimates_df_agg['highs'])==False)\n",
      "                         & (pd.isnull(estimates_df_agg['medium'])==False)\n",
      "                         & (pd.isnull(estimates_df_agg['low'])==False)].shape[0])\n",
      "\n",
      "print(estimates_df_agg[(pd.isnull(estimates_df_agg['high'])==False)\n",
      "                         | (pd.isnull(estimates_df_agg['medium'])==False)\n",
      "                         | (pd.isnull(estimates_df_agg['low'])==False)].shape[0])\n",
      "43/1086:\n",
      "print(estimates_df_agg[(pd.isnull(estimates_df_agg['high'])==False)\n",
      "                         & (pd.isnull(estimates_df_agg['medium'])==False)\n",
      "                         & (pd.isnull(estimates_df_agg['low'])==False)].shape[0])\n",
      "\n",
      "print(estimates_df_agg[(pd.isnull(estimates_df_agg['high'])==False)\n",
      "                         | (pd.isnull(estimates_df_agg['medium'])==False)\n",
      "                         | (pd.isnull(estimates_df_agg['low'])==False)].shape[0])\n",
      "43/1087:\n",
      "def categorical_metric(field, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    log_dt = changelog[(changelog['field'].isin([field]))]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field_in_issues]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\" FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1088:\n",
      "statuses = changelog[changelog['field']=='status'][['toString', 'author']].groupby(\n",
      "    'toString').count().sort_values('author', ascending=False)\n",
      "statuses.reset_index(level= [0], inplace=True)\n",
      "statuses.head()\n",
      "43/1089:\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'status','todo', 'inprogress', 'done')\n",
      "43/1090:\n",
      "print(states_df.shape[0], \n",
      "      states_df.drop_duplicates().shape[0],\n",
      "      states_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "states_df.head()\n",
      "43/1091:\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
      "                         & (pd.isnull(states_df_agg['todo'])==False)\n",
      "                         & (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
      "\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
      "                         | (pd.isnull(states_df_agg['todo'])==False)\n",
      "                         | (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
      "states_df_agg.head()\n",
      "43/1092:\n",
      "_high = ['High', 'Critical', 'Blocker']\n",
      "_medium = ['Medium', 'Major']\n",
      "_low = ['Low', 'Minor', 'None', 'Trivial', 'To be reviewed']\n",
      "\n",
      "priorities_df = categorical_metric('priority','','','',_high,'high',_medium,'medium',_low,'low')\n",
      "priorities_df_agg = categorical_metric_agg(priorities_df, 'priority','high', 'medium', 'low')\n",
      "43/1093:\n",
      "print(priorities_df.shape[0], \n",
      "      changelog[(changelog['field']=='priority') & (changelog['author']=='mark.pollack')]['key'].drop_duplicates().shape[0])\n",
      "priorities_df.head()\n",
      "43/1094:\n",
      "print(priorities_df.shape[0], \n",
      "      priorities_df.drop_duplicates().shape[0],\n",
      "      priorities_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "priorities_df.head()\n",
      "43/1095:\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "priorities_df_agg.head()\n",
      "43/1096: print(priorities_df.shape[0],priorities_df.shape[0], priorities_df[['author', 'key']].drop_duplicates().shape[0])\n",
      "43/1097:\n",
      "print(priorities_df.shape[0], \n",
      "      priorities_df.drop_duplicates().shape[0],\n",
      "      priorities_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "priorities_df.head()\n",
      "43/1098:\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "priorities_df_agg.head()\n",
      "43/1099:\n",
      "def categorical_metric(field, multiple_fields, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    if len(multiple_fields)>0:\n",
      "        log_dt = changelog[(changelog['field'].isin(multiple_fields))]\n",
      "    else:\n",
      "        log_dt = changelog[(changelog['field'].isin([field]))]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field_in_issues]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\" FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1100:\n",
      "def categorical_metric(field, multiple_fields, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    if len(multiple_fields)>0:\n",
      "        log_dt = changelog[(changelog['field'].isin(multiple_fields))]\n",
      "    else:\n",
      "        log_dt = changelog[(changelog['field'].isin([field]))]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field_in_issues]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\" FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1101:\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'status','todo', 'inprogress', 'done')\n",
      "43/1102:\n",
      "print(states_df.shape[0], \n",
      "      states_df.drop_duplicates().shape[0],\n",
      "      states_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "states_df.head()\n",
      "43/1103:\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
      "                         & (pd.isnull(states_df_agg['todo'])==False)\n",
      "                         & (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
      "\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
      "                         | (pd.isnull(states_df_agg['todo'])==False)\n",
      "                         | (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
      "states_df_agg.head()\n",
      "43/1104:\n",
      "_high = ['High', 'Critical', 'Blocker']\n",
      "_medium = ['Medium', 'Major']\n",
      "_low = ['Low', 'Minor', 'None', 'Trivial', 'To be reviewed']\n",
      "\n",
      "priorities_df = categorical_metric('priority','','','','',_high,'high',_medium,'medium',_low,'low')\n",
      "priorities_df_agg = categorical_metric_agg(priorities_df, 'priority','high', 'medium', 'low')\n",
      "43/1105:\n",
      "print(priorities_df.shape[0], \n",
      "      priorities_df.drop_duplicates().shape[0],\n",
      "      priorities_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "priorities_df.head()\n",
      "43/1106:\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "priorities_df_agg.head()\n",
      "43/1107:\n",
      "#field_in_issues, field_exclusion_filter, values_filter\n",
      "\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "story_fields = ['Story Points', 'Actual Story Points']\n",
      "field = 'StoryPoints'\n",
      "e_low = ['0.5', '1', '2', '3']\n",
      "e_medium = ['5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "\n",
      "estimates_df = categorical_metric(field,story_fields,'fields.issuetype.name','Epic',story_points,e_high,'high',e_medium,'medium',e_low,'low')\n",
      "estimates_df_agg = categorical_metric_agg(estimates_df, '`'+field+'`','high', 'medium', 'low')\n",
      "43/1108: estimates_df.head()\n",
      "43/1109: print(estimates_df.shape[0],estimates_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "43/1110: print(estimates_df.shape[0],estimates_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "43/1111:\n",
      "print(estimates_df_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin([story_fields])) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "estimates_df_agg.head()\n",
      "43/1112:\n",
      "print(estimates_df_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "estimates_df_agg.head()\n",
      "43/1113:\n",
      "print(estimates_df_agg[(pd.isnull(estimates_df_agg['high'])==False)\n",
      "                         & (pd.isnull(estimates_df_agg['medium'])==False)\n",
      "                         & (pd.isnull(estimates_df_agg['low'])==False)].shape[0])\n",
      "\n",
      "print(estimates_df_agg[(pd.isnull(estimates_df_agg['high'])==False)\n",
      "                         | (pd.isnull(estimates_df_agg['medium'])==False)\n",
      "                         | (pd.isnull(estimates_df_agg['low'])==False)].shape[0])\n",
      "43/1114:\n",
      "import pandasql as ps\n",
      "\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user From valid_users_times) AS U\n",
      "    Left Join ( SELECT user, count(*) AS low_timespent_tasks\n",
      "    FROM valid_users_times AS F WHERE time_spending_category = 'low' Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join (SELECT user, count(*) AS medium_timespent_tasks\n",
      "    FROM valid_users_times AS F WHERE time_spending_category = 'medium' Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join ( SELECT user, count(*) AS high_timespent_tasks\n",
      "    FROM valid_users_times AS F WHERE time_spending_category = 'high' Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_time_agg = ps.sqldf(q1, locals())\n",
      "print(valid_user_time_agg.shape[0])\n",
      "valid_user_time_agg.head()\n",
      "43/1115: user_personalities.head()\n",
      "43/1116: user_personalities.openness.hist()\n",
      "43/1117: estimates_df_agg.head()\n",
      "43/1118: estimates_df_agg.shape[0]\n",
      "43/1119: estimates_df_agg.head()\n",
      "43/1120:\n",
      "#user_personalities['opennes_bool']=\n",
      "user_personalities['openness'].apply(lambda x: Yes if x>mean(user_personalities['openness']))\n",
      "43/1121:\n",
      "#user_personalities['opennes_bool']=\n",
      "user_personalities['openness'].apply(lambda x: Yes if x>mean(user_personalities['openness'] else No))\n",
      "43/1122:\n",
      "#user_personalities['opennes_bool']=\n",
      "user_personalities['openness'].apply(lambda x: Yes if x>mean(user_personalities['openness'] else: No))\n",
      "43/1123:\n",
      "#user_personalities['opennes_bool']=\n",
      "user_personalities['openness'].apply(lambda x: Yes if x>mean(user_personalities['openness']) else No)\n",
      "\n",
      "#lambda x: True if x % 2 == 0 else False\n",
      "43/1124:\n",
      "#user_personalities['opennes_bool']=\n",
      "user_personalities['openness'].apply(lambda x: Yes if x>np.mean(user_personalities['openness']) else No)\n",
      "\n",
      "#lambda x: True if x % 2 == 0 else False\n",
      "43/1125:\n",
      "#user_personalities['opennes_bool']=\n",
      "user_personalities['openness'].apply(lambda x: 'Yes' if x>np.mean(user_personalities['openness']) else 'No')\n",
      "\n",
      "#lambda x: True if x % 2 == 0 else False\n",
      "43/1126:\n",
      "user_personalities['opennes_bool']=user_personalities['openness'].apply(lambda x: 'Yes' if x>np.mean(user_personalities['openness']) else 'No')\n",
      "\n",
      "#lambda x: True if x % 2 == 0 else False\n",
      "43/1127: user_personalities.head()\n",
      "43/1128:\n",
      "user_personalities['opennes_bool']=user_personalities['openness'].apply(lambda x: 'Yes' if x>np.median(user_personalities['openness']) else 'No')\n",
      "\n",
      "#lambda x: True if x % 2 == 0 else False\n",
      "43/1129:\n",
      "user_personalities['opennes_bool']=user_personalities['openness'].apply(lambda x: 'Yes' if x>np.median(user_personalities['openness']) else 'No')\n",
      "user_personalities['conscientiousness']=user_personalities['conscientiousness'].apply(lambda x: 'Yes' if x>np.median(user_personalities['conscientiousness']) else 'No')\n",
      "user_personalities['extraversion']=user_personalities['extraversion'].apply(lambda x: 'Yes' if x>np.median(user_personalities['extraversion']) else 'No')\n",
      "user_personalities['agreeableness']=user_personalities['agreeableness'].apply(lambda x: 'Yes' if x>np.median(user_personalities['agreeableness']) else 'No')\n",
      "user_personalities['neuroticism']=user_personalities['neuroticism'].apply(lambda x: 'Yes' if x>np.median(user_personalities['neuroticism']) else 'No')\n",
      "\n",
      "#lambda x: True if x % 2 == 0 else False\n",
      "43/1130: user_personalities.head()\n",
      "43/1131: user_personalities=pd.read_csv('user_personalities.csv')\n",
      "43/1132: user_personalities.head()\n",
      "43/1133:\n",
      "user_personalities['opennes_bool']=user_personalities['openness'].apply(lambda x: 'Yes' if x>np.median(user_personalities['openness']) else 'No')\n",
      "user_personalities['conscientiousness_bool']=user_personalities['conscientiousness'].apply(lambda x: 'Yes' if x>np.median(user_personalities['conscientiousness']) else 'No')\n",
      "user_personalities['extraversion_bool']=user_personalities['extraversion'].apply(lambda x: 'Yes' if x>np.median(user_personalities['extraversion']) else 'No')\n",
      "user_personalities['agreeableness_bool']=user_personalities['agreeableness'].apply(lambda x: 'Yes' if x>np.median(user_personalities['agreeableness']) else 'No')\n",
      "user_personalities['neuroticism_bool']=user_personalities['neuroticism'].apply(lambda x: 'Yes' if x>np.median(user_personalities['neuroticism']) else 'No')\n",
      "\n",
      "#lambda x: True if x % 2 == 0 else False\n",
      "43/1134: user_personalities.head()\n",
      "43/1135:\n",
      "user_personalities['opennes_bool']           =user_personalities['openness'].apply(lambda x: 'Yes' if x>np.median(user_personalities['openness']) else 'No')\n",
      "user_personalities['conscientiousness_bool'] =user_personalities['conscientiousness'].apply(lambda x: 'Yes' if x>np.median(user_personalities['conscientiousness']) else 'No')\n",
      "user_personalities['extraversion_bool']      =user_personalities['extraversion'].apply(lambda x: 'Yes' if x>np.median(user_personalities['extraversion']) else 'No')\n",
      "user_personalities['agreeableness_bool']     =user_personalities['agreeableness'].apply(lambda x: 'Yes' if x>np.median(user_personalities['agreeableness']) else 'No')\n",
      "user_personalities['neuroticism_bool']       =user_personalities['neuroticism'].apply(lambda x: 'Yes' if x>np.median(user_personalities['neuroticism']) else 'No')\n",
      "43/1136:\n",
      "user_personalities['opennes_bool']           =user_personalities['openness'].apply(          lambda x: 'Yes' if x>np.median(user_personalities['openness']) else 'No')\n",
      "user_personalities['conscientiousness_bool'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes' if x>np.median(user_personalities['conscientiousness']) else 'No')\n",
      "user_personalities['extraversion_bool']      =user_personalities['extraversion'].apply(      lambda x: 'Yes' if x>np.median(user_personalities['extraversion']) else 'No')\n",
      "user_personalities['agreeableness_bool']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes' if x>np.median(user_personalities['agreeableness']) else 'No')\n",
      "user_personalities['neuroticism_bool']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes' if x>np.median(user_personalities['neuroticism']) else 'No')\n",
      "43/1137:\n",
      "user_personalities['opennes_bool']           =user_personalities['openness'].apply(          lambda x: 'Yes' if x>np.median(user_personalities['openness'])          else 'No')\n",
      "user_personalities['conscientiousness_bool'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes' if x>np.median(user_personalities['conscientiousness']) else 'No')\n",
      "user_personalities['extraversion_bool']      =user_personalities['extraversion'].apply(      lambda x: 'Yes' if x>np.median(user_personalities['extraversion'])      else 'No')\n",
      "user_personalities['agreeableness_bool']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes' if x>np.median(user_personalities['agreeableness'])     else 'No')\n",
      "user_personalities['neuroticism_bool']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes' if x>np.median(user_personalities['neuroticism'])       else 'No')\n",
      "43/1138: user_personalities.head()\n",
      "43/1139:\n",
      "def categorical_metric(field, multiple_fields, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    if len(multiple_fields)>0:\n",
      "        log_dt = changelog[(changelog['field'].isin(multiple_fields))]\n",
      "    else:\n",
      "        log_dt = changelog[(changelog['field'].isin([field]))]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field_in_issues]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\", \n",
      "    CASE WHEN COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) THEN \"\"\"+cat1_label+\"\"\"\n",
      "         WHEN COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) AND COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN \"\"\"+cat2_label+\"\"\"\n",
      "         WHEN COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN \"\"\"+cat3_label+\"\"\"\n",
      "    END AS metric\n",
      "    FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1140:\n",
      "def categorical_metric(field, multiple_fields, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    if len(multiple_fields)>0:\n",
      "        log_dt = changelog[(changelog['field'].isin(multiple_fields))]\n",
      "    else:\n",
      "        log_dt = changelog[(changelog['field'].isin([field]))]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field_in_issues]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\", \n",
      "    CASE WHEN COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) THEN \"\"\"+cat1_label+\"\"\"\n",
      "         WHEN COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) AND COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN \"\"\"+cat2_label+\"\"\"\n",
      "         WHEN COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN \"\"\"+cat3_label+\"\"\"\n",
      "    END AS metric\n",
      "    FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1141:\n",
      "statuses = changelog[changelog['field']=='status'][['toString', 'author']].groupby(\n",
      "    'toString').count().sort_values('author', ascending=False)\n",
      "statuses.reset_index(level= [0], inplace=True)\n",
      "statuses.head()\n",
      "43/1142:\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'status','todo', 'inprogress', 'done')\n",
      "43/1143:\n",
      "print(states_df.shape[0], \n",
      "      states_df.drop_duplicates().shape[0],\n",
      "      states_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "states_df.head()\n",
      "43/1144:\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
      "                         & (pd.isnull(states_df_agg['todo'])==False)\n",
      "                         & (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
      "\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
      "                         | (pd.isnull(states_df_agg['todo'])==False)\n",
      "                         | (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
      "states_df_agg.head()\n",
      "43/1145:\n",
      "def categorical_metric(field, multiple_fields, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    if len(multiple_fields)>0:\n",
      "        log_dt = changelog[(changelog['field'].isin(multiple_fields))]\n",
      "    else:\n",
      "        log_dt = changelog[(changelog['field'].isin([field]))]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field_in_issues]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\", \n",
      "    CASE WHEN COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) THEN '\"\"\"+cat1_label+\"\"\"'\n",
      "         WHEN COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) AND COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN '\"\"\"+cat2_label+\"\"\"'\n",
      "         WHEN COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN '\"\"\"+cat3_label+\"\"\"'\n",
      "    END AS metric\n",
      "    FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1146:\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'status','todo', 'inprogress', 'done')\n",
      "43/1147:\n",
      "print(states_df.shape[0], \n",
      "      states_df.drop_duplicates().shape[0],\n",
      "      states_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "states_df.head()\n",
      "43/1148:\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
      "                         & (pd.isnull(states_df_agg['todo'])==False)\n",
      "                         & (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
      "\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
      "                         | (pd.isnull(states_df_agg['todo'])==False)\n",
      "                         | (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
      "states_df_agg.head()\n",
      "43/1149:\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "priorities_df_agg.head()\n",
      "43/1150:\n",
      "_high = ['High', 'Critical', 'Blocker']\n",
      "_medium = ['Medium', 'Major']\n",
      "_low = ['Low', 'Minor', 'None', 'Trivial', 'To be reviewed']\n",
      "\n",
      "priorities_df = categorical_metric('priority','','','','',_high,'high',_medium,'medium',_low,'low')\n",
      "priorities_df_agg = categorical_metric_agg(priorities_df, 'priority','high', 'medium', 'low')\n",
      "43/1151:\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "priorities_df_agg.head()\n",
      "43/1152:\n",
      "#field_in_issues, field_exclusion_filter, values_filter\n",
      "\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "story_fields = ['Story Points', 'Actual Story Points']\n",
      "field = 'StoryPoints'\n",
      "e_low = ['0.5', '1', '2', '3']\n",
      "e_medium = ['5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "\n",
      "estimates_df = categorical_metric(field,story_fields,'fields.issuetype.name','Epic',story_points,e_high,'high',e_medium,'medium',e_low,'low')\n",
      "estimates_df_agg = categorical_metric_agg(estimates_df, '`'+field+'`','high', 'medium', 'low')\n",
      "43/1153: print(estimates_df.shape[0],estimates_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "43/1154:\n",
      "print(estimates_df_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "estimates_df_agg.head()\n",
      "43/1155:\n",
      "print(estimates_df_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "estimates_df_agg.head(100)\n",
      "43/1156:\n",
      "#field_in_issues, field_exclusion_filter, values_filter\n",
      "\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "story_fields = ['Story Points', 'Actual Story Points']\n",
      "field = 'StoryPoints'\n",
      "e_low = ['0.5', '1', '2']\n",
      "e_medium = ['3', '5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "\n",
      "estimates_df = categorical_metric(field,story_fields,'fields.issuetype.name','Epic',story_points,e_high,'high',e_medium,'medium',e_low,'low')\n",
      "estimates_df_agg = categorical_metric_agg(estimates_df, '`'+field+'`','high', 'medium', 'low')\n",
      "43/1157: estimates_df.head()\n",
      "43/1158: print(estimates_df.shape[0],estimates_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "43/1159:\n",
      "print(estimates_df_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "estimates_df_agg.head(100)\n",
      "43/1160: estimates_df.head()\n",
      "43/1161:\n",
      "user_personalities['Is_Open']           =user_personalities['openness'].apply(          lambda x: 'Yes' if x>np.median(user_personalities['openness'])          else 'No')\n",
      "user_personalities['Is_conscientious'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes' if x>np.median(user_personalities['conscientiousness']) else 'No')\n",
      "user_personalities['Is_extravert']      =user_personalities['extraversion'].apply(      lambda x: 'Yes' if x>np.median(user_personalities['extraversion'])      else 'No')\n",
      "user_personalities['Is_agreeable']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes' if x>np.median(user_personalities['agreeableness'])     else 'No')\n",
      "user_personalities['Is_neurotic']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes' if x>np.median(user_personalities['neuroticism'])       else 'No')\n",
      "43/1162: user_personalities.head()\n",
      "43/1163:\n",
      "pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')\n",
      "         [['user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1164:\n",
      "pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1165:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1166:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "estimates_assoc_df.head()\n",
      "43/1167: from apyori import apriori\n",
      "43/1168: from apyori import apriori\n",
      "43/1169:\n",
      "rules = apriori(estimates_assoc_df, min_support = 0.1, min_confidence = 0.8)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1170: rules_result\n",
      "43/1171:\n",
      "rules = apriori(estimates_assoc_df[['StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] , min_support = 0.1, min_confidence = 0.8)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1172: rules_result\n",
      "48/1:\n",
      "import pandas as pd\n",
      "\n",
      "titanic_df = pd.read_csv(\"titanic.csv\")\n",
      "titanic = titanic_df.values # Convert dataFrame into Array\n",
      "print(titanic_df.head(5))\n",
      "print('Class')\n",
      "print(titanic_df.Class.value_counts())\n",
      "print('Sex')\n",
      "print(titanic_df.Sex.value_counts())\n",
      "print('Age')\n",
      "print(titanic_df.Age.value_counts())\n",
      "print('Survived')\n",
      "print(titanic_df.Survived.value_counts())\n",
      "48/2:\n",
      "import pandas as pd\n",
      "\n",
      "titanic_df = pd.read_csv(\"titanic.csv\")\n",
      "titanic = titanic_df.values # Convert dataFrame into Array\n",
      "print(titanic_df.head(5))\n",
      "print('Class')\n",
      "print(titanic_df.Class.value_counts())\n",
      "print('Sex')\n",
      "print(titanic_df.Sex.value_counts())\n",
      "print('Age')\n",
      "print(titanic_df.Age.value_counts())\n",
      "print('Survived')\n",
      "print(titanic_df.Survived.value_counts())\n",
      "titanic_df.head()\n",
      "43/1173:\n",
      "rules = apriori(estimates_assoc_df[['StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "                , min_support = 0.5, min_confidence = 0.8)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1174: rules_result\n",
      "43/1175: estimates_assoc = estimates_assoc_df.values\n",
      "43/1176: estimates_assoc\n",
      "43/1177:\n",
      "estimates_assoc = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']].values\n",
      "43/1178: estimates_assoc\n",
      "43/1179:\n",
      "rules = apriori(estimates_assoc, min_support = 0.5, min_confidence = 0.8)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1180: rules_result\n",
      "43/1181:\n",
      "user_personalities['Is_Open']           =user_personalities['openness'].apply(          lambda x: 'Yes_openness' if x>np.median(user_personalities['openness'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes_conscientiousness' if x>np.median(user_personalities['conscientiousness']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['extraversion'].apply(      lambda x: 'Yes_extraversion' if x>np.median(user_personalities['extraversion'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes_agreeableness' if x>np.median(user_personalities['agreeableness'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes_neuroticism' if x>np.median(user_personalities['neuroticism'])       else 'No_neuroticism')\n",
      "43/1182:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "estimates_assoc_df.head()\n",
      "43/1183:\n",
      "estimates_assoc = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']].values\n",
      "43/1184: estimates_assoc\n",
      "43/1185:\n",
      "rules = apriori(estimates_assoc, min_support = 0.5, min_confidence = 0.5)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1186: rules_result\n",
      "43/1187:\n",
      "rules = apriori(estimates_assoc, min_support = 0.1, min_confidence = 0.5)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1188: rules_result\n",
      "43/1189:\n",
      "rules = apriori(estimates_assoc, min_support = 0.3, min_confidence = 0.5)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1190: rules_result\n",
      "43/1191:\n",
      "estimates_assoc_openness = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_Open']].values\n",
      "estimates_assoc_conscientiousness = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_conscientious']].values\n",
      "estimates_assoc_extraversion = estimates_assoc_df[[\n",
      "    'StoryPoints', 'Is_extravert']].values\n",
      "estimates_assoc_agreeableness = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_agreeable']].values\n",
      "estimates_assoc_neuroticism = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_neurotic']].values\n",
      "43/1192: estimates_assoc_openness\n",
      "43/1193:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.3, min_confidence = 0.5)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1194: rules_result\n",
      "43/1195:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.1, min_confidence = 0.5)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1196: rules_result\n",
      "43/1197:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.1, min_confidence = 0.8)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1198:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.01, min_confidence = 0.8)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1199:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.0, min_confidence = 0.8)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1200:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.00001, min_confidence = 0.8)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1201: rules_result\n",
      "43/1202:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.00001, min_confidence = 0.6)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1203: rules_result\n",
      "43/1204: rules_result\n",
      "43/1205:\n",
      "rules = apriori(estimates_assoc_openness, \n",
      "                #min_support = 0.00001, \n",
      "                min_confidence = 0.6)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1206: rules_result\n",
      "43/1207:\n",
      "rules = apriori(estimates_assoc_openness, \n",
      "                #min_support = 0.00001, \n",
      "                min_confidence = 0.006)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1208: rules_result\n",
      "43/1209:\n",
      "rules = apriori(estimates_assoc_openness, \n",
      "                #min_support = 0.00001, \n",
      "                min_confidence = 0.000000000006)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1210:\n",
      "rules = apriori(estimates_assoc_openness\n",
      "                #, min_support = 0.00001\n",
      "                #, min_confidence = 0.6\n",
      "               )\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1211:\n",
      "rules = apriori(estimates_assoc_openness\n",
      "                #, min_support = 0.00001\n",
      "                #, min_confidence = 0.6\n",
      "               )\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1212: rules_result\n",
      "43/1213:\n",
      "estimates_assoc_df[[\n",
      "    'StoryPoints','Is_Open'].drop_suplicates()\n",
      "43/1214:\n",
      "estimates_assoc_df[[\n",
      "    'StoryPoints','Is_Open'].drop_duplicates()\n",
      "43/1215: estimates_assoc_df[['StoryPoints','Is_Open'].drop_duplicates()\n",
      "43/1216: estimates_assoc_df[['StoryPoints','Is_Open']].drop_duplicates()\n",
      "43/1217:\n",
      "from apyori import apriori\n",
      "from orangecontrib.associate import *\n",
      "43/1218:\n",
      "from apyori import apriori\n",
      "from orangecontrib.associate import *\n",
      "43/1219:\n",
      "from apyori import apriori\n",
      "from orangecontrib.associate import *\n",
      "43/1220:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "print(len(list(freq_itemsets)))\n",
      "43/1221: freq_itemsets\n",
      "43/1222: list(freq_itemsets()\n",
      "43/1223: list(freq_itemsets\n",
      "43/1224: list(freq_itemsets)\n",
      "43/1225:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "print(len(list(freq_itemsets)))\n",
      "43/1226: len(list(freq_itemsets))\n",
      "43/1227:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "print(len(list(freq_itemsets)))\n",
      "43/1228: len(list(freq_itemsets))\n",
      "43/1229:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "print(len(list(freq_itemsets)))\n",
      "43/1230: len(list(freq_itemsets))\n",
      "43/1231: print(len(list(freq_itemsets)))\n",
      "43/1232:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "print(len(list(freq_itemsets)))\n",
      "43/1233:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "print(len(list(freq_itemsets)))\n",
      "print(len(list(freq_itemsets)))\n",
      "43/1234:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "print(len(list(freq_itemsets)))\n",
      "print(len(list(freq_itemsets)))\n",
      "43/1235:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "len(list(freq_itemsets))\n",
      "print(len(list(freq_itemsets)))\n",
      "43/1236:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "len(list(freq_itemsets))\n",
      "43/1237:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1238:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=0.1)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1239:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1240:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1241:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1242:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1243:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1244:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1245:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1246: estimates_assoc_openness\n",
      "43/1247:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1248:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness)\n",
      "# TODO \n",
      "len(list(freq_itemsets))\n",
      "43/1249:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "len(list(freq_itemsets))\n",
      "43/1250:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=3)\n",
      "# TODO \n",
      "len(list(freq_itemsets))\n",
      "43/1251:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=300)\n",
      "# TODO \n",
      "len(list(freq_itemsets))\n",
      "43/1252:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=3000)\n",
      "# TODO \n",
      "len(list(freq_itemsets))\n",
      "43/1253:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=30000)\n",
      "# TODO \n",
      "len(list(freq_itemsets))\n",
      "43/1254:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=6000)\n",
      "# TODO \n",
      "len(list(freq_itemsets))\n",
      "43/1255:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=7000)\n",
      "# TODO \n",
      "len(list(freq_itemsets))\n",
      "43/1256:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=6000)\n",
      "# TODO \n",
      "len(list(freq_itemsets))\n",
      "43/1257:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=6000)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "48/3:\n",
      "transactions = [\n",
      "  ['B','D','F','H'],\n",
      "  ['C','D','F','G'],\n",
      "  ['A','D','F','G'],\n",
      "  ['A','B','C','D','H'],\n",
      "  ['A','C','F','G'],\n",
      "  ['D','H'],\n",
      "  ['A','B','E','F'],\n",
      "  ['A','D','F','G','H'],\n",
      "  ['A','C','D','F','G'],\n",
      "  ['D','F','G','H'],\n",
      "  ['A','C','D','E'],\n",
      "  ['B','E','F','H'],\n",
      "  ['D','F','G'],\n",
      "  ['C','F','G','H'],\n",
      "  ['A','C','D','F','H']\n",
      "]\n",
      "transactions\n",
      "48/4:\n",
      "transactions = [\n",
      "  ['B','D','F','H'],\n",
      "  ['C','D','F','G'],\n",
      "  ['A','D','F','G'],\n",
      "  ['A','B','C','D','H'],\n",
      "  ['A','C','F','G'],\n",
      "  ['D','H'],\n",
      "  ['A','B','E','F'],\n",
      "  ['A','D','F','G','H'],\n",
      "  ['A','C','D','F','G'],\n",
      "  ['D','F','G','H'],\n",
      "  ['A','C','D','E'],\n",
      "  ['B','E','F','H'],\n",
      "  ['D','F','G'],\n",
      "  ['C','F','G','H'],\n",
      "  ['A','C','D','F','H']\n",
      "]\n",
      "transactions.dtype\n",
      "48/5:\n",
      "transactions = [\n",
      "  ['B','D','F','H'],\n",
      "  ['C','D','F','G'],\n",
      "  ['A','D','F','G'],\n",
      "  ['A','B','C','D','H'],\n",
      "  ['A','C','F','G'],\n",
      "  ['D','H'],\n",
      "  ['A','B','E','F'],\n",
      "  ['A','D','F','G','H'],\n",
      "  ['A','C','D','F','G'],\n",
      "  ['D','F','G','H'],\n",
      "  ['A','C','D','E'],\n",
      "  ['B','E','F','H'],\n",
      "  ['D','F','G'],\n",
      "  ['C','F','G','H'],\n",
      "  ['A','C','D','F','H']\n",
      "]\n",
      "transactions.dtype()\n",
      "48/6:\n",
      "transactions = [\n",
      "  ['B','D','F','H'],\n",
      "  ['C','D','F','G'],\n",
      "  ['A','D','F','G'],\n",
      "  ['A','B','C','D','H'],\n",
      "  ['A','C','F','G'],\n",
      "  ['D','H'],\n",
      "  ['A','B','E','F'],\n",
      "  ['A','D','F','G','H'],\n",
      "  ['A','C','D','F','G'],\n",
      "  ['D','F','G','H'],\n",
      "  ['A','C','D','E'],\n",
      "  ['B','E','F','H'],\n",
      "  ['D','F','G'],\n",
      "  ['C','F','G','H'],\n",
      "  ['A','C','D','F','H']\n",
      "]\n",
      "transactions\n",
      "43/1258:\n",
      "estimates_assoc_openness = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_Open']].values.tolist()\n",
      "estimates_assoc_conscientiousness = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_conscientious']].values\n",
      "estimates_assoc_extraversion = estimates_assoc_df[[\n",
      "    'StoryPoints', 'Is_extravert']].values\n",
      "estimates_assoc_agreeableness = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_agreeable']].values\n",
      "estimates_assoc_neuroticism = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_neurotic']].values\n",
      "43/1259: estimates_assoc_openness\n",
      "43/1260:\n",
      "rules = apriori(estimates_assoc_openness\n",
      "                #, min_support = 0.00001\n",
      "                #, min_confidence = 0.6\n",
      "               )\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "43/1261:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=6000)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1262:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness6000)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1263:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1264:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, , min_support=1)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1265:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1266:\n",
      "estimates_assoc_openness = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_Open']].values.tolist()\n",
      "estimates_assoc_conscientiousness = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_conscientious']].values.tolist()\n",
      "estimates_assoc_extraversion = estimates_assoc_df[[\n",
      "    'StoryPoints', 'Is_extravert']].values.tolist()\n",
      "estimates_assoc_agreeableness = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_agreeable']].values.tolist()\n",
      "estimates_assoc_neuroticism = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_neurotic']].values.tolist()\n",
      "43/1267:\n",
      "from apyori import apriori\n",
      "from orangecontrib.associate import *\n",
      "43/1268:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1269:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1, min_conf=0.1)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1270:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1, min_confidence=0.1)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1271:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1272:\n",
      "rules = apriori(estimates_assoc_openness\n",
      "                #, min_support = 0.00001\n",
      "                #, min_confidence = 0.6\n",
      "               )\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "#len(rules_result)rules_result\n",
      "43/1273:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1274:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1275:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=0.1)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1276:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "list(freq_itemsets)\n",
      "43/1277: association_rules(freq_itemsets)\n",
      "43/1278: association_rules(freq_itemsets, 0.1)\n",
      "43/1279: list(association_rules(freq_itemsets, 0.1))\n",
      "43/1280: list(association_rules(dict(freq_itemsets), 0.1))\n",
      "43/1281: list(association_rules(dict(freq_itemsets), 0.01))\n",
      "43/1282: list(association_rules(freq_itemsets, 0.01))\n",
      "43/1283: list(association_rules(freq_itemsets, 0.5))\n",
      "43/1284: list(association_rules(freq_itemsets, 0.3))\n",
      "43/1285: list(association_rules(list(freq_itemsets), 0.3))\n",
      "43/1286: list(association_rules(estimates_assoc_openness, 0.3))\n",
      "43/1287: list(association_rules(estimates_assoc_df[['StoryPoints','Is_Open']].values, 0.1))\n",
      "43/1288: list(association_rules(estimates_assoc_openness, 0.1))\n",
      "43/1289: list(association_rules(estimates_assoc_openness.tolist(), 0.1))\n",
      "43/1290: list(association_rules(estimates_assoc_openness, 0.1))\n",
      "43/1291:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "itemsets = dict(frequent_itemsets(estimates_assoc_openness, .05))\n",
      "list(freq_itemsets)\n",
      "43/1292:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "itemsets = dict(frequent_itemsets(estimates_assoc_openness, .05))\n",
      "list(itemsets)\n",
      "43/1293:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "itemsets = dict(frequent_itemsets(estimates_assoc_openness, .05))\n",
      "list(freq_itemsets)\n",
      "43/1294:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "itemsets = dict(frequent_itemsets(estimates_assoc_openness, min_support=1))\n",
      "list(freq_itemsets)\n",
      "43/1295:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "itemsets = dict(frequent_itemsets(estimates_assoc_openness, min_support=1))\n",
      "list(itemsets)\n",
      "43/1296:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "itemsets = dict(frequent_itemsets(estimates_assoc_openness, min_support=1))\n",
      "list(freq_itemsets)\n",
      "43/1297:\n",
      "\n",
      "\n",
      "list(association_rules(itemsets, 0.1))\n",
      "43/1298: list(association_rules(dict(freq_itemsets), 0.1))\n",
      "43/1299: list(association_rules(itemsets, 0.1))\n",
      "43/1300: list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=1)), 0.1))\n",
      "43/1301: list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=1)), 0.1))\n",
      "43/1302:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "itemsets = dict(frequent_itemsets(estimates_assoc_openness, min_support=1))\n",
      "list(freq_itemsets)\n",
      "43/1303: list(association_rules(dict(freq_itemsets), 0.1))\n",
      "43/1304:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "#itemsets = dict(frequent_itemsets(estimates_assoc_openness, min_support=1))\n",
      "#list(freq_itemsets)\n",
      "43/1305: list(association_rules(dict(freq_itemsets), 0.1))\n",
      "43/1306:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "#itemsets = dict(frequent_itemsets(estimates_assoc_openness, min_support=1))\n",
      "list(freq_itemsets)\n",
      "43/1307: list(association_rules(dict(freq_itemsets), 0.1))\n",
      "43/1308:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "#itemsets = dict(frequent_itemsets(estimates_assoc_openness, min_support=1))\n",
      "freq_itemsets_list = list(freq_itemsets)\n",
      "43/1309: list(association_rules(dict(freq_itemsets), 0.1))\n",
      "43/1310:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "#itemsets = dict(frequent_itemsets(estimates_assoc_openness, min_support=1))\n",
      "#list(freq_itemsets)\n",
      "43/1311: list(association_rules(dict(freq_itemsets), 0.1))\n",
      "43/1312: list(association_rules(dict(freq_itemsets), 0.5))\n",
      "43/1313: list(association_rules(dict(freq_itemsets), 0.3))\n",
      "43/1314: list(association_rules(dict(freq_itemsets), 0.1))\n",
      "43/1315: list(association_rules(dict(freq_itemsets), 0.001))\n",
      "43/1316:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "#itemsets = dict(frequent_itemsets(estimates_assoc_openness, min_support=1))\n",
      "#list(freq_itemsets)\n",
      "43/1317: list(association_rules(dict(freq_itemsets), 0.001))\n",
      "43/1318: list(association_rules(dict(freq_itemsets), 0.001))\n",
      "43/1319:\n",
      "freq_itemsets = frequent_itemsets(estimates_assoc_openness, min_support=1)\n",
      "# TODO \n",
      "#itemsets = dict(frequent_itemsets(estimates_assoc_openness, min_support=1))\n",
      "#list(freq_itemsets)\n",
      "43/1320: list(association_rules(dict(freq_itemsets), 0.001))\n",
      "43/1321: list(association_rules(frequent_itemsets(estimates_assoc_openness, min_support=1), 0.001))\n",
      "43/1322: list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=1)), 0.001))\n",
      "43/1323: list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=1)), 0.001))\n",
      "43/1324: list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=1)), 0.5))\n",
      "43/1325: list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.1)), 0.5))\n",
      "43/1326: list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.01)), 0.5))\n",
      "43/1327: list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5))\n",
      "48/7:\n",
      "freq_itemsets = frequent_itemsets(transactions, min_support=5)\n",
      "# TODO \n",
      "res = 0\n",
      "for itm, sup in frequent_itemsets(transactions, min_support = 5):\n",
      "    if len(itm) == 2:\n",
      "        res = res + 1\n",
      "print(res)\n",
      "\n",
      "mylist = []\n",
      "rs = []\n",
      "match = 0\n",
      "\n",
      "for itm, sup in frequent_itemsets(transactions, min_support = 5):\n",
      "    if len(list(itm)) == 2:\n",
      "        \n",
      "        twoset = list(itm) \n",
      "        for element in twoset:\n",
      "            for sub_itm, sub_sup in frequent_itemsets(transactions, min_support = 5):\n",
      "                if len(sub_itm) == 2:\n",
      "                    if itm != sub_itm:\n",
      "                        sub_twoset = list(sub_itm)\n",
      "                        match = 0\n",
      "                        for sub_element in sub_itm:\n",
      "                            if sub_element != element:\n",
      "                                elem = sub_element\n",
      "                            else:\n",
      "                                match = match + 1\n",
      "                        if match == 1:\n",
      "                            rs = twoset.copy()\n",
      "                            rs.append(elem)\n",
      "                            mylist.append(rs)\n",
      "for e in mylist:\n",
      "    e.sort()\n",
      "uniqu_threesets = [list(x) for x in set(tuple(x) for x in mylist)]\n",
      "print(len(uniqu_threesets))\n",
      "print(uniqu_threesets)\n",
      "48/8:\n",
      "transactions = [\n",
      "  ['B','D','F','H'],\n",
      "  ['C','D','F','G'],\n",
      "  ['A','D','F','G'],\n",
      "  ['A','B','C','D','H'],\n",
      "  ['A','C','F','G'],\n",
      "  ['D','H'],\n",
      "  ['A','B','E','F'],\n",
      "  ['A','D','F','G','H'],\n",
      "  ['A','C','D','F','G'],\n",
      "  ['D','F','G','H'],\n",
      "  ['A','C','D','E'],\n",
      "  ['B','E','F','H'],\n",
      "  ['D','F','G'],\n",
      "  ['C','F','G','H'],\n",
      "  ['A','C','D','F','H']\n",
      "]\n",
      "transactions\n",
      "48/9:\n",
      "final_result = 0\n",
      "\n",
      "n = 8\n",
      "fac_n = 8\n",
      "while n > 1:\n",
      "    fac_n = fac_n * (n - 1)\n",
      "    n = n - 1\n",
      "\n",
      "for k_iter in range(1, 8):\n",
      "    \n",
      "    k = 8 - k_iter\n",
      "    fac_k = 8 - k_iter\n",
      "    while k > 1:\n",
      "        fac_k = fac_k * (k - 1)\n",
      "        k = k - 1\n",
      "    \n",
      "    k_n = k_iter\n",
      "    fac_k_n = k_iter\n",
      "    while k_n > 1:\n",
      "        fac_k_n = fac_k_n * (k_n - 1)\n",
      "        k_n = k_n - 1\n",
      "\n",
      "    final_result = final_result + (fac_n / (fac_k * fac_k_n))\n",
      "    \n",
      "print (final_result)\n",
      "48/10:\n",
      "from orangecontrib.associate import *  \n",
      "freq_itemsets = frequent_itemsets(transactions, min_support=1)\n",
      "# TODO \n",
      "print((len(list(freq_itemsets))/254) * 100)\n",
      "48/11:\n",
      "freq_itemsets = frequent_itemsets(transactions, min_support=5)\n",
      "# TODO \n",
      "res = 0\n",
      "for itm, sup in frequent_itemsets(transactions, min_support = 5):\n",
      "    if len(itm) == 2:\n",
      "        res = res + 1\n",
      "print(res)\n",
      "\n",
      "mylist = []\n",
      "rs = []\n",
      "match = 0\n",
      "\n",
      "for itm, sup in frequent_itemsets(transactions, min_support = 5):\n",
      "    if len(list(itm)) == 2:\n",
      "        \n",
      "        twoset = list(itm) \n",
      "        for element in twoset:\n",
      "            for sub_itm, sub_sup in frequent_itemsets(transactions, min_support = 5):\n",
      "                if len(sub_itm) == 2:\n",
      "                    if itm != sub_itm:\n",
      "                        sub_twoset = list(sub_itm)\n",
      "                        match = 0\n",
      "                        for sub_element in sub_itm:\n",
      "                            if sub_element != element:\n",
      "                                elem = sub_element\n",
      "                            else:\n",
      "                                match = match + 1\n",
      "                        if match == 1:\n",
      "                            rs = twoset.copy()\n",
      "                            rs.append(elem)\n",
      "                            mylist.append(rs)\n",
      "for e in mylist:\n",
      "    e.sort()\n",
      "uniqu_threesets = [list(x) for x in set(tuple(x) for x in mylist)]\n",
      "print(len(uniqu_threesets))\n",
      "print(uniqu_threesets)\n",
      "48/12:\n",
      "freq_itemsets = frequent_itemsets(transactions, min_support=5)\n",
      "# TODO \n",
      "print(list(freq_itemsets)[12])\n",
      "48/13:\n",
      "freq_itemsets = frequent_itemsets(transactions, min_support=5)\n",
      "for itemset in list(freq_itemsets):\n",
      "    items = list(itemset[0])\n",
      "    support_count  = itemset[1]\n",
      "    print(\"support count(\" + str(items) + \") = \" +str(support_count))\n",
      "48/14:\n",
      "# TODO\n",
      "\n",
      "freq_itemsets = frequent_itemsets(transactions, min_support=5)\n",
      "assoc_rule = association_rules(dict(freq_itemsets), 0.5)\n",
      "\n",
      "list(assoc_rule)\n",
      "48/15:\n",
      "from apyori import apriori\n",
      "rules = apriori(transactions, min_support = 5/len(transactions), min_confidence = 0.5, min_length = 3)\n",
      "\n",
      "# TODO\n",
      "list(rules)\n",
      "48/16:\n",
      "import pandas as pd\n",
      "\n",
      "titanic_df = pd.read_csv(\"titanic.csv\")\n",
      "titanic = titanic_df.values # Convert dataFrame into Array\n",
      "print(titanic_df.head(5))\n",
      "print('Class')\n",
      "print(titanic_df.Class.value_counts())\n",
      "print('Sex')\n",
      "print(titanic_df.Sex.value_counts())\n",
      "print('Age')\n",
      "print(titanic_df.Age.value_counts())\n",
      "print('Survived')\n",
      "print(titanic_df.Survived.value_counts())\n",
      "titanic_df.head()\n",
      "48/17:\n",
      "rules = apriori(titanic, min_support = 0.1, min_confidence = 0.8)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "48/18:\n",
      "# TODO\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "rules = apriori(titanic, min_support = 0.1, min_confidence = 1)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['items_base', 'items_add', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        items_base = ''\n",
      "        items_add = ''\n",
      "        confidence = 0\n",
      "        lift = 0\n",
      "        element_nr = 0\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:\n",
      "                items_base = order_statistics\n",
      "            elif element_nr == 1:\n",
      "                items_add = order_statistics\n",
      "            elif element_nr == 2:\n",
      "                confidence = order_statistics\n",
      "            elif element_nr == 3:\n",
      "                lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'items_base':items_base, 'items_add': items_add, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.head()\n",
      "48/19:\n",
      "# TODO\n",
      "rules = apriori(titanic, min_support = 0.1, min_confidence = 0.1)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['items_base', 'items_add', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        items_base = ''\n",
      "        items_add = ''\n",
      "        confidence = 0\n",
      "        lift = 0\n",
      "        element_nr = 0\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:\n",
      "                items_base = order_statistics\n",
      "            elif element_nr == 1:\n",
      "                items_add = order_statistics\n",
      "            elif element_nr == 2:\n",
      "                confidence = order_statistics\n",
      "            elif element_nr == 3:\n",
      "                lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'items_base':items_base, 'items_add': items_add, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.sort_values('lift', ascending = False).head()\n",
      "48/20:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "rules = apriori(titanic, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['items_base', 'items_add', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        items_base = ''\n",
      "        items_add = ''\n",
      "        confidence = 0\n",
      "        lift = 0\n",
      "        element_nr = 0\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:\n",
      "                items_base = order_statistics\n",
      "            elif element_nr == 1:\n",
      "                items_add = order_statistics\n",
      "            elif element_nr == 2:\n",
      "                confidence = order_statistics\n",
      "            elif element_nr == 3:\n",
      "                lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'items_base':items_base, 'items_add': items_add, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.sort_values('lift', ascending = False).head()\n",
      "48/21:\n",
      "# TODO\n",
      "mydf2 = mydf[(mydf.lift > 3) & (mydf.confidence == 1)]\n",
      "mydf2.sort_values('confidence', ascending = False).head(9)\n",
      "48/22:\n",
      "# TODO\n",
      "mydf[mydf.support >= 0.7].sort_values('support', ascending = False).head()\n",
      "43/1328:\n",
      "for i in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.00 1)), 0.5)):\n",
      "    print(i)\n",
      "43/1329: list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.00 1)), 0.5))\n",
      "43/1330: list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.00 1)), 0.5))\n",
      "43/1331: list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.01)), 0.5))\n",
      "43/1332:\n",
      "for i in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    print(i)\n",
      "43/1333:\n",
      "for i in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    for k in i:\n",
      "        print(k)\n",
      "43/1334:\n",
      "for i in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    for k in i:\n",
      "        print(k)\n",
      "    print ('--')\n",
      "43/1335:\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    for frozenset_elem in frozenset_:\n",
      "        print(frozenset_elem)\n",
      "    print ('--')\n",
      "43/1336:\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    for frozenset_elem in frozenset_:\n",
      "        print(list(frozenset_elem))\n",
      "    print ('--')\n",
      "43/1337:\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    for frozenset_elem in frozenset_:\n",
      "        print(frozenset_elem)\n",
      "    print ('--')\n",
      "43/1338:\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    for frozenset_elem in frozenset_:\n",
      "        print(frozenset_elem)\n",
      "    print(list(frozenset_elem))\n",
      "    print ('--')\n",
      "43/1339:\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    for frozenset_elem in frozenset_:\n",
      "        print(frozenset_elem)\n",
      "#    print(list(frozenset_elem))\n",
      "    print ('--')\n",
      "43/1340:\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    for frozenset_elem in frozenset_:\n",
      "        print(frozenset_elem)\n",
      "    print(list(frozenset_elem))\n",
      "    print ('--')\n",
      "43/1341:\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    for frozenset_elem in frozenset_:\n",
      "        print(frozenset_elem)\n",
      "#    print(list(frozenset_elem))\n",
      "    print ('--')\n",
      "43/1342:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = len(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = len(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                                    , 'consequent': consequent\n",
      "                                    , 'support_count':support_count\n",
      "                                    , 'confidence': confidence}\n",
      "                           , ignore_index = True)\n",
      "assoc_df\n",
      "43/1343:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = len(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = len(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                                    , 'consequent': consequent\n",
      "                                    , 'support_count':support_count\n",
      "                                    , 'confidence': confidence}\n",
      "                           , ignore_index = True)\n",
      "assoc_df\n",
      "43/1344:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = len(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = len(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "       \n",
      "assoc_df\n",
      "43/1345:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = len(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = len(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                                    , 'consequent': consequent\n",
      "                                    , 'support_count':support_count\n",
      "                                    , 'confidence': confidence}\n",
      "                           , ignore_index = True)\n",
      "assoc_df\n",
      "43/1346:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = len(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = len(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                                    , 'consequent': consequent\n",
      "                                    , 'support_count':support_count\n",
      "                                    , 'confidence': confidence}\n",
      "                           , ignore_index = True)\n",
      "        print(antescedent)\n",
      "assoc_df\n",
      "43/1347:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = len(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = len(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                                    , 'consequent': consequent\n",
      "                                    , 'support_count':support_count\n",
      "                                    , 'confidence': confidence}\n",
      "                           , ignore_index = True)\n",
      "        print(frozenset_elem)\n",
      "assoc_df\n",
      "43/1348:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                                    , 'consequent': consequent\n",
      "                                    , 'support_count':support_count\n",
      "                                    , 'confidence': confidence}\n",
      "                           , ignore_index = True)\n",
      "\n",
      "assoc_df\n",
      "43/1349:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                                    , 'consequent': consequent\n",
      "                                    , 'support_count':support_count\n",
      "                                    , 'confidence': confidence}\n",
      "                           , ignore_index = True)\n",
      "        print(antescedent, consequent, support_count, confidence)\n",
      "        print(frozenset_elem)\n",
      "    print('--')\n",
      "\n",
      "assoc_df\n",
      "43/1350:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                                    , 'consequent': consequent\n",
      "                                    , 'support_count':support_count\n",
      "                                    , 'confidence': confidence}\n",
      "                           , ignore_index = True)\n",
      "        print(antescedent, consequent, support_count, confidence)\n",
      "        print(frozenset_elem)\n",
      "    print(frozenset_)\n",
      "    print('--')\n",
      "\n",
      "assoc_df\n",
      "43/1351:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                                    , 'consequent': consequent\n",
      "                                    , 'support_count':support_count\n",
      "                                    , 'confidence': confidence}\n",
      "                           , ignore_index = True)\n",
      "        #print(antescedent, consequent, support_count, confidence)\n",
      "        print(frozenset_elem)\n",
      "    print(frozenset_)\n",
      "    print('--')\n",
      "\n",
      "assoc_df\n",
      "43/1352:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                                    , 'consequent': consequent\n",
      "                                    , 'support_count':support_count\n",
      "                                    , 'confidence': confidence}\n",
      "                           , ignore_index = True)\n",
      "        #print(antescedent, consequent, support_count, confidence)\n",
      "        print(frozenset_elem, i)\n",
      "    print(frozenset_)\n",
      "    print('--')\n",
      "\n",
      "assoc_df\n",
      "43/1353:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=1\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==1:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==3:\n",
      "            support_count = frozenset_elem\n",
      "        if i==4:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                                    , 'consequent': consequent\n",
      "                                    , 'support_count':support_count\n",
      "                                    , 'confidence': confidence}\n",
      "                           , ignore_index = True)\n",
      "        #print(antescedent, consequent, support_count, confidence)\n",
      "        print(frozenset_elem, i)\n",
      "    print(frozenset_)\n",
      "    print('--')\n",
      "\n",
      "assoc_df\n",
      "43/1354:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                                    , 'consequent': consequent\n",
      "                                    , 'support_count':support_count\n",
      "                                    , 'confidence': confidence}\n",
      "                           , ignore_index = True)\n",
      "        #print(antescedent, consequent, support_count, confidence)\n",
      "        print(frozenset_elem, i)\n",
      "    print(frozenset_)\n",
      "    print('--')\n",
      "\n",
      "assoc_df\n",
      "43/1355:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence==frozenset_elem\n",
      "        i = i+1\n",
      "        #print(antescedent, consequent, support_count, confidence)\n",
      "        print(frozenset_elem, i)\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                            , 'consequent': consequent\n",
      "                            , 'support_count':support_count\n",
      "                            , 'confidence': confidence}\n",
      "                            , ignore_index = True)\n",
      "\n",
      "\n",
      "assoc_df\n",
      "43/1356:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "        #print(antescedent, consequent, support_count, confidence)\n",
      "        print(frozenset_elem, i)\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent\n",
      "                            , 'consequent': consequent\n",
      "                            , 'support_count':support_count\n",
      "                            , 'confidence': confidence}\n",
      "                            , ignore_index = True)\n",
      "\n",
      "\n",
      "assoc_df\n",
      "43/1357:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.001)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n",
      "43/1358:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.1)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n",
      "43/1359:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.2)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n",
      "43/1360:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.5)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n",
      "43/1361:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.3)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n",
      "43/1362:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.2)), 0.5)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n",
      "43/1363:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.2)), 0.7)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n",
      "43/1364:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.2)), 0.6)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n",
      "43/1365:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.2)), 0.51)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n",
      "43/1366:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.4)), 0.51)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n",
      "43/1367:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.3)), 0.51)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n",
      "43/1368:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.2)), 0.51)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n",
      "43/1369:\n",
      "assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "antescedent=np.nan\n",
      "consequent=np.nan\n",
      "support_count=np.nan\n",
      "confidence=np.nan\n",
      "\n",
      "for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.25)), 0.51)):\n",
      "    i=0\n",
      "    for frozenset_elem in frozenset_:\n",
      "        if i==0:\n",
      "            antescedent = list(frozenset_elem)\n",
      "        if i==1:\n",
      "            consequent = list(frozenset_elem)\n",
      "        if i==2:\n",
      "            support_count = frozenset_elem\n",
      "        if i==3:\n",
      "            confidence=frozenset_elem\n",
      "        i = i+1\n",
      "    assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                            , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "assoc_df\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/1370:\n",
      "def get_association_rules_df(items, support, confidence):\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=support)), confidence)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:\n",
      "                antescedent = list(frozenset_elem)\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1371:\n",
      "def get_association_rules_df(items_df, support, confidence):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=support)), confidence)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:\n",
      "                antescedent = list(frozenset_elem)\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1372:\n",
      "def get_association_rules_df(items_df, support, confidence):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=support)), confidence)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0: antescedent = list(frozenset_elem)\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1373:\n",
      "def get_association_rules_df(items_df, support, confidence):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=support)), confidence)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0: antescedent = list(frozenset_elem)\n",
      "            if i==1:consequent = list(frozenset_elem)\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1374: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1375:\n",
      "def get_association_rules_df(items_df, support, confidence):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=support)), confidence)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)\n",
      "            if i==1:consequent = list(frozenset_elem)\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1376: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1377:\n",
      "def get_association_rules_df(items_df, support, confidence):\n",
      "    items = items_df.values.tolist()\n",
      "    print(items)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=support)), confidence)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)\n",
      "            if i==1:consequent = list(frozenset_elem)\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1378: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1379:\n",
      "def get_association_rules_df(items_df, support, confidence):\n",
      "    items = items_df.values.tolist()\n",
      "    print(items)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=support)), confidence)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)\n",
      "            if i==1:consequent = list(frozenset_elem)\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1380:\n",
      "def get_association_rules_df(items_df, support, confidence):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=support)), confidence)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)\n",
      "            if i==1:consequent = list(frozenset_elem)\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1381: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1382:\n",
      "def get_association_rules_df(items_df, support, confidence):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=support)), confidence)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:\n",
      "                antescedent = list(frozenset_elem)\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1383: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1384:\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.1)), 0.4)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:\n",
      "                antescedent = list(frozenset_elem)\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1385:\n",
      "    #items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=0.1)), 0.4)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:\n",
      "                antescedent = list(frozenset_elem)\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1386:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(estimates_assoc_openness, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:\n",
      "                antescedent = list(frozenset_elem)\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1387: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1388:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:\n",
      "                antescedent = list(frozenset_elem)\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    assoc_df\n",
      "43/1389: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1390:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:\n",
      "                antescedent = list(frozenset_elem)\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1391: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1392:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:\n",
      "                antescedent = list(frozenset_elem)(1)\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1393: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1394:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:\n",
      "                antescedent = list(frozenset_elem)[1]\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1395: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1396:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:\n",
      "                antescedent = list(frozenset_elem)[0]\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1397: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1398:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:\n",
      "                antescedent = list(frozenset_elem)[0]\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)[0]\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1399: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1400:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)[0]\n",
      "            if i==1:\n",
      "                consequent = list(frozenset_elem)[0]\n",
      "            if i==2:\n",
      "                support_count = frozenset_elem\n",
      "            if i==3:\n",
      "                confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1401: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1402:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)[0]\n",
      "            if i==1:consequent = list(frozenset_elem)[0]\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1403: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.5)\n",
      "43/1404: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.3, 0.5)\n",
      "43/1405: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.2, 0.5)\n",
      "43/1406:\n",
      "rules = apriori(estimates_assoc_openness\n",
      "                #, min_support = 0.00001\n",
      "                #, min_confidence = 0.6\n",
      "               )\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "#len(rules_result)rules_result\n",
      "43/1407: rules_result\n",
      "43/1408:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(apriori(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)[0]\n",
      "            if i==1:consequent = list(frozenset_elem)[0]\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1409: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.2, 0.5)\n",
      "43/1410:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    antescedent=np.nan\n",
      "    consequent=np.nan\n",
      "    support_count=np.nan\n",
      "    confidence=np.nan\n",
      "\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)[0]\n",
      "            if i==1:consequent = list(frozenset_elem)[0]\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1411: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.2, 0.5)\n",
      "43/1412:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count', 'confidence']))\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)[0]\n",
      "            if i==1:consequent = list(frozenset_elem)[0]\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent\n",
      "                                , 'support_count':support_count, 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1413: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.2, 0.5)\n",
      "43/1414: items\n",
      "43/1415: estimates_assoc_df[['StoryPoints','Is_Open']].values.tolist()\n",
      "43/1416: len(estimates_assoc_df[['StoryPoints','Is_Open']].values.tolist())\n",
      "43/1417:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count','support', 'confidence']))\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)[0]\n",
      "            if i==1:consequent = list(frozenset_elem)[0]\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support_count':support_count\n",
      "                                    ,'support':support_count/len(items), 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1418: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.2, 0.5)\n",
      "43/1419:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count','support_%', 'confidence']))\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)[0]\n",
      "            if i==1:consequent = list(frozenset_elem)[0]\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support_count':support_count\n",
      "                                    ,'support_%':support_count/len(items), 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1420: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.2, 0.5)\n",
      "43/1421:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count','support_%', 'confidence']))\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)[0]\n",
      "            if i==1:consequent = list(frozenset_elem)[0]\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support_count':support_count\n",
      "                                    ,'support_%':round(support_count/len(items), 2), 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1422: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.2, 0.5)\n",
      "43/1423:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count','support_%', 'confidence']))\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)[0]\n",
      "            if i==1:consequent = list(frozenset_elem)[0]\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support_count':support_count\n",
      "                                    ,'support_%':round(support_count/len(items), 1), 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1424: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.2, 0.5)\n",
      "43/1425:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values.tolist()\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support_count','support_%', 'confidence']))\n",
      "    for frozenset_ in list(association_rules(dict(frequent_itemsets(items, min_support=min_support_)), min_confidence_)):\n",
      "        i=0\n",
      "        for frozenset_elem in frozenset_:\n",
      "            if i==0:antescedent = list(frozenset_elem)[0]\n",
      "            if i==1:consequent = list(frozenset_elem)[0]\n",
      "            if i==2:support_count = frozenset_elem\n",
      "            if i==3:confidence=frozenset_elem\n",
      "            i = i+1\n",
      "        assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support_count':support_count\n",
      "                                    ,'support_%':support_count/len(items), 'confidence': confidence}, ignore_index = True)\n",
      "    return assoc_df\n",
      "43/1426: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.2, 0.5)\n",
      "43/1427: rules_stats(association_rules(dict(frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=min_support_)), min_confidence_))\n",
      "43/1428: rules_stats(association_rules(dict(frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1)), 0.4))\n",
      "43/1429:\n",
      "rules_stats(association_rules(dict(frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1)), 0.4),\n",
      "           dict(frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1),\n",
      "               N)\n",
      "43/1430:\n",
      "rules_stats(association_rules(dict(frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1)), 0.4),\n",
      "           dict(frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1),\n",
      "               30)\n",
      "43/1431:\n",
      "rules_stats(association_rules(dict(frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1)), 0.4),\n",
      "           dict(frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1)),\n",
      "               30)\n",
      "43/1432:\n",
      "rules_stats(association_rules(dict(frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1)), 0.4),\n",
      "           dict(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1),\n",
      "               30)\n",
      "43/1433:\n",
      "rules_stats(association_rules(dict(frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1)), 0.4),\n",
      "           estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1,\n",
      "               30)\n",
      "43/1434:\n",
      "rules_stats(association_rules(dict(frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1)), 0.4),\n",
      "           estimates_assoc_df[['StoryPoints','Is_Open']],\n",
      "               30)\n",
      "43/1435:\n",
      "rules_stats(association_rules(dict(frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1)), 0.4),\n",
      "           frequent_itemsets(estimates_assoc_df[['StoryPoints','Is_Open']], min_support=0.1),\n",
      "               30)\n",
      "43/1436:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.00001, min_confidence = 0.6)\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "rules_result\n",
      "43/1437:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['items_base', 'items_add', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        items_base = ''\n",
      "        items_add = ''\n",
      "        confidence = 0\n",
      "        lift = 0\n",
      "        element_nr = 0\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:\n",
      "                items_base = order_statistics\n",
      "            elif element_nr == 1:\n",
      "                items_add = order_statistics\n",
      "            elif element_nr == 2:\n",
      "                confidence = order_statistics\n",
      "            elif element_nr == 3:\n",
      "                lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'items_base':items_base, 'items_add': items_add, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.sort_values('lift', ascending = False).head()\n",
      "43/1438:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['items_base', 'items_add', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        items_base = ''\n",
      "        items_add = ''\n",
      "        confidence = 0\n",
      "        lift = 0\n",
      "        element_nr = 0\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:\n",
      "                items_base = order_statistics\n",
      "            elif element_nr == 1:\n",
      "                items_add = order_statistics\n",
      "            elif element_nr == 2:\n",
      "                confidence = order_statistics\n",
      "            elif element_nr == 3:\n",
      "                lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'items_base':items_base, 'items_add': items_add, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.sort_values('lift', ascending = False)\n",
      "43/1439:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:\n",
      "                antescedent = order_statistics\n",
      "            elif element_nr == 1:\n",
      "                consequent = order_statistics\n",
      "            elif element_nr == 2:\n",
      "                confidence = order_statistics\n",
      "            elif element_nr == 3:\n",
      "                lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.sort_values('lift', ascending = False)\n",
      "43/1440:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:\n",
      "                antescedent = order_statistics\n",
      "            elif element_nr == 1:\n",
      "                consequent = order_statistics\n",
      "            elif element_nr == 2:\n",
      "                confidence = order_statistics\n",
      "            elif element_nr == 3:\n",
      "                lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.sort_values('lift', ascending = False)\n",
      "43/1441:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        items_base = ''\n",
      "        items_add = ''\n",
      "        confidence = 0\n",
      "        lift = 0\n",
      "        element_nr = 0\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:\n",
      "                antescedent = order_statistics\n",
      "            elif element_nr == 1:\n",
      "                consequent = order_statistics\n",
      "            elif element_nr == 2:\n",
      "                confidence = order_statistics\n",
      "            elif element_nr == 3:\n",
      "                lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'antescedent':items_base, 'consequent': items_add, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.sort_values('lift', ascending = False)\n",
      "43/1442:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        items_base = ''\n",
      "        items_add = ''\n",
      "        confidence = 0\n",
      "        lift = 0\n",
      "        element_nr = 0\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:\n",
      "                items_base = order_statistics\n",
      "            elif element_nr == 1:\n",
      "                items_add = order_statistics\n",
      "            elif element_nr == 2:\n",
      "                confidence = order_statistics\n",
      "            elif element_nr == 3:\n",
      "                lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'antescedent':items_base, 'consequent': items_add, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.sort_values('lift', ascending = False)\n",
      "43/1443:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        items_base = ''\n",
      "        items_add = ''\n",
      "        confidence = 0\n",
      "        lift = 0\n",
      "        element_nr = 0\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:\n",
      "                antescedent = order_statistics\n",
      "            elif element_nr == 1:\n",
      "                consequent = order_statistics\n",
      "            elif element_nr == 2:\n",
      "                confidence = order_statistics\n",
      "            elif element_nr == 3:\n",
      "                lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.sort_values('lift', ascending = False)\n",
      "43/1444:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        element_nr = 0\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:\n",
      "                antescedent = order_statistics\n",
      "            elif element_nr == 1:\n",
      "                consequent = order_statistics\n",
      "            elif element_nr == 2:\n",
      "                confidence = order_statistics\n",
      "            elif element_nr == 3:\n",
      "                lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.sort_values('lift', ascending = False)\n",
      "43/1445:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "43/1446:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "rules_list\n",
      "43/1447:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "rules_list[2]\n",
      "43/1448:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "rules_list\n",
      "43/1449:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "rules_list[1]\n",
      "43/1450:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "rules_list[6]\n",
      "43/1451:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.000001, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "for i in rules_list[6]:\n",
      "    print(i)\n",
      "43/1452:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.4, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        element_nr = 0\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:antescedent = order_statistics\n",
      "            elif element_nr == 1:consequent = order_statistics\n",
      "            elif element_nr == 2:confidence = order_statistics\n",
      "            elif element_nr == 3:lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.sort_values('lift', ascending = False)\n",
      "43/1453:\n",
      "rules = apriori(estimates_assoc_openness, min_support = 0.1, min_confidence = 0.000001)\n",
      "rules_list = list(rules)\n",
      "\n",
      "mydf = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "\n",
      "for relation_recordset in rules_list:\n",
      "    for rel_recordset_element in relation_recordset[2]:\n",
      "        element_nr = 0\n",
      "        for order_statistics in list(rel_recordset_element):\n",
      "            if element_nr == 0:antescedent = order_statistics\n",
      "            elif element_nr == 1:consequent = order_statistics\n",
      "            elif element_nr == 2:confidence = order_statistics\n",
      "            elif element_nr == 3:lift = order_statistics\n",
      "            element_nr = element_nr + 1 \n",
      "        mydf = mydf.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                           , ignore_index = True)\n",
      "mydf.sort_values('lift', ascending = False)\n",
      "43/1454:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1455: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001)\n",
      "43/1456: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1457:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df[anyescedent]!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1458: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1459:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df[antescedent]!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1460: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1461:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1462: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1463:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1464: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1465:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']=='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1466: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1467:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1468: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1469: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True).loc[0]\n",
      "43/1470:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent'].trim()!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1471: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True).loc[0]\n",
      "43/1472:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = strip(order_statistics)\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent'].trim()!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1473: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1474:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics.strip()\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent'].trim()!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1475: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1476:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent'].trim()!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1477: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1478:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1479: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1480:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics.trim()\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1481: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1482:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics.strip())\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1483:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics.strip()\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1484: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1485:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1486:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics(0)\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1487: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1488:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = order_statistics[0]\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1489: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1490:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = list(order_statistics)\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='()']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1491: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1492:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = list(order_statistics)\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1493: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1494:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = list(order_statistics)[0]\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1495: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1496:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = list(order_statistics)[1]\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1497: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1498:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = list(order_statistics)[]\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1499:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = list(order_statistics)(0)\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1500: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1501:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = list(order_statistics)\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1502: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1503:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = list(order_statistics)\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent'][0]='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1504:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = list(order_statistics)\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1505:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = list(order_statistics)\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent'][0]!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1506: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1507:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = list(order_statistics)\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent'][0]!='']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1508: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1509:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = list(order_statistics)\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1510: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1511:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1512: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1513:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1514:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = order_statistics\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1515: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1516:\n",
      "def get_association_rules_df(items_df, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df.values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1517: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1518: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], min_support_=0.0001, 0.0001, True)\n",
      "43/1519: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], min_support_==0.0001, 0.0001, True)\n",
      "43/1520: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], min_support_=0.0001, 0.0001, True)\n",
      "43/1521: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], =0.0001, 0.0001, True)\n",
      "43/1522: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.0001, 0.0001, True)\n",
      "43/1523: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.0001, True)\n",
      "43/1524: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.1, 0.3, True)\n",
      "43/1525: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.25, 0.3, True)\n",
      "43/1526: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.25, 0.3, False)\n",
      "43/1527: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.25, 0.3, True)\n",
      "43/1528: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.25, 0.3, False)\n",
      "43/1529: get_association_rules_df(estimates_assoc_df[['StoryPoints','Is_Open']], 0.25, 0.3, True)\n",
      "43/1530: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.3, True)\n",
      "43/1531:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "43/1532: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.3, True)\n",
      "43/1533:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if col!='user' & col!=metric:\n",
      "            df_=get_association_rules_df(items_df, var1, var2, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1534: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.3, True)\n",
      "43/1535: get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.3, True)\n",
      "43/1536:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user' & col!=metric):\n",
      "            df_=get_association_rules_df(items_df, var1, var2, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1537: get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.3, True)\n",
      "43/1538:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, var1, var2, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1539: get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.3, True)\n",
      "43/1540:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:antescedent = str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1541: get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.3, True)\n",
      "43/1542: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.5, True)\n",
      "43/1543: get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.5, True)\n",
      "43/1544: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "43/1545: get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "43/1546: get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "43/1547:\n",
      "dfr = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "dfr\n",
      "43/1548: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "43/1549: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.3, False)\n",
      "43/1550: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.3, False)\n",
      "43/1551: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "43/1552:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent = str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1553: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "43/1554:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            print(relation_recordset[2])\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1555: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "43/1556:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            order_statistic_counter = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if order_statistic_counter ==0:\n",
      "                    if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                    elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                    elif element_nr == 2:confidence = order_statistics\n",
      "                    elif element_nr == 3:lift = order_statistics\n",
      "                else:\n",
      "                    if element_nr == 2:inverse_rule_confidence = order_statistics\n",
      "                order_statistic_counter = order_statistic_counter + 1\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1557: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "43/1558:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            order_statistic_counter = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if order_statistic_counter ==0:\n",
      "                    if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                    elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                    elif element_nr == 2:confidence = order_statistics\n",
      "                    elif element_nr == 3:lift = order_statistics\n",
      "                else:\n",
      "                    if element_nr == 2:inverse_rule_confidence = order_statistics\n",
      "                order_statistic_counter = order_statistic_counter + 1\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], \n",
      "                                        'confidence': confidence, 'lift':lift, inverse_rule_confidence = ''}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1559:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            order_statistic_counter = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if order_statistic_counter ==0:\n",
      "                    if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                    elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                    elif element_nr == 2:confidence = order_statistics\n",
      "                    elif element_nr == 3:lift = order_statistics\n",
      "                else:\n",
      "                    if element_nr == 2:inverse_rule_confidence = order_statistics\n",
      "                order_statistic_counter = order_statistic_counter + 1\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], \n",
      "                                        'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':''}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1560: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "43/1561:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        antescedent = ''\n",
      "        consequent = ''\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            order_statistic_counter = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if order_statistic_counter ==0:\n",
      "                    if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                    elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                    elif element_nr == 2:confidence = order_statistics\n",
      "                    elif element_nr == 3:lift = order_statistics\n",
      "                else:\n",
      "                    if element_nr == 2:inverse_rule_confidence = order_statistics\n",
      "                order_statistic_counter = order_statistic_counter + 1\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], \n",
      "                                        'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':''}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1562: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "43/1563:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        antescedent = np.nan\n",
      "        consequent = np.nan\n",
      "        confidence = np.nan\n",
      "        lift = np.nan\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            order_statistic_counter = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if order_statistic_counter ==0:\n",
      "                    if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                    elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                    elif element_nr == 2:confidence = order_statistics\n",
      "                    elif element_nr == 3:lift = order_statistics\n",
      "                else:\n",
      "                    if element_nr == 2:inverse_rule_confidence = order_statistics\n",
      "                order_statistic_counter = order_statistic_counter + 1\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], \n",
      "                                        'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':''}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1564: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "43/1565:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('lift', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1566: get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "43/1567:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "43/1568:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\n",
      "\"\"\"SELECT *, \n",
      "FROM dfr AS A\n",
      "LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1569:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"SELECT *, \n",
      "FROM dfr AS A\n",
      "LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1570:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"SELECT * \n",
      "FROM dfr AS A\n",
      "LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1571:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "FROM dfr AS A\n",
      "LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1572:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT antescedent+consequent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    antescedent+consequent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1573:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT antescedent+consequent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    concat(antescedent, consequent)\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1574:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT antescedent+consequent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    concat(antescedent+', 'consequent)\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1575:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT concat(antescedent+', 'consequent), count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    concat(antescedent+', 'consequent)\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1576:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT antescedent+', 'consequent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    antescedent+', 'consequent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1577:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT antescedent+', '+consequent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    antescedent+', '+consequent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1578:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT cast(antescedent as varchar(100)), count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    cast(antescedent as varchar(100))\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1579:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT cast(antescedent as varchar(100)) + '-', count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    cast(antescedent as varchar(100)) + '-'\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1580:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT cast(antescedent as varchar(100)), count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    cast(antescedent as varchar(100))\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1581:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT cast(antescedent as varchar(100)) || '-', count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    cast(antescedent as varchar(100)) || '-'\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1582:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1583:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1584:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "\n",
      "sql_query=\"\"\"\n",
      "SELECT antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1585:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "\n",
      "sql_query=\"\"\"\n",
      "SELECT antescedent || consequent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    antescedent || consequent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1586:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1587:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1588:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1589:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN A.antescedent = B.consequent THEN A.antescedent \n",
      "    *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1590:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN A.antescedent = B.consequent THEN A.antescedent \n",
      "    *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1591:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT \n",
      "    *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1592:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT antescedent || `antescedent:1`,*--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT \n",
      "    *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1593:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT \n",
      "    *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1594:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT LEFT(A.antescedent, 3)\n",
      "    *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1595:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT LEFT(A.antescedent, 3),\n",
      "    *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1596:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT LEFT(A.antescedent, 3),*--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1597:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT L*--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1598:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1599:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT A.antescedent\n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1600:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT left(A.antescedent, 2)\n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1601:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT substr(A.antescedent, 0, 2)\n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1602:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT substr(A.antescedent, 1, 2)\n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1603:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT substr(A.antescedent, 2, 2)\n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1604:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT substr(A.antescedent, 2, 4)\n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1605:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT substr(A.antescedent, 3, 3)\n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1606:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN 1 ELSE 0 END\n",
      "    \n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1607:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN 'Trait' ELSE 'Metric' END\n",
      "    \n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1608:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END,\n",
      "    \n",
      "    \n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1609:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END\n",
      "    \n",
      "    \n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1610:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END,\n",
      "    CASE WHEN LEN(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END\n",
      "    \n",
      "    \n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1611:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END\n",
      "    \n",
      "    \n",
      "    , *--A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1612:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--antescedent || antescedent, count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   antescedent || antescedent\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1613:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "GROUP BY\n",
      "    Trait, Metric\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1614:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   Trait, Metric\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1615:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *,\n",
      "RANK() OVER (\n",
      "    PARTITION BY Trait, Metric\n",
      "    ORDER BY Trait, Metric ASC\n",
      ")\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   Trait, Metric\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1616:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *,\n",
      "RANK() OVER (\n",
      "    PARTITION BY 1\n",
      "    ORDER BY 1 ASC\n",
      ")\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   Trait, Metric\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1617:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *,\n",
      "RANK() OVER (\n",
      "    PARTITION BY 2\n",
      "    ORDER BY 1 ASC\n",
      ")\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   Trait, Metric\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1618:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *,\n",
      "RANK() OVER (\n",
      "    PARTITION BY 2\n",
      "    ORDER BY 2 ASC\n",
      ")\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   Trait, Metric\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1619:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *,\n",
      "RANK() OVER (\n",
      "    PARTITION BY 1,2\n",
      "    ORDER BY 1,2 ASC\n",
      ")\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   Trait, Metric\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1620:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *,\n",
      "ROW_NUM() OVER (\n",
      "    PARTITION BY 1,2\n",
      "    ORDER BY 1,2 ASC\n",
      ")\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   Trait, Metric\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1621:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *,\n",
      "ROW_NUMBER() OVER (\n",
      "    PARTITION BY 1,2\n",
      "    ORDER BY 1,2 ASC\n",
      ")\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   Trait, Metric\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1622:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *,\n",
      "ROW_NUMBER() OVER (\n",
      "    PARTITION BY Trait, Metric\n",
      "    ORDER BY 1,2 ASC\n",
      ")\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   Trait, Metric\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1623:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT\n",
      "ROW_NUMBER() OVER (\n",
      "    PARTITION BY Trait, Metric\n",
      "    ORDER BY Trait, Metric ASC\n",
      "), *\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   Trait, Metric\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1624:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT\n",
      "ROW_NUMBER() OVER (\n",
      "    PARTITION BY Trait, Metric\n",
      "    ORDER BY Trait, Metric ASC\n",
      ") AS Rank, *\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "--GROUP BY\n",
      " --   Trait, Metric\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1625:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT\n",
      "ROW_NUMBER() OVER (\n",
      "    PARTITION BY Trait, Metric\n",
      "    ORDER BY Trait, Metric ASC\n",
      ") AS Rank, *\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "WHERE\n",
      "ROW_NUMBER() OVER (\n",
      "    PARTITION BY Trait, Metric\n",
      "    ORDER BY Trait, Metric ASC\n",
      ") =1\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1626:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT\n",
      "ROW_NUMBER() OVER (\n",
      "    PARTITION BY Trait, Metric\n",
      "    ORDER BY Trait, Metric ASC\n",
      ") AS Rank, *\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "QUALIFY\n",
      "ROW_NUMBER() OVER (\n",
      "    PARTITION BY Trait, Metric\n",
      "    ORDER BY Trait, Metric ASC\n",
      ") =1\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1627:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT\n",
      "ROW_NUMBER() OVER (\n",
      "    PARTITION BY Trait, Metric\n",
      "    ORDER BY Trait, Metric ASC\n",
      ") AS Rank, *\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "QUALIFY(\n",
      "ROW_NUMBER() OVER (\n",
      "    PARTITION BY Trait, Metric\n",
      "    ORDER BY Trait, Metric ASC\n",
      ")) =1\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1628:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT\n",
      "ROW_NUMBER() OVER (\n",
      "    PARTITION BY Trait, Metric\n",
      "    ORDER BY Trait, Metric ASC\n",
      ") AS Rank, *\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "WHERE\n",
      "Rank = 1\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1629:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT\n",
      "ROW_NUMBER() OVER (\n",
      "    PARTITION BY Trait, Metric\n",
      "    ORDER BY Trait, Metric ASC\n",
      ") AS Rank, *\n",
      "--Trait, Metric, Count(*)\n",
      "FROM (\n",
      "    SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "    CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "    , A.*, B.confidence AS inverse_rule_confidence\n",
      "    FROM dfr AS A\n",
      "    LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      "\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1630:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT *\n",
      "FROM\n",
      "    (SELECT\n",
      "    ROW_NUMBER() OVER (\n",
      "        PARTITION BY Trait, Metric\n",
      "        ORDER BY Trait, Metric ASC\n",
      "    ) AS Rank, *\n",
      "    FROM (\n",
      "        SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "        CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "        , A.*, B.confidence AS inverse_rule_confidence\n",
      "        FROM dfr AS A\n",
      "        LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      ") AS T\n",
      "WHERE T.Rank = 1\n",
      "\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1631:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT antescedent, consequent, support, confidence, lift, `inverse_rule_confidence:1` AS inverse_rule_confidence\n",
      "FROM\n",
      "    (SELECT\n",
      "    ROW_NUMBER() OVER (\n",
      "        PARTITION BY Trait, Metric\n",
      "        ORDER BY Trait, Metric ASC\n",
      "    ) AS Rank, *\n",
      "    FROM (\n",
      "        SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "        CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "        , A.*, B.confidence AS inverse_rule_confidence\n",
      "        FROM dfr AS A\n",
      "        LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      ") AS T\n",
      "WHERE T.Rank = 1\n",
      "\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1632:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "sql_query=\"\"\"\n",
      "SELECT antescedent, consequent, support, confidence, lift, `inverse_rule_confidence:1` AS inverse_rule_confidence\n",
      "FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "    FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "        CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "        , A.*, B.confidence AS inverse_rule_confidence\n",
      "        FROM dfr AS A\n",
      "        LEFT JOIN dfr AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "    ) AS A\n",
      ") AS T\n",
      "WHERE T.Rank = 1\n",
      "\"\"\"\n",
      "ps.sqldf(sql_query, locals())\n",
      "43/1633:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, `inverse_rule_confidence:1` AS inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df.sort_values('lift', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1634:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "43/1635:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, `inverse_rule_confidence:1` AS inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df.sort_values('lift', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1636:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "43/1637:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, `inverse_rule_confidence` AS inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df.sort_values('lift', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1638:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "43/1639:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df.sort_values('lift', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1640:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, False)\n",
      "dfr\n",
      "43/1641:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, True)\n",
      "dfr\n",
      "43/1642:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df.sort_values(abs(1-'lift'), ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1643:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, True)\n",
      "dfr\n",
      "43/1644:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df.sort_values('lift', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1645:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, True)\n",
      "dfr\n",
      "43/1646:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1647:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.01, 0.01, True)\n",
      "dfr\n",
      "43/1648:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.00, 0.01, True)\n",
      "dfr\n",
      "43/1649:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.00001, 0.01, True)\n",
      "dfr\n",
      "43/1650:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.3)\n",
      "dfr\n",
      "43/1651:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.3, False)\n",
      "dfr\n",
      "43/1652:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.3, True)\n",
      "dfr\n",
      "43/1653:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "dfr\n",
      "43/1654: dfr = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "43/1655:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1656:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1657:\n",
      "dfr = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "dfr\n",
      "43/1658: dfr = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "43/1659:\n",
      "dfr = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "dfr\n",
      "43/1660:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "#estimates_assoc_df.head()\n",
      "#estimates_df, priorities_df, states_df,valid_users_times\n",
      "priorities_df.head()\n",
      "43/1661:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "#estimates_assoc_df.head()\n",
      "#estimates_df, priorities_df, states_df,valid_users_times\n",
      "priorities_assoc_df = = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1662:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "#estimates_assoc_df.head()\n",
      "#estimates_df, priorities_df, states_df,valid_users_times\n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1663:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "#estimates_assoc_df.head()\n",
      "#estimates_df, priorities_df, states_df,valid_users_times\n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_df.head()\n",
      "43/1664:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "#estimates_assoc_df.head()\n",
      "#estimates_df, priorities_df, states_df,valid_users_times\n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "\n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1665:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "#estimates_assoc_df.head()\n",
      "#estimates_df, priorities_df, states_df,valid_users_times\n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "valid_users_times.head()\n",
      "43/1666:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "#estimates_assoc_df.head()\n",
      "#estimates_df, priorities_df, states_df,valid_users_times\n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_assoc_df = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','time_spending_category','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1667:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_assoc_df = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','time_spending_category','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1668:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "states_res = get_association_rules_df(states_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "43/1669:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.25, 0.6, True)\n",
      "43/1670:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.25, 0.6, True)\n",
      "43/1671:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.25, 0.6, True)\n",
      "43/1672:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.25, 0.6, True)\n",
      "times_res.head()\n",
      "43/1673:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.1, True)\n",
      "times_res.head()\n",
      "43/1674:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.3, True)\n",
      "times_res.head()\n",
      "43/1675:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.3, True)\n",
      "times_res.head()\n",
      "43/1676:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.4, True)\n",
      "times_res.head()\n",
      "43/1677:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.35, True)\n",
      "times_res.head()\n",
      "43/1678:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1679:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "print(times_res.head())\n",
      "43/1680:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, True)\n",
      "print(times_res.head())\n",
      "43/1681:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "print(times_res.head())\n",
      "43/1682:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "times_res.head()\n",
      "43/1683:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, True)\n",
      "times_res.head()\n",
      "43/1684:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "times_res.head()\n",
      "43/1685:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    \n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True:\n",
      "        res_df=res_df[res_df['antescedent']!='[]']\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1686:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "times_res.head()\n",
      "43/1687:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, True)\n",
      "times_res.head()\n",
      "43/1688:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "times_res.head()\n",
      "43/1689:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    \n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True:\n",
      "        res_df=res_df[res_df['antescedent']!='[]']\n",
      "    else:\n",
      "        res_df = res_df\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1690:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "times_res.head()\n",
      "43/1691:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, True)\n",
      "times_res.head()\n",
      "43/1692:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, True)\n",
      "times_res.head()\n",
      "43/1693:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "times_res.head()\n",
      "43/1694:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    \n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    #if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]']\n",
      "    #else: res_df = res_df\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1695:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "times_res.head()\n",
      "43/1696:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, True)\n",
      "times_res.head()\n",
      "43/1697:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "times_res.head()\n",
      "43/1698:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, True)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "estimates_res.head()\n",
      "43/1699:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "estimates_res.head()\n",
      "43/1700:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "estimates_res.head()\n",
      "43/1701:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "estimates_res.head()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/1702:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "times_res.head()\n",
      "43/1703:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, True)\n",
      "times_res.head()\n",
      "43/1704:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]']\n",
      "    else: res_df = res_df\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1705:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, True)\n",
      "times_res.head()\n",
      "43/1706:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "times_res.head()\n",
      "43/1707:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, False)\n",
      "times_res.head()\n",
      "43/1708:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, True)\n",
      "times_res.head()\n",
      "43/1709:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    \n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1710:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0000001, 0.0000001, True)\n",
      "times_res.head()\n",
      "43/1711:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1712:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res.head()\n",
      "43/1713:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]'].copy()\n",
      "    else: res_df = res_df.copy()\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1714:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res.head()\n",
      "43/1715:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1716:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    \n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1717:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1718:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals()).copy()\n",
      "    \n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1719:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1720:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals()).copy(deep=True)\n",
      "    if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]'].copy()\n",
      "    else: res_df = res_df.copy()\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1721:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1722:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals()).copy(deep=True)\n",
      "    #if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]'].copy()\n",
      "    #else: res_df = res_df.copy()\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1723:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1724:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals()).copy(deep=True)\n",
      "    res_df = res_df\n",
      "    #if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]'].copy()\n",
      "    #else: res_df = res_df.copy()\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1725:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1726:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals()).copy(deep=True)\n",
      "    res_df = res_df\n",
      "    if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]'].copy()\n",
      "    else: res_df = res_df.copy()\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1727:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1728:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res.head()\n",
      "43/1729:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1730:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    \n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals()).copy(deep=True)\n",
      "    res_df = res_df\n",
      "    if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]'].copy()\n",
      "    else: res_df = res_df.copy()\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1731:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    \n",
      "\n",
      "    \n",
      "    res_df = ps.sqldf(sql_query, locals()).copy(deep=True)\n",
      "    res_df = res_df\n",
      "    if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]'].copy()\n",
      "    else: res_df = res_df.copy()\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1732:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res.head()\n",
      "43/1733:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    \n",
      "    res_df = ps.sqldf(sql_query, locals()).copy(deep=True)\n",
      "    if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]'].copy()\n",
      "    else: res_df = res_df.copy()\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1734:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    \n",
      "    res_df = ps.sqldf(sql_query, locals()).copy(deep=True)\n",
      "    if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]'].copy(deep=True)\n",
      "    else: res_df = res_df.copy()\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1735:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res.head()\n",
      "43/1736:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res.head()\n",
      "times_assoc_df.head()\n",
      "43/1737:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res.head()\n",
      "times_assoc_df[['time_spending_category', 'Is_Open']].drop_duplicates()\n",
      "43/1738:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res.head()\n",
      "times_assoc_df.groupby(['time_spending_category', 'Is_Open']).agg('user':'count')\n",
      "43/1739:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res.head()\n",
      "times_assoc_df.groupby(['time_spending_category', 'Is_Open']).agg({'user':'count'})\n",
      "43/1740:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res.head()\n",
      "43/1741:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    \n",
      "    #res_df = ps.sqldf(sql_query, locals()).copy(deep=True)\n",
      "    #if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]'].copy(deep=True)\n",
      "    #else: res_df = res_df.copy()\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1742:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res.head()\n",
      "43/1743:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    \n",
      "    #res_df = ps.sqldf(sql_query, locals()).copy(deep=True)\n",
      "    #if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]'].copy(deep=True)\n",
      "    #else: res_df = res_df.copy()\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1744:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    \n",
      "    #res_df = ps.sqldf(sql_query, locals()).copy(deep=True)\n",
      "    #if filter_twosets==True: res_df=res_df[res_df['antescedent']!='[]'].copy(deep=True)\n",
      "    #else: res_df = res_df.copy()\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1745:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res.head()\n",
      "43/1746:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1747:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1748:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1749:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1750:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1751:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res.head()\n",
      "43/1752:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res\n",
      "43/1753:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)\n",
      "    \n",
      "    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True:\n",
      "        res_df=res_df[res_df['antescedent']!='[]']\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1754:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, True)\n",
      "times_res\n",
      "43/1755:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.001, 0.001, False)\n",
      "times_res\n",
      "43/1756:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.00001, 0.00001, False)\n",
      "times_res\n",
      "43/1757:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.6, False)\n",
      "times_res\n",
      "43/1758:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.5, False)\n",
      "times_res\n",
      "43/1759:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.3, False)\n",
      "times_res\n",
      "43/1760:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.3, True)\n",
      "times_res\n",
      "43/1761:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.33, True)\n",
      "times_res\n",
      "43/1762:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.4, True)\n",
      "times_res\n",
      "43/1763:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.34, True)\n",
      "times_res\n",
      "43/1764:\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "times_res\n",
      "43/1765:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "estimates_res_all\n",
      "43/1766:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.333, True)\n",
      "43/1767:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.333, True)\n",
      "times_res_all\n",
      "43/1768:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.34, True)\n",
      "times_res_all\n",
      "43/1769:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.33, True)\n",
      "times_res_all\n",
      "43/1770:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.25, 0.6, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "43/1771:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "43/1772:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_assoc_df.head()\n",
      "43/1773:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_assoc_df.head()\n",
      "43/1774:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_assoc_df.head()\n",
      "\n",
      "items = priorities_assoc_df[['priority','Is_Open']].values\n",
      "#    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "#    rules_list = list(rules)\n",
      "43/1775:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_assoc_df.head()\n",
      "\n",
      "items = priorities_assoc_df[['priority','Is_Open']].values\n",
      "#    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "#    rules_list = list(rules)\n",
      "items\n",
      "43/1776:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_assoc_df.head()\n",
      "\n",
      "items = priorities_assoc_df[['priority','Is_Open']].values\n",
      "    rules = apriori(items, min_support = 0.1, min_confidence = 0.1)\n",
      "#    rules_list = list(rules)\n",
      "rules\n",
      "43/1777:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_assoc_df.head()\n",
      "\n",
      "items = priorities_assoc_df[['priority','Is_Open']].values\n",
      "rules = apriori(items, min_support = 0.1, min_confidence = 0.1)\n",
      "#    rules_list = list(rules)\n",
      "rules\n",
      "43/1778:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_assoc_df.head()\n",
      "\n",
      "items = priorities_assoc_df[['priority','Is_Open']].values\n",
      "rules = apriori(items, min_support = 0.1, min_confidence = 0.1)\n",
      "rules_list = list(rules)\n",
      "rules_list\n",
      "43/1779:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_assoc_df.head()\n",
      "43/1780:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_assoc_df.head()\n",
      "\n",
      "priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==True]\n",
      "43/1781:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_assoc_df.head()\n",
      "\n",
      "priorities_assoc_df[pd.isnull(priorities_assoc_df['Is_OPen'])==True]\n",
      "43/1782:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "#priorities_res = get_association_rules_df(priorities_assoc_df,'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_assoc_df.head()\n",
      "\n",
      "priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==True]\n",
      "43/1783:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False],'priority','Is_Open', 0.001, 0.001, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "43/1784:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.3, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "43/1785:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.2, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "43/1786:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.1, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "43/1787:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "43/1788:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, False)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "43/1789:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "43/1790:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "43/1791:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_res[pd.isnull(states_res['status'])==True]\n",
      "43/1792:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]\n",
      "43/1793:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True].shape[0]\n",
      "43/1794:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df.shape[0]\n",
      "43/1795:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]\n",
      "43/1796:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "43/1797:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "log_dt.head()\n",
      "43/1798:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "changelog.head()\n",
      "43/1799:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "changelog[changelog['author']=='cwilliams'].head()\n",
      "43/1800:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status')].head()\n",
      "43/1801:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status')]\n",
      "43/1802:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "43/1803:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "states_df\n",
      "43/1804:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "states_df[states_df['user']=='cwilliams']\n",
      "43/1805:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "states_df[states_df['user']=='cwilliams'].groupby('status').count()\n",
      "43/1806:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "states_df[states_df['user']=='cwilliams']\n",
      "43/1807:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "#states_df[states_df['user']=='cwilliams']\n",
      "#states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "#    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1808:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "#states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "#states_df[states_df['user']=='cwilliams']\n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1809:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "#states_df[states_df['user']=='cwilliams']\n",
      "#states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1810:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "#states_df[states_df['user']=='cwilliams']\n",
      "#states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "#    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1811:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "#states_df[states_df['user']=='cwilliams']\n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1812:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "#states_df[states_df['user']=='cwilliams']\n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user'].unique()\n",
      "43/1813:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "#states_df[states_df['user']=='cwilliams']\n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]['user']\n",
      "43/1814:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "#states_df[states_df['user']=='cwilliams']\n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]\n",
      "43/1815:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "#states_df[states_df['user']=='cwilliams']\n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')\n",
      "states_assoc_df[pd.isnull(states_assoc_df['status'])==True]\n",
      "43/1816:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "states_df[states_df['user']=='cwilliams']\n",
      "#states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')\n",
      "#states_assoc_df[pd.isnull(states_assoc_df['status'])==True]\n",
      "43/1817:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "\n",
      "\n",
      "#changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "states_df[(states_df['user']=='cwilliams') & pd.isnull(states_df['status'])==True]\n",
      "#states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')\n",
      "#states_assoc_df[pd.isnull(states_assoc_df['status'])==True]\n",
      "43/1818:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "\n",
      "\n",
      "changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['toString']=='In Progress')]\n",
      "\n",
      "#states_df[(states_df['user']=='cwilliams') & pd.isnull(states_df['status'])==True]\n",
      "#states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')\n",
      "#states_assoc_df[pd.isnull(states_assoc_df['status'])==True]\n",
      "43/1819:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "\n",
      "\n",
      "changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['key']=='TIMOB-23198')]\n",
      "\n",
      "#states_df[(states_df['user']=='cwilliams') & pd.isnull(states_df['status'])==True]\n",
      "#states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')\n",
      "#states_assoc_df[pd.isnull(states_assoc_df['status'])==True]\n",
      "43/1820:\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'status','todo', 'inprogress', 'done')\n",
      "43/1821:\n",
      "def categorical_metric(field, multiple_fields, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    if len(multiple_fields)>0:\n",
      "        log_dt = changelog[(changelog['field'].isin(multiple_fields))]\n",
      "    else:\n",
      "        log_dt = changelog[(changelog['field'].isin([field]))]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field_in_issues]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[log_dt.toString.isin(cat1_list), field]=cat1_label\n",
      "    log_dt.loc[log_dt.toString.isin(cat2_list), field]=cat2_label\n",
      "    log_dt.loc[log_dt.toString.isin(cat3_list), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\", \n",
      "    CASE WHEN COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) THEN '\"\"\"+cat1_label+\"\"\"'\n",
      "         WHEN COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) AND COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN '\"\"\"+cat2_label+\"\"\"'\n",
      "         WHEN COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN '\"\"\"+cat3_label+\"\"\"'\n",
      "    END AS metric\n",
      "    FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1822:\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'status','todo', 'inprogress', 'done')\n",
      "43/1823:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "\n",
      "\n",
      "changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['key']=='TIMOB-23198')]\n",
      "\n",
      "states_df[(states_df['user']=='cwilliams') & pd.isnull(states_df['status'])==True]\n",
      "43/1824:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "\n",
      "\n",
      "changelog[(changelog['author']=='cwilliams') & (changelog['field']=='status') & (changelog['key']=='TIMOB-23198')]\n",
      "\n",
      "states_df[(states_df['user']=='cwilliams') & pd.isnull(states_df['status'])==False]\n",
      "43/1825:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_df[(states_df['user']=='cwilliams') & pd.isnull(states_df['status'])==False]\n",
      "43/1826:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "#states_res = get_association_rules_df(states_assoc_df,'status','Is_Open', 0.25, 0.6, True)\n",
      "priorities_res\n",
      "states_df[(states_df['user']=='cwilliams') & (pd.isnull(states_df['status'])==False)]\n",
      "43/1827:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_df[(states_df['user']=='cwilliams') & (pd.isnull(states_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.25, 0.6, True)\n",
      "states_res\n",
      "43/1828:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.25, 0.6, True)\n",
      "states_res\n",
      "43/1829:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.001, 0.001, True)\n",
      "states_res\n",
      "43/1830:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.000001, 0.000001, True)\n",
      "states_res\n",
      "43/1831:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.000001, 0.000001, False)\n",
      "states_res\n",
      "43/1832:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.000000000000001, 0.0000000000001, False)\n",
      "states_res\n",
      "43/1833:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 1, 0.0000000000001, False)\n",
      "states_res\n",
      "43/1834:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.0000000000000000001, 0.000000000000000001, False)\n",
      "states_res\n",
      "43/1835:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.0000000000000000001, 0.000000000000000001, False)\n",
      "states_res\n",
      "states_assoc_df.groupby('Is_Open').count()\n",
      "43/1836:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.0000000000000000001, 0.000000000000000001, False)\n",
      "states_res\n",
      "states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1837:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.0000000000000000001, 0.000000000000000001, False)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1838:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.25, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1839:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1840:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1841:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.2, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1842:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1843:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.2, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1844:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.400, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1845:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.2, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1846:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank = 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True:\n",
      "        res_df=res_df[res_df['antescedent']!='[]']\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_association_rules_nogroup_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1847:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_nogroup_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.2, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1848:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_nogroup_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1849:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True:\n",
      "        res_df=res_df[res_df['antescedent']!='[]']\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_association_rules_nogroup_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1850:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1851:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            FULL JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True:\n",
      "        res_df=res_df[res_df['antescedent']!='[]']\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_association_rules_nogroup_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1852:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1853:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "            UNION\n",
      "            \n",
      "            SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS B\n",
      "            LEFT JOIN assoc_df AS A ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True:\n",
      "        res_df=res_df[res_df['antescedent']!='[]']\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_association_rules_nogroup_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1854:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1855:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, False)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1856:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1857:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_group_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1858:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_grouped_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1859:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1860:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_nogroup_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1861:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "            UNION\n",
      "            \n",
      "            SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON B.antescedent = A.consequent AND B.consequent = A.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True:\n",
      "        res_df=res_df[res_df['antescedent']!='[]']\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_association_rules_nogroup_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1862:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_nogroup_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1863:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1864:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "            UNION\n",
      "            \n",
      "            SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS B\n",
      "            LEFT JOIN assoc_df AS A ON B.antescedent = A.consequent AND B.consequent = A.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True:\n",
      "        res_df=res_df[res_df['antescedent']!='[]']\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_association_rules_nogroup_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1865:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1866:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "            UNION\n",
      "            \n",
      "            SELECT CASE WHEN substr(B.antescedent, 3, 3) IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.antescedent) >3 AND substr(A.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN A.antescedent ELSE B.consequent END AS Metric\n",
      "            , B.*, A.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS B\n",
      "            LEFT JOIN assoc_df AS A ON B.antescedent = A.consequent AND B.consequent = A.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True:\n",
      "        res_df=res_df[res_df['antescedent']!='[]']\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_association_rules_nogroup_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1867:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1868:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "            UNION\n",
      "            \n",
      "            SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , B.*, A.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS B\n",
      "            LEFT JOIN assoc_df AS A ON B.antescedent = A.consequent AND B.consequent = A.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    if filter_twosets==True:\n",
      "        res_df=res_df[res_df['antescedent']!='[]']\n",
      "    return res_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def get_association_rules_nogroup_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1869:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1870:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_nogroup_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1871:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations():\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "            UNION\n",
      "            \n",
      "            SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , B.*, A.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS B\n",
      "            LEFT JOIN assoc_df AS A ON B.antescedent = A.consequent AND B.consequent = A.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1872:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_nogroup_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1873:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_nogroup_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1874:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations():\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "            UNION\n",
      "            \n",
      "            SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , B.*, A.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS B\n",
      "            LEFT JOIN assoc_df AS A ON B.antescedent = A.consequent AND B.consequent = A.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1875:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_nogroup_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1876:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.4, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1877:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.001, 0.001, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1878:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.15, 0.3, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1879:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "            UNION\n",
      "            \n",
      "            SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , B.*, A.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS B\n",
      "            LEFT JOIN assoc_df AS A ON B.antescedent = A.consequent AND B.consequent = A.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1880:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.15, 0.3, True)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1881:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.15, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1882:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "            UNION ALL\n",
      "            \n",
      "            SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , B.*, A.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS B\n",
      "            LEFT JOIN assoc_df AS A ON B.antescedent = A.consequent AND B.consequent = A.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1883:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.15, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1884:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(B.antescedent) >3 AND substr(B.antescedent, 3, 3) NOT IN ('Yes', 'No_') THEN B.antescedent ELSE B.consequent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1885:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.15, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1886:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1887:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.15, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1888:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1889:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1890:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.15, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1891:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1892:\n",
      "#estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.33, True)\n",
      "times_res_all\n",
      "43/1893:\n",
      "#estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(states_res, 'time_spending_category', 0.1, 0.33, True)\n",
      "times_res_all\n",
      "43/1894:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=group_associations(get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets))\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1895:\n",
      "#estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.33, True)\n",
      "times_res_all\n",
      "43/1896:\n",
      "#estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.34, True)\n",
      "times_res_all\n",
      "43/1897:\n",
      "#estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.9, True)\n",
      "times_res_all\n",
      "43/1898:\n",
      "#estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "times_res_all\n",
      "43/1899:\n",
      "#estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.4, True)\n",
      "times_res_all\n",
      "43/1900:\n",
      "#estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.3, True)\n",
      "times_res_all\n",
      "43/1901:\n",
      "#estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "times_res_all\n",
      "43/1902:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "times_res_all\n",
      "43/1903:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df, 'priority', 0.1, 0.5, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df, 'status', 0.1, 0.5, True)\n",
      "priorities_res_all\n",
      "43/1904:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)], 'status', 0.1, 0.5, True)\n",
      "priorities_res_all\n",
      "43/1905:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)], 'status', 0.1, 0.1, True)\n",
      "priorities_res_all\n",
      "43/1906:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1907:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             , 'status', 0.1, 0.1, True)\n",
      "priorities_res_all\n",
      "43/1908:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "priorities_res_all\n",
      "43/1909:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.1, 0.1, True)\n",
      "priorities_res_all\n",
      "43/1910:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "            \n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=group_associations(get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets))\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1911:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1912:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.1, 0.1, True)\n",
      "priorities_res_all\n",
      "43/1913:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.1, 0.1, True)\n",
      "priorities_res_all\n",
      "43/1914:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=group_associations(get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets))\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1915:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1916:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1917:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.1, 0.1, True)\n",
      "priorities_res_all\n",
      "43/1918:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "#states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.1, 0.1, True)\n",
      "priorities_res_all\n",
      "43/1919:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "#states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             #,'status', 0.1, 0.1, True)\n",
      "priorities_res_all\n",
      "43/1920:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "#states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             #,'status', 0.1, 0.1, True)\n",
      "#priorities_res_all\n",
      "43/1921:\n",
      "df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)].columns:\n",
      "        if (col!='user') & (col!='status'):\n",
      "            df_=group_associations(get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)], 'status', col, 0.1, 0.3, True))\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "43/1922:\n",
      "df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "for col in states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)].columns:\n",
      "    if (col!='user') & (col!='status'):\n",
      "        df_=group_associations(get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)], 'status', col, 0.1, 0.3, True))\n",
      "        sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "        df = ps.sqldf(sql_query, locals())\n",
      "43/1923:\n",
      "df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "for col in states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)].columns:\n",
      "    if (col!='user') & (col!='status'):\n",
      "        df_ = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)], 'status', col, 0.1, 0.3, True)\n",
      "       # df_=group_associations()\n",
      "        sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "        df = ps.sqldf(sql_query, locals())\n",
      "43/1924:\n",
      "df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "for col in states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)].columns:\n",
      "    if (col!='user') & (col!='status'):\n",
      "        df_ = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)], 'status', col, 0.1, 0.3, True)\n",
      "       # df_=group_associations()\n",
      "   \n",
      "        sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "        df = ps.sqldf(sql_query, locals())\n",
      "43/1925:\n",
      "df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "for col in states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)].columns:\n",
      "    if (col!='user') & (col!='status'):\n",
      "        df_ = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)], 'status', 'Is_Open', 0.1, 0.3, True)\n",
      "       # df_=group_associations()\n",
      "   \n",
      "        sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "        df = ps.sqldf(sql_query, locals())\n",
      "43/1926:\n",
      "df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "for col in states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)].columns:\n",
      "    if (col!='user') & (col!='status'):\n",
      "        df_ = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)], 'status', 'Is_Open', 0.1, 0.3, True)\n",
      "        df_=group_associations(df_)\n",
      "   \n",
      "        sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "        df = ps.sqldf(sql_query, locals())\n",
      "43/1927:\n",
      "df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "for col in states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)].columns:\n",
      "    if (col!='user') & (col!='status'):\n",
      "        df_ = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)], 'status', 'Is_Open', 0.1, 0.3, True)\n",
      "        df_=group_associations(df_)\n",
      "        print(col)\n",
      "   \n",
      "        sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "        df = ps.sqldf(sql_query, locals())\n",
      "43/1928:\n",
      "df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "for col in priorities_assoc_df[(pd.isnull(priorities_assoc_df['status'])==False)].columns:\n",
      "    if (col!='user') & (col!='status'):\n",
      "        df_ = get_association_rules_df(priorities_assoc_df[(pd.isnull(priorities_assoc_df['status'])==False)], 'status', 'Is_Open', 0.1, 0.3, True)\n",
      "        df_=group_associations(df_)\n",
      "        print(col)\n",
      "   \n",
      "        sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "        df = ps.sqldf(sql_query, locals())\n",
      "43/1929:\n",
      "df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "for col in priorities_assoc_df[(pd.isnull(priorities_assoc_df['status'])==False)].columns:\n",
      "    if (col!='user') & (col!='status'):\n",
      "        df_ = get_association_rules_df(priorities_assoc_df[(pd.isnull(priorities_assoc_df['status'])==False)], 'priority', 'Is_Open', 0.1, 0.3, True)\n",
      "        df_=group_associations(df_)\n",
      "        print(col)\n",
      "   \n",
      "        sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "        df = ps.sqldf(sql_query, locals())\n",
      "43/1930:\n",
      "df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "for col in priorities_assoc_df[(pd.isnull(priorities_assoc_df['priority'])==False)].columns:\n",
      "    if (col!='user') & (col!='status'):\n",
      "        df_ = get_association_rules_df(priorities_assoc_df[(pd.isnull(priorities_assoc_df['priority'])==False)], 'priority', 'Is_Open', 0.1, 0.3, True)\n",
      "        df_=group_associations(df_)\n",
      "        print(col)\n",
      "   \n",
      "        sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "        df = ps.sqldf(sql_query, locals())\n",
      "43/1931: states_assoc_df\n",
      "43/1932:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_assoc_df = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','time_spending_category','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1933:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=group_associations(get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets))\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1934:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1935:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.1, 0.1, True)\n",
      "priorities_res_all\n",
      "43/1936:\n",
      "valid_users_times['time_spending_category'] = np.nan\n",
      "\n",
      "valid_users_times.loc[(valid_users_times['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( valid_users_times['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)\n",
      "                  , 'time_spending_category'] = 'time_medium'\n",
      "\n",
      "valid_users_times.loc[valid_users_times['minutes_spent']>=_top_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='time_high'\n",
      "\n",
      "valid_users_times.loc[valid_users_times['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='time_low'\n",
      "# check the actual values\n",
      "valid_users_times[['time_spending_category', 'user']].groupby('time_spending_category').count()\n",
      "43/1937: valid_users_times.head()\n",
      "43/1938:\n",
      "import pandasql as ps\n",
      "\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user From valid_users_times) AS U\n",
      "    Left Join ( SELECT user, count(*) AS low_timespent_tasks\n",
      "    FROM valid_users_times AS F WHERE time_spending_category = 'time_low' Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join (SELECT user, count(*) AS medium_timespent_tasks\n",
      "    FROM valid_users_times AS F WHERE time_spending_category = 'time_medium' Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join ( SELECT user, count(*) AS high_timespent_tasks\n",
      "    FROM valid_users_times AS F WHERE time_spending_category = 'time_high' Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_time_agg = ps.sqldf(q1, locals())\n",
      "print(valid_user_time_agg.shape[0])\n",
      "valid_user_time_agg.head()\n",
      "43/1939:\n",
      "import pandasql as ps\n",
      "\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user From valid_users_times) AS U\n",
      "    Left Join ( SELECT user, count(*) AS low_timespent_tasks\n",
      "    FROM valid_users_times AS F WHERE time_spending_category = 'time_low' Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join (SELECT user, count(*) AS medium_timespent_tasks\n",
      "    FROM valid_users_times AS F WHERE time_spending_category = 'time_medium' Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join ( SELECT user, count(*) AS high_timespent_tasks\n",
      "    FROM valid_users_times AS F WHERE time_spending_category = 'time_high' Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_time_agg = ps.sqldf(q1, locals())\n",
      "print(valid_user_time_agg.shape[0])\n",
      "valid_user_time_agg.head()\n",
      "43/1940:\n",
      "print(valid_user_time_agg[(pd.isnull(valid_user_time_agg['low_timespent_tasks'])==False)\n",
      "                         & (pd.isnull(valid_user_time_agg['medium_timespent_tasks'])==False)\n",
      "                         & (pd.isnull(valid_user_time_agg['high_timespent_tasks'])==False)].shape[0])\n",
      "\n",
      "print(valid_user_time_agg[(pd.isnull(valid_user_time_agg['low_timespent_tasks'])==False)\n",
      "                         | (pd.isnull(valid_user_time_agg['medium_timespent_tasks'])==False)\n",
      "                         | (pd.isnull(valid_user_time_agg['high_timespent_tasks'])==False)].shape[0])\n",
      "43/1941:\n",
      "def categorical_metric(field, multiple_fields, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    if len(multiple_fields)>0:\n",
      "        log_dt = changelog[(changelog['field'].isin(multiple_fields))]\n",
      "    else:\n",
      "        log_dt = changelog[(changelog['field'].isin([field]))]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field_in_issues]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[log_dt.toString.isin(cat1_list), field]=cat1_label\n",
      "    log_dt.loc[log_dt.toString.isin(cat2_list), field]=cat2_label\n",
      "    log_dt.loc[log_dt.toString.isin(cat3_list), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\", \n",
      "    CASE WHEN COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) THEN '\"\"\"+cat1_label+\"\"\"'\n",
      "         WHEN COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) AND COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN '\"\"\"+cat2_label+\"\"\"'\n",
      "         WHEN COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN '\"\"\"+cat3_label+\"\"\"'\n",
      "    END AS metric\n",
      "    FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "43/1942:\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','','',_todo,'status_todo',_inprogress,'status_inprogress',_done,'status_done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'status','status_todo', 'status_inprogress', 'status_done')\n",
      "43/1943:\n",
      "print(states_df.shape[0], \n",
      "      states_df.drop_duplicates().shape[0],\n",
      "      states_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "states_df.head()\n",
      "43/1944:\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['status_done'])==False)\n",
      "                         & (pd.isnull(states_df_agg['status_todo'])==False)\n",
      "                         & (pd.isnull(states_df_agg['status_inprogress'])==False)].shape[0])\n",
      "\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['status_done'])==False)\n",
      "                         | (pd.isnull(states_df_agg['status_todo'])==False)\n",
      "                         | (pd.isnull(states_df_agg['status_inprogress'])==False)].shape[0])\n",
      "states_df_agg.head()\n",
      "43/1945:\n",
      "changelog[changelog['field']=='priority'][['toString', 'author']].groupby(\n",
      "    'toString').count().sort_values('author', ascending=False)\n",
      "43/1946:\n",
      "_high = ['High', 'Critical', 'Blocker']\n",
      "_medium = ['Medium', 'Major']\n",
      "_low = ['Low', 'Minor', 'None', 'Trivial', 'To be reviewed']\n",
      "\n",
      "priorities_df = categorical_metric('priority','','','','',_high,'priority_high',_medium,'priority_medium',_low,'priority_low')\n",
      "priorities_df_agg = categorical_metric_agg(priorities_df, 'priority','priority_high', 'priority_medium', 'priority_low')\n",
      "43/1947:\n",
      "print(priorities_df.shape[0], \n",
      "      priorities_df.drop_duplicates().shape[0],\n",
      "      priorities_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "priorities_df.head()\n",
      "43/1948:\n",
      "#field_in_issues, field_exclusion_filter, values_filter\n",
      "\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "story_fields = ['Story Points', 'Actual Story Points']\n",
      "field = 'StoryPoints'\n",
      "e_low = ['0.5', '1', '2']\n",
      "e_medium = ['3', '5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "\n",
      "estimates_df = categorical_metric(field,story_fields,'fields.issuetype.name','Epic',story_points,e_high,'StoryPoints_high',e_medium,'StoryPoints_medium',e_low,'StoryPoints_low')\n",
      "estimates_df_agg = categorical_metric_agg(estimates_df, '`'+field+'`','StoryPoints_high', 'StoryPoints_medium', 'StoryPoints_low')\n",
      "43/1949: estimates_df.head()\n",
      "43/1950: print(estimates_df.shape[0],estimates_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "43/1951:\n",
      "print(estimates_df_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "estimates_df_agg.head(100)\n",
      "43/1952:\n",
      "print(estimates_df_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "estimates_df_agg.head()\n",
      "43/1953:\n",
      "print(estimates_df_agg[(pd.isnull(estimates_df_agg['StoryPoints_high'])==False)\n",
      "                         & (pd.isnull(estimates_df_agg['StoryPoints_medium'])==False)\n",
      "                         & (pd.isnull(estimates_df_agg['StoryPoints_low'])==False)].shape[0])\n",
      "\n",
      "print(estimates_df_agg[(pd.isnull(estimates_df_agg['StoryPoints_high'])==False)\n",
      "                         | (pd.isnull(estimates_df_agg['StoryPoints_medium'])==False)\n",
      "                         | (pd.isnull(estimates_df_agg['StoryPoints_low'])==False)].shape[0])\n",
      "43/1954:\n",
      "from apyori import apriori\n",
      "from orangecontrib.associate import *\n",
      "43/1955:\n",
      "user_personalities['Is_Open']           =user_personalities['openness'].apply(          lambda x: 'Yes_openness' if x>np.median(user_personalities['openness'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes_conscientiousness' if x>np.median(user_personalities['conscientiousness']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['extraversion'].apply(      lambda x: 'Yes_extraversion' if x>np.median(user_personalities['extraversion'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes_agreeableness' if x>np.median(user_personalities['agreeableness'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes_neuroticism' if x>np.median(user_personalities['neuroticism'])       else 'No_neuroticism')\n",
      "43/1956:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_assoc_df = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','time_spending_category','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/1957:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=group_associations(get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets))\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1958:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1959:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str.replace(str(list(order_statistics)),'[','')\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=group_associations(get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets))\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1960:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1961:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str.replace(str.replace(str(list(order_statistics)),'[',''),']','')\n",
      "                elif element_nr == 1:consequent = str.replace(str.replace(str(list(order_statistics)),'[',''),']','')\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=group_associations(get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets))\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1962:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1963:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=group_associations(get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets))\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/1964:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/1965:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.1, 0.1, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT \"\"\"++\"\"\"\"\"\"* FROM estimates_res_all\n",
      "    UNION ALL \n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "43/1966:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.1, 0.1, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df\n",
      "43/1967:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.25, 0.6, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.1, 0.5, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.1, 0.1, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df.head()\n",
      "43/1968:\n",
      "tab = pd.crosstab(res_df['antescedent'], res_df['consequent'], values = res_df['support'])\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/1969:\n",
      "tab = pd.crosstab(res_df['antescedent'], res_df['consequent'], values = res_df['support'], aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/1970:\n",
      "tab = pd.crosstab(res_df['antescedent'], res_df['consequent'], values = res_df['support'], aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(16,8)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/1971:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.0000001, 0.0000001, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df.head()\n",
      "43/1972:\n",
      "tab = pd.crosstab(res_df['antescedent'], res_df['consequent'], values = res_df['support'], aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(16,8)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/1973:\n",
      "tab = pd.crosstab(res_df['antescedent'], res_df['consequent'], values = round(res_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(16,8)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/1974:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/1975:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/1976:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=60)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/1977:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=60)\n",
      "sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/1978:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm, g = sns.set(rc={'figure.figsize':(8,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=60)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "43/1979:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=60)\n",
      "43/1980:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=60)\n",
      "g.plot()\n",
      "43/1981:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=60)\n",
      "g.how()\n",
      "43/1982:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=60)\n",
      "g.show()\n",
      "43/1983:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=60)\n",
      "g.draw()\n",
      "43/1984:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=60)\n",
      "g.plot()\n",
      "43/1985:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1986:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "ax = hm.add_subplot(2,2,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1987:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "ax = rc.add_subplot(2,2,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1988:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "\n",
      "g=add_subplot(2,2,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1989:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "#hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "fig = plt.figure(figsize=(16, 10))\n",
      "ax = fig.add_subplot(2,2,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1990:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "#hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "fig = plt.figure(figsize=(16, 10))\n",
      "ax = fig.add_subplot(2,2,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(2,2,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1991:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "#hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "fig = plt.figure(figsize=(8, 4))\n",
      "ax = fig.add_subplot(2,2,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(2,2,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1992:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "#hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "fig = plt.figure(figsize=(18, 12))\n",
      "ax = fig.add_subplot(2,2,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(2,2,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1993:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "#hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "fig = plt.figure(figsize=(18, 12))\n",
      "ax = fig.add_subplot(2,2,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(2,2,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(2,2,3)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1994:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "#hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "fig = plt.figure(figsize=(18, 12))\n",
      "ax = fig.add_subplot(3,1,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(3,1,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(3,1,3)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1995:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "#hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "fig = plt.figure(figsize=(18, 12))\n",
      "ax = fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1996:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "#hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "fig = plt.figure(figsize=(18, 6))\n",
      "ax = fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1997:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "#hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "fig = plt.figure(figsize=(18, 6))\n",
      "ax = fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "\n",
      "ax = fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=90)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
      "g.plot()\n",
      "43/1998:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "#hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "fig = plt.figure(figsize=(18, 6))\n",
      "ax = fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
      "\n",
      "ax = fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.plot()\n",
      "43/1999:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "#hm = sns.set(rc={'figure.figsize':(8,4)})\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "ax = fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
      "\n",
      "ax = fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.plot()\n",
      "43/2000:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "g = fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
      "\n",
      "ax = fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.plot()\n",
      "43/2001:\n",
      "tab_df = priorities_res_all\n",
      "tab = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
      "\n",
      "ax = fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.plot()\n",
      "43/2002:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
      "\n",
      "ax = fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.plot()\n",
      "43/2003:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Greens', fmt='g')\n",
      "g.plot()\n",
      "43/2004:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Greens', fmt='g')\n",
      "g.title('')\n",
      "g.plot()\n",
      "43/2005:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Greens', fmt='g')\n",
      "g.title('as')\n",
      "g.plot()\n",
      "43/2006:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "ax = fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Greens', fmt='g')\n",
      "g.plot()\n",
      "43/2007:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Greens', fmt='g')\n",
      "g.plot()\n",
      "43/2008:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 3))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Greens', fmt='g')\n",
      "g.plot()\n",
      "43/2009:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 3.5))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Greens', fmt='g')\n",
      "g.plot()\n",
      "43/2010:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Greens', fmt='g')\n",
      "g.plot()\n",
      "43/2011:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "g.plot()\n",
      "43/2012:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGnRd', fmt='g')\n",
      "g.plot()\n",
      "43/2013:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGnBl', fmt='g')\n",
      "g.plot()\n",
      "43/2014:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Purples', fmt='g')\n",
      "g.plot()\n",
      "43/2015:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='RdGnBl', fmt='g')\n",
      "g.plot()\n",
      "43/2016:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Reds', fmt='g')\n",
      "g.plot()\n",
      "43/2017:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Purples', fmt='g')\n",
      "g.plot()\n",
      "43/2018:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='RdGn', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Purples', fmt='g')\n",
      "g.plot()\n",
      "43/2019:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='GnRd', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Purples', fmt='g')\n",
      "g.plot()\n",
      "43/2020:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='RdYl', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Purples', fmt='g')\n",
      "g.plot()\n",
      "43/2021:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='YlRd', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Purples', fmt='g')\n",
      "g.plot()\n",
      "43/2022:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='YlBl', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Purples', fmt='g')\n",
      "g.plot()\n",
      "43/2023:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='GnYl', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Purples', fmt='g')\n",
      "g.plot()\n",
      "43/2024:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='YlGn', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='Purples', fmt='g')\n",
      "g.plot()\n",
      "43/2025:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "g.plot()\n",
      "43/2026:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "g.plot()\n",
      "43/2027:\n",
      "tab_df = priorities_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2028: tab_sup\n",
      "43/2029: tab_conf\n",
      "43/2030: priorities_res_all\n",
      "43/2031:\n",
      "tab_df = times_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2032:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/2033:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/2034:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.3, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df.head()\n",
      "43/2035:\n",
      "tab_df = times_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2036:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.6, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df.head()\n",
      "43/2037:\n",
      "tab_df = times_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2038:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.2, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df.head()\n",
      "43/2039:\n",
      "tab_df = times_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2040:\n",
      "tab_df = times_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2041:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.2, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df.head()\n",
      "43/2042:\n",
      "tab_df = times_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2043:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.5, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df.head()\n",
      "43/2044:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.1, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df.head()\n",
      "43/2045:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.3, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df.head()\n",
      "43/2046:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1], 'confidence': confidence, 'lift':lift}\n",
      "                               , 'inverse_rule_confidence':'-', ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=group_associations(get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets))\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/2047:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, inverse_rule_confidence\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=group_associations(get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets))\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/2048:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/2049:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.3, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df.head()\n",
      "43/2050:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "\n",
      "def group_associations(assoc_df):\n",
      "    sql_query=\"\"\"\n",
      "    SELECT antescedent, consequent, support, confidence, lift, `inverse_rule_confidence:1`\n",
      "    FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Trait, Metric ORDER BY Trait, Metric ASC) AS Rank, *\n",
      "        FROM (SELECT CASE WHEN substr(A.antescedent, 3, 3) IN ('Yes', 'No_') THEN A.antescedent ELSE A.consequent END AS Trait,\n",
      "            CASE WHEN LENGTH(A.consequent) >3 AND substr(A.consequent, 3, 3) NOT IN ('Yes', 'No_') THEN A.consequent ELSE A.antescedent END AS Metric\n",
      "            , A.*, B.confidence AS inverse_rule_confidence\n",
      "            FROM assoc_df AS A\n",
      "            LEFT JOIN assoc_df AS B ON A.antescedent = B.consequent AND A.consequent = B.antescedent\n",
      "        ) AS A\n",
      "    ) AS T\n",
      "    WHERE T.Rank <= 1\n",
      "    \"\"\"\n",
      "    res_df = ps.sqldf(sql_query, locals())\n",
      "    return res_df\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=group_associations(get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets))\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/2051:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.3, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df.head()\n",
      "43/2052:\n",
      "states_res = times_res_all\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2053:\n",
      "tab_df = states_res\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2054: states_res\n",
      "43/2055:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "print(states_res)\n",
      "group_associations(states_res)\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/2056:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.1, 0.333, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.25, 0.6, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.001, 0.15, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.1, 0.3, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/2057:\n",
      "tab_df = states_res\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2058:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0001, 0.0001, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.0001, 0.0001, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.0001, 0.0001, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.0001, 0.0001, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/2059:\n",
      "tab_df = states_res\n",
      "tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "fig = plt.figure(figsize=(18, 4))\n",
      "fig.add_subplot(1,3,1)\n",
      "g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,2)\n",
      "g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "fig.add_subplot(1,3,3)\n",
      "g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2060:\n",
      "def heatmap_rule_stats(df)\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 4))\n",
      "    fig.add_subplot(1,3,1)\n",
      "    g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "    fig.add_subplot(1,3,2)\n",
      "    g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "    fig.add_subplot(1,3,3)\n",
      "    g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2061:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 4))\n",
      "    fig.add_subplot(1,3,1)\n",
      "    g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "\n",
      "    fig.add_subplot(1,3,2)\n",
      "    g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "\n",
      "    fig.add_subplot(1,3,3)\n",
      "    g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2062: heatmap_rule_stats(states_res)\n",
      "43/2063:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 4))\n",
      "    fig.add_subplot(2,2,1)\n",
      "    g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "    fig.add_subplot(2,2,2)\n",
      "    g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "    fig.add_subplot(3,2,3)\n",
      "    g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2064: heatmap_rule_stats(states_res)\n",
      "43/2065:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 8))\n",
      "    fig.add_subplot(2,2,1)\n",
      "    g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "    fig.add_subplot(2,2,2)\n",
      "    g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "    fig.add_subplot(3,2,3)\n",
      "    g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2066: heatmap_rule_stats(states_res)\n",
      "43/2067:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 8))\n",
      "    fig.add_subplot(2,2,1)\n",
      "    g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "    fig.add_subplot(2,2,2)\n",
      "    g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "    fig.add_subplot(2,2,3)\n",
      "    g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2068: heatmap_rule_stats(states_res)\n",
      "43/2069:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    fig.add_subplot(2,2,1)\n",
      "    g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "    fig.add_subplot(2,2,2)\n",
      "    g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "    fig.add_subplot(2,2,3)\n",
      "    g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "43/2070: heatmap_rule_stats(states_res)\n",
      "43/2071:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    fig.add_subplot(2,2,1)\n",
      "    g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "    fig.add_subplot(2,2,2)\n",
      "    g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "    fig.add_subplot(2,2,3)\n",
      "    g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "    \n",
      "    g.xticks(rotation=45)\n",
      "    plt.title(_title, fontsize=20)\n",
      "    plt.ylabel(_ylabel, fontsize=15)\n",
      "    plt.xlabel(_xlabel, fontsize=15)\n",
      "    plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "43/2072: heatmap_rule_stats(states_res)\n",
      "43/2073:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    fig.add_subplot(2,2,1)\n",
      "    g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "    fig.add_subplot(2,2,2)\n",
      "    g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "    fig.add_subplot(2,2,3)\n",
      "    g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "    \n",
      "    plt.xticks(rotation=45)\n",
      "    plt.title('asd', fontsize=20)\n",
      "    plt.ylabel('y;b', fontsize=15)\n",
      "    plt.xlabel('xlb', fontsize=15)\n",
      "    plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "43/2074: heatmap_rule_stats(states_res)\n",
      "43/2075:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    fig.add_subplot(2,2,1)\n",
      "    g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "    plt.title('asd', fontsize=20)\n",
      "    plt.ylabel('y;b', fontsize=15)\n",
      "    plt.xlabel('xlb', fontsize=15)\n",
      "    plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "    \n",
      "    fig.add_subplot(2,2,2)\n",
      "    g = sns.heatmap(tab_conf, annot=True, cmap='Purples', fmt='g')\n",
      "    plt.title('asd', fontsize=20)\n",
      "    plt.ylabel('y;b', fontsize=15)\n",
      "    plt.xlabel('xlb', fontsize=15)\n",
      "    plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "    \n",
      "    fig.add_subplot(2,2,3)\n",
      "    g = sns.heatmap(tab_lif, annot=True, cmap='YlGn', fmt='g')\n",
      "    plt.title('asd', fontsize=20)\n",
      "    plt.ylabel('y;b', fontsize=15)\n",
      "    plt.xlabel('xlb', fontsize=15)\n",
      "    plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "43/2076: heatmap_rule_stats(states_res)\n",
      "43/2077:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig = plt.figure(figsize=(18, 12))\n",
      "        fig.add_subplot(2,2,1)\n",
      "        if var=='support':tab_sup else tab_conf\n",
      "        g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title('asd', fontsize=20)\n",
      "        plt.ylabel('y;b', fontsize=15)\n",
      "        plt.xlabel('xlb', fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "43/2078:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig = plt.figure(figsize=(18, 12))\n",
      "        fig.add_subplot(2,2,1)\n",
      "        if var=='support':tab_sup else: tab_conf\n",
      "        g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title('asd', fontsize=20)\n",
      "        plt.ylabel('y;b', fontsize=15)\n",
      "        plt.xlabel('xlb', fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "43/2079:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig = plt.figure(figsize=(18, 12))\n",
      "        fig.add_subplot(2,2,1)\n",
      "        if var=='support':tab_sup \n",
      "        else: tab_conf\n",
      "        g = sns.heatmap(tab_sup, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title('asd', fontsize=20)\n",
      "        plt.ylabel('y;b', fontsize=15)\n",
      "        plt.xlabel('xlb', fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "43/2080:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig = plt.figure(figsize=(18, 12))\n",
      "        fig.add_subplot(2,2,1)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "43/2081: heatmap_rule_stats(states_res)\n",
      "43/2082:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig = plt.figure(figsize=(18, 12))\n",
      "        fig.add_subplot(2,2,1)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        i=i+1\n",
      "43/2083: heatmap_rule_stats(states_res)\n",
      "43/2084:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig = plt.figure(figsize=(18, 12))\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        i=i+1\n",
      "43/2085: heatmap_rule_stats(states_res)\n",
      "43/2086: heatmap_rule_stats(states_res)\n",
      "43/2087:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig = plt.figure(figsize=(18, 12))\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        i=i+1\n",
      "43/2088:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig = plt.figure(figsize=(18, 12))\n",
      "        fig.add_subplot(2,2,i+1)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        i=i+1\n",
      "43/2089: heatmap_rule_stats(states_res)\n",
      "43/2090:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig = plt.figure(figsize=(18, 12))\n",
      "        fig.add_subplot(2,2,9)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        i=i+1\n",
      "43/2091: heatmap_rule_stats(states_res)\n",
      "43/2092:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig = plt.figure(figsize=(18, 12))\n",
      "        fig.add_subplot(2,2,3)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        i=i+1\n",
      "43/2093:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig = plt.figure(figsize=(18, 12))\n",
      "        fig.add_subplot(2,2,4)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        i=i+1\n",
      "43/2094: heatmap_rule_stats(states_res)\n",
      "43/2095:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        \n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        i=i+1\n",
      "43/2096:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        i=i+1\n",
      "43/2097: heatmap_rule_stats(states_res)\n",
      "43/2098:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5, wspace = 0.1)\n",
      "        i=i+1\n",
      "43/2099: heatmap_rule_stats(states_res)\n",
      "43/2100:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5, wspace = 0.2)\n",
      "        i=i+1\n",
      "43/2101: heatmap_rule_stats(states_res)\n",
      "43/2102:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.55, wspace = 0.25)\n",
      "        i=i+1\n",
      "43/2103:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.55, wspace = 0.25)\n",
      "        i=i+1\n",
      "43/2104: heatmap_rule_stats(states_res)\n",
      "43/2105:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.6, wspace = 0.25)\n",
      "        i=i+1\n",
      "43/2106: heatmap_rule_stats(states_res)\n",
      "43/2107:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "43/2108:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0001, 0.0001, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.0001, 0.0001, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.0001, 0.0001, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.0001, 0.0001, True)\n",
      "states_res\n",
      "#states_assoc_df[(states_assoc_df['user']=='cwilliams') & (pd.isnull(states_assoc_df['status'])==False)].groupby('Is_Open').count()\n",
      "43/2109:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.1, 0.3, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "res_df.head()\n",
      "43/2110:\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    \n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 12))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.6, wspace = 0.25)\n",
      "        i=i+1\n",
      "43/2111:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.0000001, 0.0000001, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "sql_query=\"\"\"\n",
      "    SELECT * FROM times_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM estimates_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM priorities_res_all\n",
      "    UNION ALL\n",
      "    SELECT * FROM states_res_all\n",
      "    \"\"\"\n",
      "res_df = ps.sqldf(sql_query, locals())\n",
      "43/2112:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.0000001, 0.0000001, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "43/2113: heatmap_rule_stats(states_res_all)\n",
      "43/2114:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0001, 0.0001, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.0001, 0.0001, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.0001, 0.0001, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.0001, 0.0001, True)\n",
      "43/2115:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 14))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap='Greens', fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "        i=i+1\n",
      "43/2116:\n",
      "times_res = get_association_rules_df(times_assoc_df,'time_spending_category','Is_Open', 0.0001, 0.0001, True)\n",
      "estimates_res = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_Open', 0.0001, 0.0001, False)\n",
      "priorities_res = get_association_rules_df(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                          ,'priority','Is_Open', 0.0001, 0.0001, True)\n",
      "states_res = get_association_rules_df(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                      ,'status','Is_Open', 0.0001, 0.0001, True)\n",
      "43/2117:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.0000001, 0.0000001, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "43/2118: heatmap_rule_stats(states_res_all)\n",
      "43/2119:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    colors = ['YlGnBu', 'Blues', 'BuPu']\n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 14))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "        i=i+1\n",
      "43/2120: heatmap_rule_stats(states_res_all)\n",
      "43/2121:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df):\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 14))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "        i=i+1\n",
      "43/2122: heatmap_rule_stats(states_res_all)\n",
      "43/2123: heatmap_rule_stats(states_res)\n",
      "43/2124: times_assoc_df\n",
      "43/2125:\n",
      "user_personalities['Is_open']           =user_personalities['openness'].apply(          lambda x: 'Yes_openness' if x>np.median(user_personalities['openness'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes_conscientiousness' if x>np.median(user_personalities['conscientiousness']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['extraversion'].apply(      lambda x: 'Yes_extraversion' if x>np.median(user_personalities['extraversion'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes_agreeableness' if x>np.median(user_personalities['agreeableness'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes_neuroticism' if x>np.median(user_personalities['neuroticism'])       else 'No_neuroticism')\n",
      "43/2126:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_assoc_df = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','time_spending_category','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/2127:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_assoc_df = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','time_spending_category','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/2128:\n",
      "user_personalities['Is_open']           =user_personalities['openness'].apply(          lambda x: 'Yes_openness' if x>np.median(user_personalities['openness'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes_conscientiousness' if x>np.median(user_personalities['conscientiousness']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['extraversion'].apply(      lambda x: 'Yes_extraversion' if x>np.median(user_personalities['extraversion'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes_agreeableness' if x>np.median(user_personalities['agreeableness'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes_neuroticism' if x>np.median(user_personalities['neuroticism'])       else 'No_neuroticism')\n",
      "43/2129:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_assoc_df = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','time_spending_category','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/2130:\n",
      "estimates_res_all = get_all_metric_associations(estimates_assoc_df, 'StoryPoints', 0.0000001, 0.0000001, True)\n",
      "times_res_all = get_all_metric_associations(times_assoc_df, 'time_spending_category', 0.0000001, 0.0000001, True)\n",
      "priorities_res_all = get_all_metric_associations(priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "                                                 , 'priority', 0.0000001, 0.0000001, True)\n",
      "states_res_all = get_all_metric_associations(states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "                                             ,'status', 0.0000001, 0.0000001, True)\n",
      "43/2131: heatmap_rule_stats(times_res_all)\n",
      "43/2132: times_res_allhead()\n",
      "43/2133: times_res_all.head()\n",
      "43/2134: heatmap_rule_stats(times_res_all)\n",
      "43/2135: heatmap_rule_stats(states_o)\n",
      "43/2136:\n",
      "times_o = get_association_rules_df(times_assoc_df,'time_spending_category','Is_open', 0.0001, 0.0001, True)\n",
      "times_c = get_association_rules_df(times_assoc_df,'time_spending_category','Is_conscientious', 0.0001, 0.0001, True)\n",
      "times_e = get_association_rules_df(times_assoc_df,'time_spending_category','Is_extravert', 0.0001, 0.0001, True)\n",
      "times_a = get_association_rules_df(times_assoc_df,'time_spending_category','Is_agreeable', 0.0001, 0.0001, True)\n",
      "times_n = get_association_rules_df(times_assoc_df,'time_spending_category','Is_neurotic', 0.0001, 0.0001, True)\n",
      "\n",
      "estimates_o = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_open', 0.0001, 0.0001, True)\n",
      "estimates_c = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_conscientious', 0.0001, 0.0001, True)\n",
      "estimates_e = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_extravert', 0.0001, 0.0001, True)\n",
      "estimates_a = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_agreeable', 0.0001, 0.0001, True)\n",
      "estimates_n = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_neurotic', 0.0001, 0.0001, True)\n",
      "#some rows have null in priority\n",
      "priorities_assoc_df = priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "priorities_o = get_association_rules_df(priorities_assoc_df,'priority','Is_open', 0.0001, 0.0001, True)\n",
      "priorities_c = get_association_rules_df(priorities_assoc_df,'priority','Is_conscientious', 0.0001, 0.0001, True)\n",
      "priorities_e = get_association_rules_df(priorities_assoc_df,'priority','Is_extravert', 0.0001, 0.0001, True)\n",
      "priorities_a = get_association_rules_df(priorities_assoc_df,'priority','Is_agreeable', 0.0001, 0.0001, True)\n",
      "priorities_n = get_association_rules_df(priorities_assoc_df,'priority','Is_neurotic', 0.0001, 0.0001, True)\n",
      "#some rows have nulls in status\n",
      "states_assoc_df = states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "states_o = get_association_rules_df(states_assoc_df,'status','Is_open', 0.0001, 0.0001, True)\n",
      "states_c = get_association_rules_df(states_assoc_df,'status','Is_conscientious', 0.0001, 0.0001, True)\n",
      "states_e = get_association_rules_df(states_assoc_df,'status','Is_extravert', 0.0001, 0.0001, True)\n",
      "states_a = get_association_rules_df(states_assoc_df,'status','Is_agreeable', 0.0001, 0.0001, True)\n",
      "states_n = get_association_rules_df(states_assoc_df,'status','Is_neurotic', 0.0001, 0.0001, True)\n",
      "43/2137: heatmap_rule_stats(states_o)\n",
      "43/2138: states_o.head()\n",
      "43/2139:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df, min_support=0, min_confidence=0):\n",
      "    df[(df['min_support']>=min_support) & (df['confidence']>=min_confidence)]\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 14))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "        i=i+1\n",
      "43/2140: heatmap_rule_stats(states_o)\n",
      "43/2141:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df, min_support=0, min_confidence=0):\n",
      "    df[(df['min_support']>=min_support) & (df['confidence']>=min_confidence)]\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 14))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "        i=i+1\n",
      "43/2142: heatmap_rule_stats(states_o)\n",
      "43/2143:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df, min_support=None, min_confidence=None):\n",
      "    df[(df['min_support']>=min_support) & (df['confidence']>=min_confidence)]\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 14))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "        i=i+1\n",
      "43/2144: heatmap_rule_stats(states_o)\n",
      "43/2145:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df, min_support_=None, min_confidence_=None):\n",
      "    df[(df['min_support']>=min_support) & (df['confidence']>=min_confidence)]\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 14))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "        i=i+1\n",
      "heatmap_rule_stats(states_o)\n",
      "43/2146:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df, min_support_=0, min_confidence_=0):\n",
      "    df[(df['support']>=min_support_) & (df['confidence']>=min_confidence_)]\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 14))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "        i=i+1\n",
      "heatmap_rule_stats(states_o)\n",
      "43/2147:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df, min_support_=0, min_confidence_=0):\n",
      "    df[(df['support']>=min_support_) & (df['confidence']>=min_confidence_)]\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 14))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "        i=i+1\n",
      "43/2148: heatmap_rule_stats(states_o)\n",
      "43/2149: heatmap_rule_stats(states_o,0,0.5)\n",
      "43/2150: heatmap_rule_stats(states_o,0.5,0.5)\n",
      "43/2151: heatmap_rule_stats(states_o,min_support_=0.5,min_confidence=0.5)\n",
      "43/2152: heatmap_rule_stats(states_o,min_support_=0.5,min_confidence_=0.5)\n",
      "43/2153:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df, min_support_=None, min_confidence_=None):\n",
      "    df[(df['support']>=min_support_) & (df['confidence']>=min_confidence_)]\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 14))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "        i=i+1\n",
      "43/2154: heatmap_rule_stats(states_o,min_support_=0.5,min_confidence_=0.5)\n",
      "43/2155:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df, min_support_=None, min_confidence_=None):\n",
      "    min_support_=min_support_ or 0\n",
      "    min_confidence_ = min_confidence_ or 0\n",
      "    df[(df['support']>=min_support_) & (df['confidence']>=min_confidence_)]\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 14))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "        i=i+1\n",
      "43/2156: heatmap_rule_stats(states_o,min_support_=0.5,min_confidence_=0.5)\n",
      "43/2157:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df, min_support_=0, min_confidence_=0):\n",
      "    df = df[(df['support']>=min_support_) & (df['confidence']>=min_confidence_)]\n",
      "    tab_df = df\n",
      "    tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "    tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "    tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "    colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "    i=1\n",
      "    fig = plt.figure(figsize=(18, 14))\n",
      "    for var in (['support', 'confidence', 'lift']):\n",
      "        fig.add_subplot(2,2,i)\n",
      "        if var=='support':src_tab = tab_sup \n",
      "        elif var=='confidence':src_tab = tab_conf\n",
      "        elif var=='lift':src_tab = tab_lif\n",
      "        g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "        plt.title(var, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "        i=i+1\n",
      "43/2158: heatmap_rule_stats(states_o,min_support_=0.5,min_confidence_=0.5)\n",
      "43/2159: heatmap_rule_stats(states_o,min_support_=0.1,min_confidence_=0.5)\n",
      "43/2160:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df, min_support_=0, min_confidence_=0):\n",
      "    df = df[(df['support']>=min_support_) & (df['confidence']>=min_confidence_)]\n",
      "    if df.shape[0]>0:\n",
      "        tab_df = df\n",
      "        tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "        tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "        tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "        colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "        i=1\n",
      "        fig = plt.figure(figsize=(18, 14))\n",
      "        for var in (['support', 'confidence', 'lift']):\n",
      "            fig.add_subplot(2,2,i)\n",
      "            if var=='support':src_tab = tab_sup \n",
      "            elif var=='confidence':src_tab = tab_conf\n",
      "            elif var=='lift':src_tab = tab_lif\n",
      "            g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "            plt.title(var, fontsize=15)\n",
      "            plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "            i=i+1\n",
      "    else\n",
      "        print('No associations match min support and confidence criteria')\n",
      "43/2161:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df, min_support_=0, min_confidence_=0):\n",
      "    df = df[(df['support']>=min_support_) & (df['confidence']>=min_confidence_)]\n",
      "    if df.shape[0]>0:\n",
      "        tab_df = df\n",
      "        tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "        tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "        tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "        colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "        i=1\n",
      "        fig = plt.figure(figsize=(18, 14))\n",
      "        for var in (['support', 'confidence', 'lift']):\n",
      "            fig.add_subplot(2,2,i)\n",
      "            if var=='support':src_tab = tab_sup \n",
      "            elif var=='confidence':src_tab = tab_conf\n",
      "            elif var=='lift':src_tab = tab_lif\n",
      "            g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "            plt.title(var, fontsize=15)\n",
      "            plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "            i=i+1\n",
      "    else:\n",
      "        print('No associations match min support and confidence criteria')\n",
      "43/2162: heatmap_rule_stats(states_o,min_support_=0.1,min_confidence_=0.5)\n",
      "43/2163: heatmap_rule_stats(states_o,min_support_=0.5,min_confidence_=0.5)\n",
      "43/2164: heatmap_rule_stats(states_o,min_support_=0.1,min_confidence_=0.3)\n",
      "43/2165: heatmap_rule_stats(states_o)\n",
      "43/2166: heatmap_rule_stats(states_o,min_support_=0.1,min_confidence_=0.3)\n",
      "43/2167: heatmap_rule_stats(states_o,min_support_=0.1,min_confidence_=0.4)\n",
      "43/2168: heatmap_rule_stats(priorities_n)\n",
      "43/2169: heatmap_rule_stats(priorities_n,min_support_=0.1,min_confidence_=0.3)\n",
      "43/2170: heatmap_rule_stats(time_o,min_support_=0.1,min_confidence_=0.3)\n",
      "43/2171: heatmap_rule_stats(times_o,min_support_=0.1,min_confidence_=0.3)\n",
      "43/2172: heatmap_rule_stats(estimates_o,min_support_=0.1,min_confidence_=0.3)\n",
      "43/2173: heatmap_rule_stats(estimates_o,min_support_=0.1,min_confidence_=0.1)\n",
      "43/2174: times_assoc_df\n",
      "43/2175: times_assoc_df\n",
      "43/2176: times_assoc_df\n",
      "43/2177: times_assoc_df.head()\n",
      "43/2178:\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category, Is_open  FROM times_assoc_df\n",
      "UNION ALL SELECT StoryPoints, Is_open FROM estimates_assoc_df\n",
      "UNION ALL SELECT priorit, Is_openy FROM priorities_assoc_df\n",
      "UNION ALL SELECT status, Is_open FROM states_assoc_df\n",
      "\"\"\"\n",
      "o_all = ps.sqldf(sql_query, locals())\n",
      "o_all\n",
      "43/2179:\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category, Is_open  FROM times_assoc_df\n",
      "UNION ALL SELECT StoryPoints, Is_open FROM estimates_assoc_df\n",
      "UNION ALL SELECT priorit, Is_openy FROM priorities_assoc_df\n",
      "UNION ALL SELECT status, Is_open FROM states_assoc_df\n",
      "\"\"\"\n",
      "o_all = ps.sqldf(query, locals())\n",
      "o_all\n",
      "43/2180:\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category, Is_open  FROM times_assoc_df\n",
      "UNION ALL SELECT StoryPoints, Is_open FROM estimates_assoc_df\n",
      "UNION ALL SELECT priority, Is_openy FROM priorities_assoc_df\n",
      "UNION ALL SELECT status, Is_open FROM states_assoc_df\n",
      "\"\"\"\n",
      "o_all = ps.sqldf(query, locals())\n",
      "o_all\n",
      "43/2181:\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category, Is_open  FROM times_assoc_df\n",
      "UNION ALL SELECT StoryPoints, Is_open FROM estimates_assoc_df\n",
      "UNION ALL SELECT priority, Is_open FROM priorities_assoc_df\n",
      "UNION ALL SELECT status, Is_open FROM states_assoc_df\n",
      "\"\"\"\n",
      "o_all = ps.sqldf(query, locals())\n",
      "o_all\n",
      "43/2182:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df WHERE time_spending_category IS NOT NULL\n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df WHERE StoryPoints IS NOT NULL\n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df WHERE priority IS NOT NULL\n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df WHERE status IS NOT NULL\n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "\n",
      "o_all = trait_all_metrics('Is_Open')\n",
      "c_all = trait_all_metrics('Is_conscientious')\n",
      "e_all = trait_all_metrics('Is_extravert')\n",
      "a_all = trait_all_metrics('Is_agreeable')\n",
      "n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2183:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df WHERE time_spending_category IS NOT NULL\n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df WHERE StoryPoints IS NOT NULL\n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df WHERE priority IS NOT NULL\n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df WHERE status IS NOT NULL\n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "\n",
      "o_all = trait_all_metrics('Is_Open')\n",
      "c_all = trait_all_metrics('Is_conscientious')\n",
      "e_all = trait_all_metrics('Is_extravert')\n",
      "a_all = trait_all_metrics('Is_agreeable')\n",
      "n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2184:\n",
      "times_o = get_association_rules_df(times_assoc_df,'time_spending_category','Is_open', 0.0001, 0.0001, True)\n",
      "times_c = get_association_rules_df(times_assoc_df,'time_spending_category','Is_conscientious', 0.0001, 0.0001, True)\n",
      "times_e = get_association_rules_df(times_assoc_df,'time_spending_category','Is_extravert', 0.0001, 0.0001, True)\n",
      "times_a = get_association_rules_df(times_assoc_df,'time_spending_category','Is_agreeable', 0.0001, 0.0001, True)\n",
      "times_n = get_association_rules_df(times_assoc_df,'time_spending_category','Is_neurotic', 0.0001, 0.0001, True)\n",
      "\n",
      "estimates_o = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_open', 0.0001, 0.0001, True)\n",
      "estimates_c = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_conscientious', 0.0001, 0.0001, True)\n",
      "estimates_e = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_extravert', 0.0001, 0.0001, True)\n",
      "estimates_a = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_agreeable', 0.0001, 0.0001, True)\n",
      "estimates_n = get_association_rules_df(estimates_assoc_df,'StoryPoints','Is_neurotic', 0.0001, 0.0001, True)\n",
      "#some rows have null in priority\n",
      "priorities_assoc_df = priorities_assoc_df[pd.isnull(priorities_assoc_df['priority'])==False]\n",
      "priorities_o = get_association_rules_df(priorities_assoc_df,'priority','Is_open', 0.0001, 0.0001, True)\n",
      "priorities_c = get_association_rules_df(priorities_assoc_df,'priority','Is_conscientious', 0.0001, 0.0001, True)\n",
      "priorities_e = get_association_rules_df(priorities_assoc_df,'priority','Is_extravert', 0.0001, 0.0001, True)\n",
      "priorities_a = get_association_rules_df(priorities_assoc_df,'priority','Is_agreeable', 0.0001, 0.0001, True)\n",
      "priorities_n = get_association_rules_df(priorities_assoc_df,'priority','Is_neurotic', 0.0001, 0.0001, True)\n",
      "#some rows have nulls in status\n",
      "states_assoc_df = states_assoc_df[(pd.isnull(states_assoc_df['status'])==False)]\n",
      "states_o = get_association_rules_df(states_assoc_df,'status','Is_open', 0.0001, 0.0001, True)\n",
      "states_c = get_association_rules_df(states_assoc_df,'status','Is_conscientious', 0.0001, 0.0001, True)\n",
      "states_e = get_association_rules_df(states_assoc_df,'status','Is_extravert', 0.0001, 0.0001, True)\n",
      "states_a = get_association_rules_df(states_assoc_df,'status','Is_agreeable', 0.0001, 0.0001, True)\n",
      "states_n = get_association_rules_df(states_assoc_df,'status','Is_neurotic', 0.0001, 0.0001, True)\n",
      "43/2185:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df WHERE time_spending_category IS NOT NULL\n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df WHERE StoryPoints IS NOT NULL\n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df WHERE priority IS NOT NULL\n",
      "   \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "\n",
      "o_all = trait_all_metrics('Is_Open')\n",
      "c_all = trait_all_metrics('Is_conscientious')\n",
      "e_all = trait_all_metrics('Is_extravert')\n",
      "a_all = trait_all_metrics('Is_agreeable')\n",
      "n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2186: states_assoc_df\n",
      "43/2187:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "\n",
      "o_all = trait_all_metrics('Is_Open')\n",
      "c_all = trait_all_metrics('Is_conscientious')\n",
      "e_all = trait_all_metrics('Is_extravert')\n",
      "a_all = trait_all_metrics('Is_agreeable')\n",
      "n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2188:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "\n",
      "o_all = trait_all_metrics('Is_Open')\n",
      "c_all = trait_all_metrics('Is_conscientious')\n",
      "e_all = trait_all_metrics('Is_extravert')\n",
      "a_all = trait_all_metrics('Is_agreeable')\n",
      "n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2189:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category FROM times_assoc_df \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "\n",
      "o_all = trait_all_metrics('Is_Open')\n",
      "c_all = trait_all_metrics('Is_conscientious')\n",
      "e_all = trait_all_metrics('Is_extravert')\n",
      "a_all = trait_all_metrics('Is_agreeable')\n",
      "n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2190:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category FROM estimates_n \n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "\n",
      "o_all = trait_all_metrics('Is_Open')\n",
      "c_all = trait_all_metrics('Is_conscientious')\n",
      "e_all = trait_all_metrics('Is_extravert')\n",
      "a_all = trait_all_metrics('Is_agreeable')\n",
      "n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2191:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM estimates_n \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "\n",
      "o_all = trait_all_metrics('Is_Open')\n",
      "c_all = trait_all_metrics('Is_conscientious')\n",
      "e_all = trait_all_metrics('Is_extravert')\n",
      "a_all = trait_all_metrics('Is_agreeable')\n",
      "n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2192:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "\n",
      "o_all = trait_all_metrics('Is_Open')\n",
      "c_all = trait_all_metrics('Is_conscientious')\n",
      "e_all = trait_all_metrics('Is_extravert')\n",
      "a_all = trait_all_metrics('Is_agreeable')\n",
      "n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2193:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "\n",
      "o_all = trait_all_metrics('Is_Open')\n",
      "c_all = trait_all_metrics('Is_conscientious')\n",
      "e_all = trait_all_metrics('Is_extravert')\n",
      "a_all = trait_all_metrics('Is_agreeable')\n",
      "n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2194:\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category, Is_Open  FROM times_assoc_df \n",
      "UNION ALL SELECT StoryPoints, Is_Open FROM estimates_assoc_df \n",
      "UNION ALL SELECT priority, Is_Open FROM priorities_assoc_df \n",
      "UNION ALL SELECT status, Is_Open FROM states_assoc_df \n",
      "\"\"\"\n",
      "trait_df = ps.sqldf(query, locals())\n",
      "\n",
      "#o_all = trait_all_metrics('Is_Open')\n",
      "#c_all = trait_all_metrics('Is_conscientious')\n",
      "#e_all = trait_all_metrics('Is_extravert')\n",
      "#a_all = trait_all_metrics('Is_agreeable')\n",
      "#n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2195:\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df \n",
      "\"\"\"\n",
      "trait_df = ps.sqldf(query, locals())\n",
      "\n",
      "#o_all = trait_all_metrics('Is_Open')\n",
      "#c_all = trait_all_metrics('Is_conscientious')\n",
      "#e_all = trait_all_metrics('Is_extravert')\n",
      "#a_all = trait_all_metrics('Is_agreeable')\n",
      "#n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2196:\n",
      "trait = 'Is_open'\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df \n",
      "\"\"\"\n",
      "trait_df = ps.sqldf(query, locals())\n",
      "\n",
      "#o_all = trait_all_metrics('Is_Open')\n",
      "#c_all = trait_all_metrics('Is_conscientious')\n",
      "#e_all = trait_all_metrics('Is_extravert')\n",
      "#a_all = trait_all_metrics('Is_agreeable')\n",
      "#n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2197:\n",
      "trait = 'Is_open'\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df \n",
      "\"\"\"\n",
      "trait_df = ps.sqldf(query, locals())\n",
      "trait_df.head()\n",
      "#o_all = trait_all_metrics('Is_open')\n",
      "#c_all = trait_all_metrics('Is_conscientious')\n",
      "#e_all = trait_all_metrics('Is_extravert')\n",
      "#a_all = trait_all_metrics('Is_agreeable')\n",
      "#n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2198:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    trait_df.head()\n",
      "#o_all = trait_all_metrics('Is_open')\n",
      "#c_all = trait_all_metrics('Is_conscientious')\n",
      "#e_all = trait_all_metrics('Is_extravert')\n",
      "#a_all = trait_all_metrics('Is_agreeable')\n",
      "#n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2199:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    trait_df.head()\n",
      "o_all = trait_all_metrics('Is_open')\n",
      "#c_all = trait_all_metrics('Is_conscientious')\n",
      "#e_all = trait_all_metrics('Is_extravert')\n",
      "#a_all = trait_all_metrics('Is_agreeable')\n",
      "#n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2200:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    trait_df.head()\n",
      "#o_all = trait_all_metrics('Is_open')\n",
      "#c_all = trait_all_metrics('Is_conscientious')\n",
      "e_all = trait_all_metrics('Is_extravert')\n",
      "#a_all = trait_all_metrics('Is_agreeable')\n",
      "#n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2201:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "#o_all = trait_all_metrics('Is_open')\n",
      "#c_all = trait_all_metrics('Is_conscientious')\n",
      "e_all = trait_all_metrics('Is_extravert')\n",
      "#a_all = trait_all_metrics('Is_agreeable')\n",
      "#n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2202:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df \n",
      "    \"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "#o_all = trait_all_metrics('Is_open')\n",
      "#c_all = trait_all_metrics('Is_conscientious')\n",
      "#e_all = trait_all_metrics('Is_extravert')\n",
      "#a_all = trait_all_metrics('Is_agreeable')\n",
      "#n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2203: a\n",
      "43/2204: e_all\n",
      "43/2205: e_all\n",
      "43/2206: e_all = trait_all_metrics('Is_extravert')\n",
      "43/2207:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, Is_open  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, Is_open FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, Is_open FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, Is_open FROM states_assoc_df\"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "#o_all = trait_all_metrics('Is_open')\n",
      "#c_all = trait_all_metrics('Is_conscientious')\n",
      "#e_all = trait_all_metrics('Is_extravert')\n",
      "#a_all = trait_all_metrics('Is_agreeable')\n",
      "#n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2208:\n",
      "def trait_all_metrics(trait):\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category, Is_open  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, Is_open FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, Is_open FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, Is_open FROM states_assoc_df\"\"\"\n",
      "    trait_df = ps.sqldf(query, locals())\n",
      "    return trait_df\n",
      "o_all = trait_all_metrics('Is_open')\n",
      "#c_all = trait_all_metrics('Is_conscientious')\n",
      "#e_all = trait_all_metrics('Is_extravert')\n",
      "#a_all = trait_all_metrics('Is_agreeable')\n",
      "#n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2209:\n",
      "query=\"\"\"\n",
      "    SELECT time_spending_category, Is_open  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, Is_open FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, Is_open FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, Is_open FROM states_assoc_df\"\"\"\n",
      "trait_df = ps.sqldf(query, locals())\n",
      "trait_df.head()\n",
      "43/2210:\n",
      "\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df\"\"\"\n",
      "trait = 'Is_open'\n",
      "o_all = ps.sqldf(query, locals())\n",
      "#c_all = trait_all_metrics('Is_conscientious')\n",
      "#e_all = trait_all_metrics('Is_extravert')\n",
      "#a_all = trait_all_metrics('Is_agreeable')\n",
      "#n_all = trait_all_metrics('Is_neurotic')\n",
      "43/2211:\n",
      "\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category, \"\"\"+trait+\"\"\"  FROM times_assoc_df \n",
      "UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df\"\"\"\n",
      "trait = 'Is_open'\n",
      "o_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_conscientious'\n",
      "c_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_extravert'\n",
      "e_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_agreeable'\n",
      "a_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_neurotic'\n",
      "n_all = ps.sqldf(query, locals())\n",
      "43/2212: o_all\n",
      "43/2213: .head(o_all\n",
      "43/2214: heatmap_rule_stats(o_all,min_support_=0.1,min_confidence_=0.1)\n",
      "43/2215: o_all.head()\n",
      "43/2216: e_all.head()\n",
      "43/2217:\n",
      "\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category AS metric, \"\"\"+trait+\"\"\" as trait  FROM times_assoc_df \n",
      "UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df\"\"\"\n",
      "trait = 'Is_open'\n",
      "o_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_conscientious'\n",
      "c_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_extravert'\n",
      "e_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_agreeable'\n",
      "a_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_neurotic'\n",
      "n_all = ps.sqldf(query, locals())\n",
      "\n",
      "o_all_rules = get_association_rules_df(o_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "c_all_rules = get_association_rules_df(c_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "e_all_rules = get_association_rules_df(e_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "a_all_rules = get_association_rules_df(a_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "n_all_rules = get_association_rules_df(n_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "43/2218: o_all_rules.head()\n",
      "43/2219: o_all_rules.shape[0]\n",
      "43/2220: o_all_rules\n",
      "43/2221: heatmap_rule_stats(o_all_rules,min_support_=0.1,min_confidence_=0.1)\n",
      "43/2222: heatmap_rule_stats(c_all_rules,min_support_=0.1,min_confidence_=0.1)\n",
      "43/2223: o_all_rules.head()\n",
      "43/2224:\n",
      "\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category AS metric, \"\"\"+trait+\"\"\" as trait  FROM times_assoc_df \n",
      "UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df\"\"\"\n",
      "trait = 'Is_open'\n",
      "o_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_conscientious'\n",
      "c_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_extravert'\n",
      "print query\n",
      "e_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_agreeable'\n",
      "a_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_neurotic'\n",
      "n_all = ps.sqldf(query, locals())\n",
      "\n",
      "o_all_rules = get_association_rules_df(o_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "c_all_rules = get_association_rules_df(c_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "e_all_rules = get_association_rules_df(e_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "a_all_rules = get_association_rules_df(a_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "n_all_rules = get_association_rules_df(n_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "43/2225:\n",
      "\n",
      "query=\"\"\"\n",
      "SELECT time_spending_category AS metric, \"\"\"+trait+\"\"\" as trait  FROM times_assoc_df \n",
      "UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df\"\"\"\n",
      "trait = 'Is_open'\n",
      "o_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_conscientious'\n",
      "c_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_extravert'\n",
      "print(query)\n",
      "e_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_agreeable'\n",
      "a_all = ps.sqldf(query, locals())\n",
      "trait = 'Is_neurotic'\n",
      "n_all = ps.sqldf(query, locals())\n",
      "\n",
      "o_all_rules = get_association_rules_df(o_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "c_all_rules = get_association_rules_df(c_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "e_all_rules = get_association_rules_df(e_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "a_all_rules = get_association_rules_df(a_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "n_all_rules = get_association_rules_df(n_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "43/2226:\n",
      "traits = ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
      "for trait in traits:\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category AS metric, \"\"\"+trait+\"\"\" as trait  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df\"\"\"\n",
      "    if trait=='Is_open':o_all = ps.sqldf(query, locals())\n",
      "    elif trait=='Is_conscientious':c_all = ps.sqldf(query, locals())\n",
      "    elif trait=='Is_extravert':e_all = ps.sqldf(query, locals())\n",
      "    elif trait=='Is_agreeable':a_all = ps.sqldf(query, locals())\n",
      "    elif trait=='Is_neurotic':n_all = ps.sqldf(query, locals())\n",
      "    \n",
      "o_all_rules = get_association_rules_df(o_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "c_all_rules = get_association_rules_df(c_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "e_all_rules = get_association_rules_df(e_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "a_all_rules = get_association_rules_df(a_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "n_all_rules = get_association_rules_df(n_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "43/2227: o_all_rules.head()\n",
      "43/2228: c_all_rules.head()\n",
      "43/2229: e_all_rules.head()\n",
      "43/2230: a_all_rules.head()\n",
      "43/2231: n_all_rules.head()\n",
      "43/2232: heatmap_rule_stats(c_all_rules,min_support_=0.1,min_confidence_=0.1)\n",
      "43/2233: heatmap_rule_stats(c_all_rules,min_support_=0.0001,min_confidence_=0000.1)\n",
      "43/2234: heatmap_rule_stats(o_all_rules,min_support_=0.0001,min_confidence_=0000.1)\n",
      "43/2235: heatmap_rule_stats(states_o, min_support_=0.0001,min_confidence_=0000.1)\n",
      "43/2236:\n",
      "#traits = ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
      "#for trait in traits:\n",
      "#    query=\"\"\"\n",
      "#    SELECT time_spending_category AS metric, \"\"\"+trait+\"\"\" as trait  FROM times_assoc_df \n",
      "#    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "#    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "#    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df\"\"\"\n",
      "#    if trait=='Is_open':o_all = ps.sqldf(query, locals())\n",
      "#    elif trait=='Is_conscientious':c_all = ps.sqldf(query, locals())\n",
      "#    elif trait=='Is_extravert':e_all = ps.sqldf(query, locals())\n",
      "#    elif trait=='Is_agreeable':a_all = ps.sqldf(query, locals())\n",
      "#    elif trait=='Is_neurotic':n_all = ps.sqldf(query, locals())\n",
      "#    \n",
      "#o_all_rules = get_association_rules_df(o_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "#c_all_rules = get_association_rules_df(c_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "#e_all_rules = get_association_rules_df(e_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "#a_all_rules = get_association_rules_df(a_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "#n_all_rules = get_association_rules_df(n_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "43/2237:\n",
      "traits = ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
      "for trait in traits:\n",
      "    query=\"\"\"\n",
      "    SELECT time_spending_category AS metric, \"\"\"+trait+\"\"\" as trait  FROM times_assoc_df \n",
      "    UNION ALL SELECT StoryPoints, \"\"\"+trait+\"\"\" FROM estimates_assoc_df \n",
      "    UNION ALL SELECT priority, \"\"\"+trait+\"\"\" FROM priorities_assoc_df \n",
      "    UNION ALL SELECT status, \"\"\"+trait+\"\"\" FROM states_assoc_df\"\"\"\n",
      "    if trait=='Is_open':o_all = ps.sqldf(query, locals())\n",
      "    elif trait=='Is_conscientious':c_all = ps.sqldf(query, locals())\n",
      "    elif trait=='Is_extravert':e_all = ps.sqldf(query, locals())\n",
      "    elif trait=='Is_agreeable':a_all = ps.sqldf(query, locals())\n",
      "    elif trait=='Is_neurotic':n_all = ps.sqldf(query, locals())\n",
      "    \n",
      "o_all_rules = get_association_rules_df(o_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "c_all_rules = get_association_rules_df(c_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "e_all_rules = get_association_rules_df(e_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "a_all_rules = get_association_rules_df(a_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "n_all_rules = get_association_rules_df(n_all,'metric','trait', 0.0001, 0.0001, True)\n",
      "43/2238: print(rpy2.__version__)\n",
      "43/2239:\n",
      "import rpy2\n",
      "print(rpy2.__version__)\n",
      "43/2240: heatmap_rule_stats(e_all_rules, min_support_=0.0001,min_confidence_=0000.1)\n",
      "43/2241: heatmap_rule_stats(e_all_rules, min_support_=0.1,min_confidence_=0.3)\n",
      "43/2242: heatmap_rule_stats(a_all_rules, min_support_=0.1,min_confidence_=0.3)\n",
      "43/2243: heatmap_rule_stats(a_all_rules, min_support_=0.05,min_confidence_=0.3)\n",
      "43/2244: heatmap_rule_stats(a_all_rules, min_support_=0.00001,min_confidence_=0.00001)\n",
      "43/2245: heatmap_rule_stats(c_all_rules, min_support_=0.00001,min_confidence_=0.00001)\n",
      "43/2246: valid_users_times.head()\n",
      "43/2247: times_assoc_df.head()\n",
      "43/2248: valid_users_times.head()\n",
      "43/2249:\n",
      "times_assoc_df = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','minutes_spent', 'time_spending_category','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/2250:\n",
      "times_personalities = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','minutes_spent', 'time_spending_category','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/2251: times_personalities.head()\n",
      "43/2252: times_personalities.head()\n",
      "43/2253:\n",
      "times_personalities = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','minutes_spent', 'time_spending_category','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_personalities.head()\n",
      "43/2254:\n",
      "boxpl(times_personalities\n",
      "      , ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
      "      , ['minutes_spent']\n",
      "      , 'Time spending for each personality trait')\n",
      "43/2255:\n",
      "boxpl(times_personalities\n",
      "      , ['minutes_spent']\n",
      "      , ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
      "      , 'Time spending for each personality trait')\n",
      "43/2256:\n",
      "def boxpl(dt, x_cols, y_cols, title):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(18, 3.5 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(5, 1, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle(title, size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/2257:\n",
      "boxpl(times_personalities\n",
      "      , ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
      "      , ['minutes_spent']\n",
      "      , 'Time spending for each personality trait')\n",
      "43/2258:\n",
      "def boxpl(dt, x_cols, y_cols, title):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(18, 3.5 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(1, 5, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle(title, size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/2259:\n",
      "boxpl(times_personalities\n",
      "      , ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
      "      , ['minutes_spent']\n",
      "      , 'Time spending for each personality trait')\n",
      "43/2260:\n",
      "def boxpl(dt, x_cols, y_cols, title):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(18, 3.5))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(1, 5, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle(title, size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/2261:\n",
      "boxpl(times_personalities\n",
      "      , ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
      "      , ['minutes_spent']\n",
      "      , 'Time spending for each personality trait')\n",
      "43/2262: times_personalities.sort_values('minutes_spent', ascending=True)\n",
      "43/2263: times_personalities.sort_values('minutes_spent', ascending=False)\n",
      "43/2264:\n",
      "times_personalities = pd.merge(user_personalities\n",
      "    , (valid_users_times[valid_users_times['minutes_spent']>0])&(valid_users_times[valid_users_times['minutes_spent']<788400])\n",
      "    , how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','minutes_spent', 'time_spending_category','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_personalities.head()\n",
      "43/2265:\n",
      "times_personalities = pd.merge(user_personalities\n",
      "    , valid_users_times[(valid_users_times['minutes_spent']>0)&(valid_users_times['minutes_spent']<788400)]\n",
      "    , how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','minutes_spent', 'time_spending_category','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_personalities.head()\n",
      "43/2266:\n",
      "def boxpl(dt, x_cols, y_cols, title):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(18, 3.5))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(1, 5, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle(title, size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/2267:\n",
      "boxpl(times_personalities\n",
      "      , ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
      "      , ['minutes_spent']\n",
      "      , 'Time spending for each personality trait')\n",
      "43/2268:\n",
      "times_personalities = pd.merge(user_personalities\n",
      "    , valid_users_times[(valid_users_times['minutes_spent']>0)&(valid_users_times['minutes_spent']<100000)]\n",
      "    , how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','minutes_spent', 'time_spending_category','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_personalities.head()\n",
      "43/2269:\n",
      "boxpl(times_personalities\n",
      "      , ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
      "      , ['minutes_spent']\n",
      "      , 'Time spending for each personality trait')\n",
      "43/2270:\n",
      "def boxpl(dt, x_cols, y_cols, title):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(18, 15))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(1, 5, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle(title, size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "43/2271:\n",
      "boxpl(times_personalities\n",
      "      , ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
      "      , ['minutes_spent']\n",
      "      , 'Time spending for each personality trait')\n",
      "43/2272:\n",
      "times_personalities = pd.merge(user_personalities\n",
      "    , valid_users_times[(valid_users_times['minutes_spent']>0)&(valid_users_times['minutes_spent']<60000)]\n",
      "    , how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','minutes_spent', 'time_spending_category','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_personalities.head()\n",
      "43/2273:\n",
      "boxpl(times_personalities\n",
      "      , ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']\n",
      "      , ['minutes_spent']\n",
      "      , 'Time spending for each personality trait')\n",
      "43/2274: times_personalities['minutes_spent'].mean()\n",
      "43/2275: times_personalities['minutes_spent'].median()\n",
      "43/2276: times_personalities['minutes_spent'].mean()\n",
      "43/2277:\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    print(times_personalities.groupby(trait).mean())\n",
      "43/2278:\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    print(times_personalities.groupby(trait).agg({'minutes_spent':'mean'}))\n",
      "43/2279:\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    print(times_personalities.groupby(trait).agg({'minutes_spent':'mean','minutes_spent':'median' }))\n",
      "43/2280:\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    print(times_personalities.groupby(trait).agg('minutes_spent':['mean','median']))\n",
      "43/2281:\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    print(times_personalities.groupby(trait).agg('minutes_spent',['mean','median']))\n",
      "43/2282:\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    print(times_personalities.groupby(trait).agg({'minutes_spent',['mean','median']}))\n",
      "43/2283:\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    print(times_personalities.groupby(trait).agg({'minutes_spent',[np.mean,np.median]}))\n",
      "43/2284:\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    print(times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]}))\n",
      "43/2285:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "43/2286:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "43/2287:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    times_personalities.reset_index(level= [0,1,2], inplace=True)\n",
      "times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "43/2288:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    times_personalities.reset_index(level= [0,1], inplace=True)\n",
      "times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "43/2289:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    times_personalities.reset_index(level= [0], inplace=True)\n",
      "times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "43/2290:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    times_personalities.reset_index(level= [0,1,2,3], inplace=True)\n",
      "times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "43/2291:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    times_personalities.reset_index(level= [0,1,2,3,4], inplace=True)\n",
      "times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "43/2292:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    #times_personalities.reset_index(level= [0], inplace=True)\n",
      "times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "43/2293:\n",
      "query=\"\"\"\n",
      "SELECT Is_open, Is_conscientious, Is_extravert, Is_agreeable, Is_neurotic, avg(minutes_spent) \n",
      "FROM times_personalities GROUP BY Is_open, Is_conscientious, Is_extravert, Is_agreeable, Is_neurotic\"\"\"\n",
      "ps.sqldf(query, locals())\n",
      "43/2294:\n",
      "query=\"\"\"\n",
      "SELECT Is_open, avg(minutes_spent) \n",
      "FROM times_personalities GROUP BY Is_open\n",
      "UNION ALL\n",
      "SELECT Is_conscientious, avg(minutes_spent) \n",
      "FROM times_personalities GROUP BY Is_conscientious\n",
      "UNION ALL\n",
      "SELECT Is_extravert, avg(minutes_spent) \n",
      "FROM times_personalities GROUP BY Is_extravert\n",
      "UNION ALL\n",
      "SELECT Is_agreeable, avg(minutes_spent) \n",
      "FROM times_personalities GROUP BY Is_agreeable\n",
      "UNION ALL\n",
      "SELECT Is_neurotic, avg(minutes_spent) \n",
      "FROM times_personalities GROUP BY Is_neurotic\n",
      "\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals())\n",
      "43/2295:\n",
      "query=\"\"\"\n",
      "SELECT Is_open, avg(minutes_spent), median(minutes_spent)\n",
      "FROM times_personalities GROUP BY Is_open\n",
      "UNION ALL\n",
      "SELECT Is_conscientious, avg(minutes_spent), median(minutes_spent)\n",
      "FROM times_personalities GROUP BY Is_conscientious\n",
      "UNION ALL\n",
      "SELECT Is_extravert, avg(minutes_spent), median(minutes_spent) \n",
      "FROM times_personalities GROUP BY Is_extravert\n",
      "UNION ALL\n",
      "SELECT Is_agreeable, avg(minutes_spent), median(minutes_spent) \n",
      "FROM times_personalities GROUP BY Is_agreeable\n",
      "UNION ALL\n",
      "SELECT Is_neurotic, avg(minutes_spent), median(minutes_spent) \n",
      "FROM times_personalities GROUP BY Is_neurotic\n",
      "\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals())\n",
      "43/2296:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    #times_personalities.reset_index(level= [0], inplace=True)\n",
      "    time_stats.append()\n",
      "times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "43/2297:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    \n",
      "times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "43/2298:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    df = times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    query = \"Select * from time_stats union all Select * from df\"\n",
      "    time_stats = ps.sqldf(query, locals())\n",
      "time_stats\n",
      "43/2299:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    df = times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    \n",
      "    query = \"Select \"+trait+\" AS trait, mean, median from time_stats union all Select * from df\"\n",
      "    time_stats = ps.sqldf(query, locals())\n",
      "time_stats\n",
      "43/2300:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    df = times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    \n",
      "    query = \"Select \"+trait+\" AS trait, mean, median from time_stats union all Select \"+trait+\" AS trait, mean, median from df\"\n",
      "    time_stats = ps.sqldf(query, locals())\n",
      "time_stats\n",
      "43/2301:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    df = times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    \n",
      "    query = \"Select \"+trait+\" AS trait, * from time_stats union all Select \"+trait+\" AS trait, * from df\"\n",
      "    time_stats = ps.sqldf(query, locals())\n",
      "time_stats\n",
      "43/2302:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    df = times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    \n",
      "    query = \"Select '\"+trait+\"', * from time_stats union all Select '\"+trait+\"' , * from df\"\n",
      "    time_stats = ps.sqldf(query, locals())\n",
      "time_stats\n",
      "43/2303:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    df = times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    \n",
      "    query = \"Select '\"+trait+\"' as trait, * from time_stats union all Select '\"+trait+\"' as trait , * from df\"\n",
      "    time_stats = ps.sqldf(query, locals())\n",
      "time_stats\n",
      "43/2304:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    df = times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    query = \"Select '\"+trait+\"' as trait, * from time_stats union all Select '\"+trait+\"' as trait , * from df\"\n",
      "    time_stats = ps.sqldf(query, locals())\n",
      "time_stats\n",
      "43/2305:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    df = times_personalities.groupby(trait).agg({'trait':'max','minutes_spent':[np.mean,np.median]})\n",
      "    query = \"Select * from time_stats union all Select * from df\"\n",
      "    time_stats = ps.sqldf(query, locals())\n",
      "time_stats\n",
      "43/2306: times_personalities.head(3)\n",
      "43/2307:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    df = times_personalities.groupby(trait).agg({trait:'max','minutes_spent':[np.mean,np.median]})\n",
      "    query = \"Select * from time_stats union all Select * from df\"\n",
      "    time_stats = ps.sqldf(query, locals())\n",
      "time_stats\n",
      "43/2308:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    df = times_personalities.groupby(trait).agg({trait:'max','minutes_spent':[np.mean,np.median]})\n",
      "    query = \"Select * from time_stats union all Select * from df\"\n",
      "    time_stats = ps.sqldf(query, locals())\n",
      "time_stats\n",
      "43/2309:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    df = times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]})\n",
      "    query = \"Select * from time_stats union all Select * from df\"\n",
      "    time_stats = ps.sqldf(query, locals())\n",
      "time_stats\n",
      "43/2310:\n",
      "time_stats = pd.DataFrame(columns=('trait', 'mean', 'median'))\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    df = times_personalities.groupby(trait).agg({'minutes_spent':[np.mean,np.median]}).reset_index()\n",
      "    \n",
      "    query = \"Select * from time_stats union all Select * from df\"\n",
      "    time_stats = ps.sqldf(query, locals())\n",
      "time_stats\n",
      "43/2311: from scipy.stats import mannwhitneyu\n",
      "43/2312:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "43/2313: times_personalities[substr(times_personalities[trait],0,3)=='Yes']\n",
      "43/2314: times_personalities[substring(times_personalities[trait],0,3)=='Yes']\n",
      "43/2315: times_personalities[str.replace(times_personalities[trait],0,3)=='Yes']\n",
      "43/2316: times_personalities[(times_personalities[trait].apply(lambda x: left(x,3)))=='Yes']\n",
      "43/2317: times_personalities[(times_personalities[trait].apply(lambda x: str.left(x,3)))=='Yes']\n",
      "43/2318: times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']\n",
      "43/2319: times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['Is_neurotic'].unique()\n",
      "43/2320:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    mannwhitneyu(trait_yes,trait_no)\n",
      "43/2321:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    mannwhitneyu(trait_yes,trait_no)\n",
      "43/2322:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    print(mannwhitneyu(trait_yes,trait_no))\n",
      "43/2323:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    print(trait, mannwhitneyu(trait_yes,trait_no))\n",
      "43/2324:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    print(trait,': ', mannwhitneyu(trait_yes,trait_no))\n",
      "43/2325:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    stat,p=mannwhitneyu(trait_yes,trait_no)\n",
      "    print(trait,': ', stat, p)\n",
      "43/2326:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    print(trait,': ', mannwhitneyu(trait_yes,trait_no))\n",
      "43/2327:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    print(trait,': ', statistic, p_value)\n",
      "43/2328:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no,  use_continuity=True)\n",
      "    print(trait,': ', statistic, p_value)\n",
      "43/2329:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no,  use_continuity=False)\n",
      "    print(trait,': ', statistic, p_value)\n",
      "43/2330:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no,  use_continuity=True)\n",
      "    print(trait,': ', statistic, p_value)\n",
      "43/2331:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no,  alternative='two-sided')\n",
      "    print(trait,': ', statistic, p_value)\n",
      "43/2332:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no,  alternative='more')\n",
      "    print(trait,': ', statistic, p_value)\n",
      "43/2333:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no,  alternative='less')\n",
      "    print(trait,': ', statistic, p_value)\n",
      "43/2334:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    print(trait,': ', statistic, p_value)\n",
      "43/2335:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    print(trait,': ', statistic, format(p_value, 'f'))\n",
      "43/2336:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    print(trait,': ', statistic, format(p_value, '.8f'))\n",
      "43/2337:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    print(trait,': ', statistic, format(p_value, '.20f'))\n",
      "43/2338:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    print(trait,': ', statistic, format(p_value, '.28f'))\n",
      "43/2339:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    print(trait,',', (if p>p_value:'equal, ' else:'not equal, ') statistic, format(p_value, '.28f'))\n",
      "43/2340:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    print(trait,',', (if p>p_value:'equal, ' else:'not equal, '), statistic, format(p_value, '.28f'))\n",
      "43/2341:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='equal'\n",
      "    elif p<=p_valueresult ='not equal'\n",
      "    print(trait,',', (), statistic, format(p_value, '.28f'))\n",
      "43/2342:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='equal'\n",
      "    elif p<=p_valueresult ='not equal'\n",
      "    print(trait,',', , statistic, format(p_value, '.28f'))\n",
      "43/2343:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='equal'\n",
      "    elif p<=p_valueresult ='not equal'\n",
      "    print(trait,',', statistic, format(p_value, '.28f'))\n",
      "43/2344:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='equal'\n",
      "    elif p<=p_valueresult: ='not equal'\n",
      "    print(trait,',', statistic, format(p_value, '.28f'))\n",
      "43/2345:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='equal'\n",
      "    elif: p<=p_valueresult ='not equal'\n",
      "    print(trait,',', statistic, format(p_value, '.28f'))\n",
      "43/2346:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='equal'\n",
      "    elif p<=p_value:result ='not equal'\n",
      "    print(trait,',', statistic, format(p_value, '.28f'))\n",
      "43/2347:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='equal'\n",
      "    elif p<=p_value:result ='not equal'\n",
      "    print('personality trait: ',trait, 'datasets are: ',result\n",
      "          , '. U-test results- statistic: ', statistic\n",
      "          , 'p-value: ',format(p_value, '.28f'))\n",
      "43/2348:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='equal'\n",
      "    elif p<=p_value:result ='not equal'\n",
      "    print('personality trait: ',trait, 'datasets are: ',result\n",
      "          , '. \\nU-test results- statistic: ', statistic\n",
      "          , 'p-value: ',format(p_value, '.28f'))\n",
      "43/2349:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='equal'\n",
      "    elif p<=p_value:result ='not equal'\n",
      "    print('personality trait: ',trait, 'datasets are: ',result\n",
      "          , '. \\nU-test results- statistic: ', statistic\n",
      "          , 'p-value: ',format(p_value, '.28f'), '\\n')\n",
      "43/2350:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='different'\n",
      "    elif p<=p_value:result ='equal'\n",
      "    print('personality trait: ',trait, 'datasets are: ',result\n",
      "          , '. \\nU-test results- statistic: ', statistic\n",
      "          , 'p-value: ',format(p_value, '.28f'), '\\n')\n",
      "43/2351:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='different'\n",
      "    elif p<=p_value:result ='equal'\n",
      "    print('personality trait: ',trait, 'datasets are:',result\n",
      "          , '. \\nU-test results- statistic: ', statistic\n",
      "          , 'p-value: ',format(p_value, '.28f'), '\\n')\n",
      "43/2352:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='different'\n",
      "    elif p<=p_value:result ='equal'\n",
      "    print('personality trait: ',trait, 'datasets are:',result\n",
      "          , '. \\nU-test results- statistic: ', statistic\n",
      "          , ';p-value: ',format(p_value, '.28f'), '\\n')\n",
      "43/2353:\n",
      "mannwhitneyu(times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'], \n",
      "            times_personalities[times_personalities['Is_open']=='Yes_openness']['minutes_spent'])\n",
      "p = 0.05\n",
      "for trait in ['Is_open', 'Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']:\n",
      "    trait_yes = times_personalities[(times_personalities[trait].apply(lambda x: x[:3]))=='Yes']['minutes_spent']\n",
      "    trait_no = times_personalities[(times_personalities[trait].apply(lambda x: x[:2]))=='No']['minutes_spent']\n",
      "    statistic,p_value=mannwhitneyu(trait_yes,trait_no)\n",
      "    if p>p_value:result ='different'\n",
      "    elif p<=p_value:result ='equal'\n",
      "    print('personality trait: ',trait, 'datasets are:',result\n",
      "          , '. \\nU-test results- statistic: ', statistic\n",
      "          , ';p-value: ',format(p_value, '.28f'), ';\\n')\n",
      "43/2354: user_personalities['Is_open'].head()\n",
      "43/2355: user_personalities['openness']\n",
      "43/2356: user_personalities['openness'].hist()\n",
      "43/2357: user_personalities['conscientiousness'].hist()\n",
      "43/2358: user_personalities['extraversion'].hist()\n",
      "43/2359: user_personalities['conscientiousness'].hist()\n",
      "43/2360: user_personalities['agreeableness'].hist()\n",
      "43/2361: user_personalities['neuroticism'].hist()\n",
      "43/2362: user_personalities['extraversion'].hist()\n",
      "43/2363:\n",
      "#user_personalities['extraversion'].hist()\n",
      "user_personalities.corr()\n",
      "43/2364:\n",
      "user_personalities['extraversion'].hist()\n",
      "#user_personalities.corr()\n",
      "43/2365:\n",
      "#user_personalities['extraversion'].hist()\n",
      "user_personalities.corr()\n",
      "43/2366:\n",
      "user_personalities['e_raw'].hist()\n",
      "user_personalities.corr()\n",
      "43/2367:\n",
      "user_personalities['e_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "43/2368:\n",
      "user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "43/2369:\n",
      "user_personalities['c_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "43/2370:\n",
      "user_personalities['e_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "43/2371:\n",
      "user_personalities['a_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "43/2372:\n",
      "user_personalities['n_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "43/2373:\n",
      "user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "43/2374: heatmap_rule_stats(c_all_rules, min_support_=0.00001,min_confidence_=0.00001)\n",
      "43/2375: heatmap_rule_stats(times_c, min_support_=0.00001,min_confidence_=0.00001)\n",
      "43/2376: heatmap_rule_stats(times_c, min_support_=0.1,min_confidence_=0.3)\n",
      "43/2377: heatmap_rule_stats(times_c, min_support_=0.1,min_confidence_=0.4)\n",
      "43/2378:\n",
      "user_personalities['Is_open']           =user_personalities['openness'].apply(          lambda x: 'Yes_openness' if x>np.median(user_personalities['o_raw'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes_conscientiousness' if x>np.median(user_personalities['c_raw']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['extraversion'].apply(      lambda x: 'Yes_extraversion' if x>np.median(user_personalities['e_raw'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes_agreeableness' if x>np.median(user_personalities['a_raw'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes_neuroticism' if x>np.median(user_personalities['n_raw'])       else 'No_neuroticism')\n",
      "43/2379:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "43/2380:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_assoc_df = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','time_spending_category','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/2381:\n",
      "def get_association_rules_df(items_df,var1, var2, min_support_, min_confidence_, filter_twosets):\n",
      "    items = items_df[[var1,var2]].values\n",
      "    rules = apriori(items, min_support = min_support_, min_confidence = min_confidence_)\n",
      "    rules_list = list(rules)\n",
      "    assoc_df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for relation_recordset in rules_list:\n",
      "        for rel_recordset_element in relation_recordset[2]:\n",
      "            element_nr = 0\n",
      "            for order_statistics in list(rel_recordset_element):\n",
      "                if element_nr == 0:  antescedent= str(list(order_statistics))\n",
      "                elif element_nr == 1:consequent = str(list(order_statistics))\n",
      "                elif element_nr == 2:confidence = order_statistics\n",
      "                elif element_nr == 3:lift = order_statistics\n",
      "                element_nr = element_nr + 1 \n",
      "            assoc_df = assoc_df.append({'antescedent':antescedent, 'consequent': consequent, 'support':relation_recordset[1]\n",
      "                                        , 'confidence': confidence, 'lift':lift, 'inverse_rule_confidence':'-'}\n",
      "                               ,  ignore_index = True)    \n",
      "    if filter_twosets==True:\n",
      "        assoc_df=assoc_df[assoc_df['antescedent']!='[]']\n",
      "    return assoc_df.sort_values('antescedent', ascending = False)\n",
      "\n",
      "def get_all_metric_associations(items_df, metric, min_support_, min_confidence_, filter_twosets):\n",
      "    df = pd.DataFrame(columns =(['antescedent', 'consequent', 'support', 'confidence', 'lift', 'inverse_rule_confidence']))\n",
      "    for col in items_df.columns:\n",
      "        if (col!='user') & (col!=metric):\n",
      "            df_=get_association_rules_df(items_df, metric, col, min_support_, min_confidence_, filter_twosets)\n",
      "            sql_query=\"SELECT * FROM df_ UNION ALL SELECT * FROM df\"\n",
      "            df = ps.sqldf(sql_query, locals())\n",
      "    return df\n",
      "\n",
      "def heatmap_rule_stats(df, min_support_=0, min_confidence_=0):\n",
      "    df = df[(df['support']>=min_support_) & (df['confidence']>=min_confidence_)]\n",
      "    if df.shape[0]>0:\n",
      "        tab_df = df\n",
      "        tab_sup = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['support'], 2), aggfunc='max')\n",
      "        tab_conf = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['confidence'], 2), aggfunc='max')\n",
      "        tab_lif = pd.crosstab(tab_df['antescedent'], tab_df['consequent'], values = round(tab_df['lift'], 2), aggfunc='max')\n",
      "        colors = ['Blues', 'BuPu', 'YlGnBu']\n",
      "        i=1\n",
      "        fig = plt.figure(figsize=(18, 14))\n",
      "        for var in (['support', 'confidence', 'lift']):\n",
      "            fig.add_subplot(2,2,i)\n",
      "            if var=='support':src_tab = tab_sup \n",
      "            elif var=='confidence':src_tab = tab_conf\n",
      "            elif var=='lift':src_tab = tab_lif\n",
      "            g = sns.heatmap(src_tab, annot=True, cmap=colors[i-1], fmt='g')\n",
      "            plt.title(var, fontsize=15)\n",
      "            plt.subplots_adjust(bottom=0.15, hspace=0.8, wspace = 0.4)\n",
      "            i=i+1\n",
      "    else:\n",
      "        print('No associations match min support and confidence criteria')\n",
      "43/2382:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "estimates_df.head()\n",
      "43/2383:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "estimates_df.shape[0]\n",
      "43/2384:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "estimates_df.shape[0], estimates_df.user.drop_duplicates()\n",
      "43/2385:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0]\n",
      "43/2386:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print(estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print(estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "43/2387:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "43/2388:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count()\n",
      "43/2389:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().sort_values('emailAddress', ascending=False)\n",
      "43/2390:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().sort_values('emailAddress', ascending=False)\n",
      "\n",
      "states_df.groupby('user').count()['user', 'emailAddress'].hist()\n",
      "43/2391:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().sort_values('emailAddress', ascending=False)\n",
      "\n",
      "states_df.groupby('user').count().reset_insex()['user', 'emailAddress'].hist()\n",
      "43/2392:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().sort_values('emailAddress', ascending=False)\n",
      "\n",
      "states_df.groupby('user').count().reset_insex()#['user', 'emailAddress'].hist()\n",
      "43/2393:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().sort_values('emailAddress', ascending=False)\n",
      "\n",
      "states_df.groupby('user').count()#['user', 'emailAddress'].hist()\n",
      "43/2394:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().sort_values('emailAddress', ascending=False)\n",
      "\n",
      "states_df.groupby('user').count().reset_index()#['user', 'emailAddress'].hist()\n",
      "43/2395:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().sort_values('emailAddress', ascending=False)\n",
      "\n",
      "states_df.groupby('user').count().reset_index()['user', 'emailAddress'].hist()\n",
      "43/2396:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().sort_values('emailAddress', ascending=False)\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist()\n",
      "43/2397:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().sort_values('emailAddress', ascending=False)\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(, bins = 5)\n",
      "43/2398:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().sort_values('emailAddress', ascending=False)\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 5)\n",
      "43/2399:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().sort_values('emailAddress', ascending=False)\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 20)\n",
      "43/2400:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 20)\n",
      "43/2401:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "43/2402:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT * FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals())\n",
      "43/2403:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals())\n",
      "43/2404:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities\n",
      "43/2405:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities['Is_Open']\n",
      "43/2406:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities['Is_open']\n",
      "43/2407:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities['Is_Open']\n",
      "43/2408:\n",
      "user_personalities['Is_open']           =user_personalities['openness'].apply(          lambda x: 'Yes_openness' if x>np.mean(user_personalities['o_raw'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes_conscientiousness' if x>np.mean(user_personalities['c_raw']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['extraversion'].apply(      lambda x: 'Yes_extraversion' if x>np.mean(user_personalities['e_raw'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes_agreeableness' if x>np.mean(user_personalities['a_raw'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes_neuroticism' if x>np.mean(user_personalities['n_raw'])       else 'No_neuroticism')\n",
      "43/2409:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities['Is_Open']\n",
      "43/2410:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities['Is_Open']['Is_open']\n",
      "43/2411:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities[['Is_Open','Is_open']]\n",
      "43/2412:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_open')\n",
      "43/2413:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_open').count()\n",
      "43/2414:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_Open').count()\n",
      "43/2415:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_open').count()\n",
      "43/2416:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities = user_personalities.drop(columns=['Is_Open'])\n",
      "#user_personalities.groupby('Is_open').count()\n",
      "43/2417:\n",
      "user_personalities['Is_open']           =user_personalities['openness'].apply(          lambda x: 'Yes_openness' if x>np.mean(user_personalities['o_raw'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes_conscientiousness' if x>np.mean(user_personalities['c_raw']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['extraversion'].apply(      lambda x: 'Yes_extraversion' if x>np.mean(user_personalities['e_raw'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes_agreeableness' if x>np.mean(user_personalities['a_raw'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes_neuroticism' if x>np.mean(user_personalities['n_raw'])       else 'No_neuroticism')\n",
      "43/2418:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_open').count()\n",
      "43/2419:\n",
      "user_personalities['Is_open']           =user_personalities['openness'].apply(          lambda x: 'Yes_openness' if x>np.median(user_personalities['o_raw'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes_conscientiousness' if x>np.mean(user_personalities['c_raw']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['extraversion'].apply(      lambda x: 'Yes_extraversion' if x>np.mean(user_personalities['e_raw'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes_agreeableness' if x>np.mean(user_personalities['a_raw'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes_neuroticism' if x>np.mean(user_personalities['n_raw'])       else 'No_neuroticism')\n",
      "43/2420:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_open').count()\n",
      "43/2421:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_open').count()\n",
      "43/2422:\n",
      "user_personalities['Is_open']           =user_personalities['openness'].apply(          lambda x: 'Yes_openness' if x>np.median(user_personalities['o_raw'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes_conscientiousness' if x>np.mean(user_personalities['c_raw']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['extraversion'].apply(      lambda x: 'Yes_extraversion' if x>np.mean(user_personalities['e_raw'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes_agreeableness' if x>np.mean(user_personalities['a_raw'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes_neuroticism' if x>np.mean(user_personalities['n_raw'])       else 'No_neuroticism')\n",
      "43/2423:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_open').count()\n",
      "43/2424:\n",
      "user_personalities['Is_open']           =user_personalities['o_raw'].apply(          lambda x: 'Yes_openness' if x>np.median(user_personalities['o_raw'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['c_raw'].apply( lambda x: 'Yes_conscientiousness' if x>np.mean(user_personalities['c_raw']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['e_raw'].apply(      lambda x: 'Yes_extraversion' if x>np.mean(user_personalities['e_raw'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['a_raw'].apply(     lambda x: 'Yes_agreeableness' if x>np.mean(user_personalities['a_raw'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['n_raw'].apply(       lambda x: 'Yes_neuroticism' if x>np.mean(user_personalities['n_raw'])       else 'No_neuroticism')\n",
      "43/2425:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_open').count()\n",
      "43/2426:\n",
      "user_personalities['Is_open']           =user_personalities['o_raw'].apply(          lambda x: 'Yes_openness' if x>np.mean(user_personalities['o_raw'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['c_raw'].apply( lambda x: 'Yes_conscientiousness' if x>np.mean(user_personalities['c_raw']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['e_raw'].apply(      lambda x: 'Yes_extraversion' if x>np.mean(user_personalities['e_raw'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['a_raw'].apply(     lambda x: 'Yes_agreeableness' if x>np.mean(user_personalities['a_raw'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['n_raw'].apply(       lambda x: 'Yes_neuroticism' if x>np.mean(user_personalities['n_raw'])       else 'No_neuroticism')\n",
      "43/2427:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_open').count()\n",
      "43/2428:\n",
      "user_personalities['Is_open']           =user_personalities['o_raw'].apply(          lambda x: 'Yes_openness' if x>np.min(user_personalities['o_raw'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['c_raw'].apply( lambda x: 'Yes_conscientiousness' if x>np.mean(user_personalities['c_raw']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['e_raw'].apply(      lambda x: 'Yes_extraversion' if x>np.mean(user_personalities['e_raw'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['a_raw'].apply(     lambda x: 'Yes_agreeableness' if x>np.mean(user_personalities['a_raw'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['n_raw'].apply(       lambda x: 'Yes_neuroticism' if x>np.mean(user_personalities['n_raw'])       else 'No_neuroticism')\n",
      "43/2429:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_open').count()\n",
      "43/2430:\n",
      "user_personalities['Is_open']           =user_personalities['o_raw'].apply(          lambda x: 'Yes_openness' if x>np.mean(user_personalities['o_raw'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['c_raw'].apply( lambda x: 'Yes_conscientiousness' if x>np.mean(user_personalities['c_raw']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['e_raw'].apply(      lambda x: 'Yes_extraversion' if x>np.mean(user_personalities['e_raw'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['a_raw'].apply(     lambda x: 'Yes_agreeableness' if x>np.mean(user_personalities['a_raw'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['n_raw'].apply(       lambda x: 'Yes_neuroticism' if x>np.mean(user_personalities['n_raw'])       else 'No_neuroticism')\n",
      "43/2431:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_open').count()\n",
      "43/2432:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_Extravert').count()\n",
      "43/2433:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities.groupby('Is_extravert').count()\n",
      "43/2434:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities[['Is_extravert', 'user']].groupby('Is_extravert').count()\n",
      "43/2435:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "#states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "user_personalities[['Is_open', 'user']].groupby('Is_open').count()\n",
      "user_personalities[['Is_conscientious', 'user']].groupby('Is_conscientious').count()\n",
      "user_personalities[['Is_extravert', 'user']].groupby('Is_extravert').count()\n",
      "user_personalities[['Is_agreeable', 'user']].groupby('Is_agreeable').count()\n",
      "user_personalities[['Is_neurotic', 'user']].groupby('Is_neurotic').count()\n",
      "43/2436:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "#states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "#ps.sqldf(query, locals())\n",
      "print(user_personalities[['Is_open', 'user']].groupby('Is_open').count(),\n",
      "user_personalities[['Is_conscientious', 'user']].groupby('Is_conscientious').count(),\n",
      "user_personalities[['Is_extravert', 'user']].groupby('Is_extravert').count(),\n",
      "user_personalities[['Is_agreeable', 'user']].groupby('Is_agreeable').count(),\n",
      "user_personalities[['Is_neurotic', 'user']].groupby('Is_neurotic').count())\n",
      "43/2437:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', estimates_df.shape[0], estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', priorities_df.shape[0], priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', states_df.shape[0], states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', valid_users_times.shape[0], valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "43/2438:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df', 'total nr of records: ', estimates_df.shape[0], 'unique users: ',estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', 'total nr of records: ',priorities_df.shape[0], 'unique users: ',priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', 'total nr of records: ',states_df.shape[0], 'unique users: ',states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', 'total nr of records: ',valid_users_times.shape[0], 'unique users: ',valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "43/2439:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('<br>estimates_df', 'total nr of records: ', estimates_df.shape[0], 'unique users: ',estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', 'total nr of records: ',priorities_df.shape[0], 'unique users: ',priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', 'total nr of records: ',states_df.shape[0], 'unique users: ',states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', 'total nr of records: ',valid_users_times.shape[0], 'unique users: ',valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "43/2440:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('**estimates_df', 'total nr of records: ', estimates_df.shape[0], 'unique users: ',estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df', 'total nr of records: ',priorities_df.shape[0], 'unique users: ',priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df', 'total nr of records: ',states_df.shape[0], 'unique users: ',states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times', 'total nr of records: ',valid_users_times.shape[0], 'unique users: ',valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "43/2441:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df---', 'total nr of records: ', estimates_df.shape[0], 'unique users: ',estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df---', 'total nr of records: ',priorities_df.shape[0], 'unique users: ',priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df---', 'total nr of records: ',states_df.shape[0], 'unique users: ',states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times---', 'total nr of records: ',valid_users_times.shape[0], 'unique users: ',valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "43/2442:\n",
      "#user_personalities['o_raw'].hist()\n",
      "#user_personalities.corr()\n",
      "print('estimates_df---', 'total nr of records: ', estimates_df.shape[0], '; unique users: ',estimates_df.user.drop_duplicates().shape[0])\n",
      "print('priorities_df---', 'total nr of records: ',priorities_df.shape[0], '; unique users: ',priorities_df.user.drop_duplicates().shape[0])\n",
      "print('states_df---', 'total nr of records: ',states_df.shape[0], '; unique users: ',states_df.user.drop_duplicates().shape[0])\n",
      "print('valid_users_times---', 'total nr of records: ',valid_users_times.shape[0], '; unique users: ',valid_users_times.user.drop_duplicates().shape[0])\n",
      "\n",
      "states_df.groupby('user').count().reset_index()[['user', 'emailAddress']].hist(bins = 30)\n",
      "43/2443:\n",
      "print(user_personalities[['Is_open', 'user']].groupby('Is_open').count(),\n",
      "user_personalities[['Is_conscientious', 'user']].groupby('Is_conscientious').count(),\n",
      "user_personalities[['Is_extravert', 'user']].groupby('Is_extravert').count(),\n",
      "user_personalities[['Is_agreeable', 'user']].groupby('Is_agreeable').count(),\n",
      "user_personalities[['Is_neurotic', 'user']].groupby('Is_neurotic').count())\n",
      "\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN estimates_df AS E on U.user = E.user\n",
      "INNER JOIN priorities_df AS P on U.user = P.user\n",
      "INNER JOIN states_df AS S on U.user = S.user\n",
      "INNER JOIN valid_users_times AS T on U.user = T.user\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals())\n",
      "43/2444:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "priorities_assoc_df = pd.merge(user_personalities, priorities_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','priority','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "states_assoc_df = pd.merge(user_personalities, states_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','status','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "times_assoc_df = pd.merge(user_personalities, valid_users_times, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','time_spending_category','Is_open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']]\n",
      "43/2445:\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN (SELECT DISTINCT user FROM estimates_df) AS E on U.user = E.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM priorities_df) AS P on U.user = P.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM states_df) AS S on U.user = S.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM valid_users_times) AS T on U.user = T.user\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals())\n",
      "43/2446:\n",
      "query = \"\"\"\n",
      "SELECT U.* FROM user_personalities AS U\n",
      "INNER JOIN (SELECT DISTINCT user FROM estimates_df) AS E on U.user = E.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM priorities_df) AS P on U.user = P.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM states_df) AS S on U.user = S.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM valid_users_times) AS T on U.user = T.user\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals()).shape[0]\n",
      "43/2447:\n",
      "query = \"\"\"\n",
      "SELECT count(distinct user) FROM user_personalities AS U\n",
      "INNER JOIN (SELECT DISTINCT user FROM estimates_df) AS E on U.user = E.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM priorities_df) AS P on U.user = P.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM states_df) AS S on U.user = S.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM valid_users_times) AS T on U.user = T.user\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals()).shape[0]\n",
      "43/2448:\n",
      "query = \"\"\"\n",
      "SELECT count(user) FROM user_personalities AS U\n",
      "INNER JOIN (SELECT DISTINCT user FROM estimates_df) AS E on U.user = E.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM priorities_df) AS P on U.user = P.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM states_df) AS S on U.user = S.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM valid_users_times) AS T on U.user = T.user\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals()).shape[0]\n",
      "43/2449:\n",
      "query = \"\"\"\n",
      "SELECT count(*) FROM user_personalities AS U\n",
      "INNER JOIN (SELECT DISTINCT user FROM estimates_df) AS E on U.user = E.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM priorities_df) AS P on U.user = P.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM states_df) AS S on U.user = S.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM valid_users_times) AS T on U.user = T.user\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals()).shape[0]\n",
      "43/2450:\n",
      "query = \"\"\"\n",
      "SELECT count(*) FROM user_personalities AS U\n",
      "INNER JOIN (SELECT DISTINCT user FROM estimates_df) AS E on U.user = E.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM priorities_df) AS P on U.user = P.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM states_df) AS S on U.user = S.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM valid_users_times) AS T on U.user = T.user\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals())\n",
      "43/2451:\n",
      "query = \"\"\"\n",
      "SELECT U.user FROM user_personalities AS U\n",
      "INNER JOIN (SELECT DISTINCT user FROM estimates_df) AS E on U.user = E.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM priorities_df) AS P on U.user = P.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM states_df) AS S on U.user = S.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM valid_users_times) AS T on U.user = T.user\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals()).shape[0]\n",
      "43/2452:\n",
      "query = \"\"\"\n",
      "SELECT U.user FROM user_personalities AS U\n",
      "INNER JOIN (SELECT DISTINCT user FROM estimates_df) AS E on U.user = E.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM priorities_df) AS P on U.user = P.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM states_df) AS S on U.user = S.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM valid_users_times) AS T on U.user = T.user\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals())\n",
      "43/2453:\n",
      "query = \"\"\"\n",
      "SELECT U.user FROM user_personalities AS U\n",
      "INNER JOIN (SELECT DISTINCT user FROM estimates_df) AS E on U.user = E.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM priorities_df) AS P on U.user = P.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM states_df) AS S on U.user = S.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM valid_users_times) AS T on U.user = T.user\n",
      "\"\"\"\n",
      "ps.sqldf(query, locals()).shape[0]\n",
      "43/2454:\n",
      "query = \"\"\"\n",
      "SELECT U.user FROM user_personalities AS U\n",
      "INNER JOIN (SELECT DISTINCT user FROM estimates_df) AS E on U.user = E.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM priorities_df) AS P on U.user = P.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM states_df) AS S on U.user = S.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM valid_users_times) AS T on U.user = T.user\n",
      "\"\"\"\n",
      "all_mterics_available_users = ps.sqldf(query, locals())\n",
      "all_mterics_available_users.shape[0]\n",
      "43/2455:\n",
      "user_personalities['Is_open']           =user_personalities['o_raw'].apply(          lambda x: 'Yes_openness' if x>np.mean(user_personalities['o_raw'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['c_raw'].apply( lambda x: 'Yes_conscientiousness' if x>np.mean(user_personalities['c_raw']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['e_raw'].apply(      lambda x: 'Yes_extraversion' if x>np.mean(user_personalities['e_raw'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['a_raw'].apply(     lambda x: 'Yes_agreeableness' if x>np.mean(user_personalities['a_raw'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['n_raw'].apply(       lambda x: 'Yes_neuroticism' if x>np.mean(user_personalities['n_raw'])       else 'No_neuroticism')\n",
      "\n",
      "\n",
      "print(user_personalities[['Is_open', 'user']].groupby('Is_open').count(),'\\n',\n",
      "user_personalities[['Is_conscientious', 'user']].groupby('Is_conscientious').count(),'\\n',\n",
      "user_personalities[['Is_extravert', 'user']].groupby('Is_extravert').count(),'\\n',\n",
      "user_personalities[['Is_agreeable', 'user']].groupby('Is_agreeable').count(),'\\n',\n",
      "user_personalities[['Is_neurotic', 'user']].groupby('Is_neurotic').count())\n",
      "\n",
      "query = \"\"\"\n",
      "SELECT U.user FROM user_personalities AS U\n",
      "INNER JOIN (SELECT DISTINCT user FROM estimates_df) AS E on U.user = E.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM priorities_df) AS P on U.user = P.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM states_df) AS S on U.user = S.user\n",
      "INNER JOIN (SELECT DISTINCT user FROM valid_users_times) AS T on U.user = T.user\n",
      "\"\"\"\n",
      "all_mterics_available_users = ps.sqldf(query, locals())\n",
      "print(all_mterics_available_users.shape[0])\n",
      "   1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from matplotlib.ticker import PercentFormatter\n",
      "from matplotlib.patches import Rectangle\n",
      "import seaborn as sns\n",
      "import re\n",
      "from collections import Counter\n",
      "   2:\n",
      "changelog = pd.read_csv('jiradataset_changelog.csv')\n",
      "issues = pd.read_csv('jiradataset_issues.csv')\n",
      "sprints = pd.read_csv('jiradataset_sprints.csv')\n",
      "users = pd.read_csv('jiradataset_users.csv')\n",
      "   3: changelog.head(3)\n",
      "   4: issues.head(3)\n",
      "   5: sprints.head(3)\n",
      "   6: users.head(3)\n",
      "   7: changelog.head(3)\n",
      "   8:\n",
      "# declare the empty series\n",
      "i_row = []\n",
      "field_row = []\n",
      "from_row = []\n",
      "to_row = []\n",
      "from_string_row = []\n",
      "to_string_row = []\n",
      "\n",
      "# loop through each unique field, take top two values from the columns 'fromString', 'toString', 'from' and 'to'\n",
      "# append these two values to the respective series\n",
      "# create dataframe 'df' with these filled series, and export it to .csv file, and check it manually.\n",
      "i = 1\n",
      "for field in changelog['field'].unique():\n",
      "    from_str = changelog[changelog['field'] == field]['fromString'].head(2)\n",
      "    from_ = changelog[changelog['field'] == field]['from'].head(2)\n",
      "    to_str = changelog[changelog['field'] == field]['toString'].head(2)\n",
      "    to_ = changelog[changelog['field'] == field]['to'].head(2)\n",
      "    i_row.append(i)\n",
      "    field_row.append(field)\n",
      "    from_row.append(from_str)\n",
      "    to_row.append(to_str)\n",
      "    from_string_row.append(from_)\n",
      "    to_string_row.append(to_)\n",
      "    i = i + 1\n",
      "df = pd.DataFrame({'id':i_row,\n",
      "                   'field':field_row, \n",
      "                   'from':from_row, \n",
      "                   'to':to_row, \n",
      "                   'fromString':from_string_row, \n",
      "                   'toString':to_string_row })\n",
      "df.to_csv('fields_check.csv')\n",
      "df.head()\n",
      "   9:\n",
      "# filter changelog with the textual fields\n",
      "log_filtered = changelog[changelog['field'].isin(['summary', 'description', 'Acceptance Criteria', 'Comment', 'Epic Name',\n",
      " 'Out of Scope', 'QA Test Plan', 'Epic/Theme', 'Migration Impact', 'Business Value'])]\n",
      "\n",
      "# take only necessary columns - key (Jira task unique key-name), project (one of the eight project codes), \n",
      "# author (author of change), field (what field has been changed), created (date of the change action),\n",
      "# toString (what textual value was assigned to the field), \n",
      "# and from(what was the value of the field, this only works for comments)\n",
      "\n",
      "cols = ['key', 'project', 'author', 'field', 'created', 'toString', 'from']\n",
      "log_cols = log_filtered[cols].copy(deep=True)\n",
      "log_cols[log_cols['field']!='Comment']['from'] = np.NaN\n",
      "log_cols['text'] = log_cols['from'].combine_first(log_cols['toString'])\n",
      "\n",
      "newcols = ['key', 'project', 'author', 'field', 'created', 'text']\n",
      "log_cols = log_cols[newcols]\n",
      "log_cols.head()\n",
      "  10:\n",
      "#1. in descriptions: only leave the latest edited descriptions per unique project key. \n",
      "###(get uniques issues, join log with 'project', 'key', 'status' and get the lates onr by created date.)\n",
      "log_grouped = log_cols.groupby((['project', 'key', 'field', 'author'])).agg({'created':'max'})\n",
      "log_grouped.reset_index(level= [0,1,2], inplace=True)\n",
      "\n",
      "latest_logs_from_issues = pd.merge(log_cols, log_grouped, how = 'inner',\n",
      "                                   left_on = ['project', 'key', 'field', 'created'],\n",
      "                                   right_on = ['project', 'key', 'field', 'created'])\n",
      "\n",
      "print(log_cols.shape[0], log_grouped.shape[0], latest_logs_from_issues.shape[0]) \n",
      "log_cols = latest_logs_from_issues\n",
      "  11:\n",
      "# let's export the dataset before cleaning values, to compare afterwards.\n",
      "log_cols.to_csv('original_text_cols.csv')\n",
      "log_cols.head(3)\n",
      "  12:\n",
      "# remove the whitespaces. \n",
      "#### ----  Clean the texts: remove the bad characters / similar to trim\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\n', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\r', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\t', ' ')\n",
      "log_cols['text'] = log_cols['text'].str.replace('\\s', ' ')\n",
      "  13:\n",
      "### --- Clean the texts: \n",
      "def removeCodeSnippet(text):\n",
      "    text = str(text).replace('&nbsp;', ' ')\n",
      "    text = str(text).replace('sp_executesql', ' ')\n",
      "    text = str(text).replace('exec', ' ')\n",
      "    # remove not formatted code and trace part\n",
      "    text = re.sub(\"{noformat}(.+?){noformat}\", '', str(text)) \n",
      "    # remove code snippet (Both, {code} and {code: [programming language]}:\n",
      "    text = re.sub(\"{code(.+?){code}\", '', str(text)) \n",
      "    # remove html tags:\n",
      "    text = re.sub(\"<(.+?)>\", '', str(text))\n",
      "    # remove another type code snippets:\n",
      "    text = re.sub(\"{{(.+?)}}\", '', str(text))\n",
      "    #remove tags\n",
      "    text = re.sub(\"{(.+?)}\", '', str(text))\n",
      "    #remove java calls \n",
      "    text = re.sub('\"jdbc(.+?)\"', \" \", str(text))\n",
      "    # remove module calls\n",
      "    text = re.sub('\"module(.+?)\"', '', str(text)) \n",
      "    # remove job calls\n",
      "    text = re.sub('\"job(.+?)\"', '', str(text)) \n",
      "    # remove SQL Begin-end transactions\n",
      "    text = re.sub('\\s*(B|b)(egin|EGIN)\\s+.+\\s+(E|e)(nd|ND)\\s*', '', str(text))\n",
      "    # remove SQL SELECT Statements\n",
      "    text = re.sub('\\s*(s|S)(elect|ELECT).+(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', ' ', str(text))\n",
      "    # remove SQL INSERT statements\n",
      "    text = re.sub('\\s*(I|I)(nsert|NSERT)\\s*(I|i)(nto|NTO)\\s+.+(V|v)(alues|ALUES)\\s*.+\\(.+\\)\\s*', ' ', str(text)) \n",
      "    # remove SQL DELETE statements\n",
      "    text = re.sub('\\s*(d|D)(elete|ELETE)\\s*(f|F)(rom|ROM)\\s*\\S+(\\s*(w|W)(here|HERE)\\s*\\S+\\s*\\S*\\s*\\S*\\s|)', '***', str(text)) \n",
      "    # remove system version information part\n",
      "    text = re.sub('[*][*][*]Version(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove deployment system descriptions\n",
      "    text = re.sub('[*][*][*]Describe XD Deployment(.+?)[*][*][*]', '***', str(text)) \n",
      "    #remove system component descriptions\n",
      "    text = re.sub('[*][*][*]Describe Other Components(.+?)[*][*][*]', '***', str(text)) \n",
      "    # remove system generated headers within description\n",
      "    text = text.replace('***Description', '')\n",
      "    text = text.replace('***Steps to recreate the problem', '')\n",
      "    text = text.replace('***Error Message:', '')\n",
      "    # remove square brakets with one word in it (since they are tags)\n",
      "    text = re.sub('\\[([^[\\]{}()]+?)\\]', '', str(text))\n",
      "    #remove web links:\n",
      "    text = re.sub('http[s]?://\\S+', ' ', str(text))\n",
      "    #remove local path links (with slashes)\n",
      "    text = re.sub('\\S+?(?=\\/)\\/\\S*\\/\\S*', ' ', str(text))\n",
      "    #remove local path links (with backslashes)\n",
      "    text = re.sub(r'\\S+?(?=\\\\)\\\\\\S*\\\\\\S*', \" \", str(text))    \n",
      "    #remove logs within asterisks\n",
      "    text = re.sub('\\*{50,}(.+?)\\*{50,}', ' ', str(text)) \n",
      "    text = re.sub('\\*+(.+?)\\*+', ' ', str(text))\n",
      "    #remove text with more than 18 character, that usually are the command codes. \n",
      "    text = re.sub('.\\S{15,}.', \" \", str(text))  \n",
      "    # remove email addresses and commands containing @ (mainly used as sql function parameter)\n",
      "    text = re.sub('(\\s|\\S+(?=@))@\\S*', \" \", str(text))\n",
      "    #remove  call commands with \"--\"\n",
      "    text = re.sub(\"--(\\s{0,1})\\S*\", '', str(text))\n",
      "    #remove  call commands with \"-\" - PROBLEM it can also delete normal words that are listed\n",
      "    #text = re.sub(\"-\\S*\", '', str(text))\n",
      "    # remove call commands with \"--\"\n",
      "    text = re.sub(\"~(\\s{0,1})\\S*\", '', str(text))\n",
      "    # remove sql SELECT statements\n",
      "    text = re.sub('SELECT\\s\\S+\\sFROM\\s\\S+\\s(WHERE\\s\\S+\\s\\S+\\s\\S|)*', '', str(text))\n",
      "    # remove websites and one dotted version numbers\n",
      "    text = re.sub('\\S+\\.\\S+', '', str(text))\n",
      "    # remove words containing :\n",
      "    text = re.sub('\\S+\\:\\S+', '', str(text))\n",
      "    # remove command words and versions\n",
      "    text = re.sub('\\S*(_|-|:|\\.)\\S*(_|-|:|\\.)\\S+', '', str(text))\n",
      "    # remove multiple 'at' left after the code snippets cleaning\n",
      "    text = text.replace('at at ', ' ') \n",
      "    #remove multiple whitespaces (needed for removing 'at at' texts, regex is the next command below)\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    # remove multiple 'at'\n",
      "    text = re.sub('at\\sat\\s', ' ', str(text))\n",
      "    # remove the non-textual characters\n",
      "    text = re.sub(r'(\\||~|=|>|_|\\[|\\]|{|}|--|\\/|\\\\|#)', ' ', str(text))\n",
      "    # remove non-unicode characters\n",
      "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
      "    # remove dates:\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9][0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]', \" \", str(text))\n",
      "    text = re.sub('[0-9][0-9][0-9][0-9](-|\\\\|.|\\/| )([0-9][0-9]|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(-|\\\\|.|\\/| )[0-9][0-9]', \" \", str(text))\n",
      "    #remove multiple whitespaces\n",
      "    text = re.sub(\"\\s{2,}\", ' ', str(text))\n",
      "    \n",
      "    return text\n",
      "  14:\n",
      "log_cols['text'] = log_cols['text'].apply(removeCodeSnippet)\n",
      "log_cols['text'] = log_cols['text'].apply(lambda x: str.strip(x))\n",
      "  15:\n",
      "#Check the number of rows per field type\n",
      "print(log_cols.groupby('field').size())\n",
      "  16:\n",
      "log_cols = log_cols[log_cols['field'].isin(['summary', 'description', 'Comment',\n",
      "                                            'Acceptance Criteria', 'Migration Impact', 'QA Test Plan', 'Out of Scope'])]\n",
      "# create datasets for each field type\n",
      "descriptions = log_cols[log_cols['field'] == 'description']\n",
      "summaries = log_cols[log_cols['field'] == 'summary']\n",
      "comments = log_cols[log_cols['field'] == 'Comment']\n",
      "  17:\n",
      "# detect very long texts. likely, these are log traces, we can eliminate from them.\n",
      "log_cols['textLength'] = log_cols['text'].str.len()\n",
      "  18: log_cols.sort_values('textLength', ascending=False).head(20)\n",
      "  19:\n",
      "#remove outliers\n",
      "# I have checked them manually, \n",
      "#these are the ones that the text cleaning functions could not properly clean and contain mostly the code snippet or logs\n",
      "#log_cols = log_cols.drop([3923, 1861, 3801, 2763, 7794, 2157])\n",
      "  20: log_cols.to_csv('log_cols.csv')\n",
      "  21:\n",
      "cut_val = round((int(log_cols.shape[0]) * 0.05))\n",
      "cut_val_top_length = log_cols.sort_values('textLength', ascending=False).head(cut_val).iloc[cut_val-1].textLength\n",
      "cut_val_bottom_length = log_cols.sort_values('textLength', ascending=True).head(cut_val).iloc[cut_val-1].textLength\n",
      "\n",
      "print('nr of rows from 5% to 95%: ', log_cols[(log_cols['textLength']<cut_val_top_length) & (log_cols['textLength']>cut_val_bottom_length)].shape[0])\n",
      "print('nr of rows above 95% and minimum length of them: ', log_cols[log_cols['textLength']>=cut_val_top_length].shape[0], cut_val_top_length)\n",
      "print('nr of rows below 5% and maximum length of them: ',log_cols[log_cols['textLength']<=cut_val_bottom_length].shape[0], cut_val_bottom_length)\n",
      "print('Total number of rows in dataset', log_cols.shape[0])\n",
      "  22:\n",
      "def hist_with_perc(_data1, _bins1, _title1, _xlabel, _ylabel, _color,\n",
      "                  _data2, _bins2, _title2, \n",
      "                  _data3, _bins3, _title3, \n",
      "                  _data4, _bins4, _title4):\n",
      "\n",
      "    fig = plt.figure(figsize=(16, 10))\n",
      "    for i in range(0, 4):\n",
      "        if i ==0:\n",
      "            _data=_data1\n",
      "            _bins=_bins1\n",
      "            _title=_title1\n",
      "        elif i ==1:\n",
      "            _data=_data2\n",
      "            _bins=_bins2\n",
      "            _title=_title2\n",
      "        elif i ==2:\n",
      "            _data=_data3\n",
      "            _bins=_bins3\n",
      "            _title=_title3\n",
      "        elif i ==3:\n",
      "            _data=_data4\n",
      "            _bins=_bins4\n",
      "            _title=_title4\n",
      "        ax = fig.add_subplot(2,2,i+1)\n",
      "        counts, bins, patches = ax.hist(_data, facecolor=_color, edgecolor='gray', bins=_bins)\n",
      "        ax.set_xticks(bins.round(0))\n",
      "        plt.xticks(rotation=45)\n",
      "        plt.title(_title, fontsize=20)\n",
      "        plt.ylabel(_ylabel, fontsize=15)\n",
      "        plt.xlabel(_xlabel, fontsize=15)\n",
      "        plt.subplots_adjust(bottom=0.15, hspace=0.5)\n",
      "        bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
      "        bin_y_centers = ax.get_yticks()[1] * 0.25\n",
      "        for i in range(len(bins)-1):\n",
      "            bin_label = \"{0:,}\".format(counts[i]) + \"  ({0:,.2f}%)\".format((counts[i]/counts.sum())*100)\n",
      "            plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
      "    plt.show()\n",
      "  23:\n",
      "hist_with_perc(log_cols['textLength'], 15,'text lengths \\n in the whole dataset','text length','Count','lightblue'\n",
      "            , log_cols[log_cols['textLength']>=cut_val_top_length]['textLength'],15,'the longest 5%  texts lengths \\n in the dataset'\n",
      "            , log_cols[log_cols['textLength']<=cut_val_bottom_length]['textLength'],15,'shortest 5%  texts lengths \\n in the dataset'\n",
      "            , log_cols[(log_cols['textLength']<cut_val_top_length)&(log_cols['textLength']>cut_val_bottom_length)].textLength,15,'the texts between 5%-95% percentile length \\n in the dataset')\n",
      "  24:\n",
      "#detect the number of rows that fall in top % and bottom %\n",
      "top= 0.01\n",
      "bottom = 0.02\n",
      "cutoff_percent_top = round((int(log_cols.shape[0]) * top))\n",
      "cutoff_percent_bottom = round((int(log_cols.shape[0]) * bottom))\n",
      "print('Total number of rows in dataset:', str(log_cols.shape[0]))\n",
      "print('Number of top '+str(round(top*100))+'% of the rows: ', str(cutoff_percent_top))\n",
      "print('Number of bottom '+str(round(bottom*100))+'% of the rows: ', str(cutoff_percent_bottom))\n",
      "\n",
      "value_Longest = log_cols.sort_values('textLength', ascending=False).head(cutoff_percent_top).iloc[cutoff_percent_top-1].textLength\n",
      "value_shortest = log_cols.sort_values('textLength', ascending=True).head(cutoff_percent_bottom).iloc[cutoff_percent_bottom-1].textLength\n",
      "\n",
      "print('length of the text, above which to filter out the rows: ', str(value_Longest))\n",
      "print('length of the text, below which to filter out the rows: ', str(value_shortest))\n",
      "\n",
      "#log_cols[log_cols['textLength']<=value_shortest].to_csv('shortest_3pct.csv')\n",
      "#log_cols[log_cols['textLength']>=value_Longest].to_csv('longest_7pct.csv')\n",
      "  25: log_cut = log_cols[(log_cols['textLength']>value_shortest) & (log_cols['textLength']<value_Longest)]\n",
      "  26:\n",
      "\n",
      "# loop through the projects, then through the authors and combine the texts written by the given author in the given project\n",
      "# store the project, author and text data into series and form a data frame.\n",
      "\n",
      "df_proj_name = []\n",
      "df_user_name = []\n",
      "df_user_text = []\n",
      "df_texts_count = []\n",
      "df_texts_length = []\n",
      "df_words_in_text = []\n",
      "for project in log_cut['project'].unique():\n",
      "    for dev_user in log_cut[log_cut['project']==project]['author'].unique():\n",
      "        user_txt = ''       \n",
      "        texts_count = 0\n",
      "        texts_length = 0\n",
      "        words_in_text = 0\n",
      "        curr_df = log_cut[(log_cut['project']==project) & (log_cut['author']==dev_user)]\n",
      "        for index, row in curr_df.iterrows():\n",
      "            user_txt = str(user_txt) + str(row['text']) + '. '\n",
      "            texts_count = texts_count + 1\n",
      "            texts_length = texts_length + len(row['text'])\n",
      "            words_in_text = words_in_text + len(row['text'].split()) \n",
      "        df_proj_name.append(project)\n",
      "        df_user_name.append(dev_user)\n",
      "        df_user_text.append(user_txt)\n",
      "        df_texts_count.append(texts_count)\n",
      "        df_texts_length.append(texts_length)\n",
      "        df_words_in_text.append(words_in_text)\n",
      "    \n",
      "user_text_combined = pd.DataFrame({'project':df_proj_name,\n",
      "                   'user':df_user_name, \n",
      "                   'text':df_user_text,\n",
      "                    'count_of_texts':df_texts_count,\n",
      "                    'words_in_text':df_words_in_text,\n",
      "                    'texts_length':df_texts_length})\n",
      "     \n",
      "user_text_combined.to_csv('user_text_combined.csv')\n",
      "print(user_text_combined.shape)\n",
      "user_text_combined.head(3)\n",
      "  27: user_text_combined[user_text_combined['words_in_text']>=600].shape[0]\n",
      "  28:\n",
      "users_df = users[['displayName','emailAddress','name','project']]\n",
      "users_df = users_df[['displayName','emailAddress','name','project']].drop_duplicates()\n",
      "  29:\n",
      "user_all_texts_emails = pd.merge(user_text_combined, users_df, how = 'left',\n",
      "                                   left_on = ['project', 'user'],\n",
      "                                   right_on = ['project', 'name'])[['project','user','text','count_of_texts','words_in_text','texts_length','emailAddress']]\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot org', '.org')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot io', '.io')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot me', '.me')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot com', '.com')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot fr', '.fr')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' dot ', '.')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' at ', '@')\n",
      "user_all_texts_emails['emailAddress'] = user_all_texts_emails['emailAddress'].str.replace(' ', '')\n",
      "  30:\n",
      "users_proj = user_all_texts_emails[['user', 'emailAddress', 'project']].groupby(['user', 'emailAddress']).count()\n",
      "users_proj.reset_index(level= [0,1], inplace=True)\n",
      "\n",
      "users_with_duplicates = users_proj[users_proj['project']>1][['user', 'emailAddress']]\n",
      "users_with_single = users_proj[users_proj['project']==1][['user', 'emailAddress']]\n",
      "users_with_single_noemail=user_all_texts_emails[pd.isnull(user_all_texts_emails.emailAddress)==True][['user', 'emailAddress']]\n",
      "\n",
      "for i in range(0, users_with_duplicates.shape[0]):\n",
      "    user_name = users_with_duplicates.iloc[i]['user']\n",
      "    user_email = users_with_duplicates.iloc[i]['emailAddress']   \n",
      "    df = (user_all_texts_emails[(user_all_texts_emails['user']==user_name) & (user_all_texts_emails['emailAddress']==user_email)])\n",
      "    text_=''\n",
      "    count_of_texts_=0\n",
      "    words_in_text_=0\n",
      "    texts_length_=0\n",
      "    for k in range(0, df.shape[0]):\n",
      "        text_ = text_ + df.iloc[k]['text']\n",
      "        count_of_texts_ = count_of_texts_ + df.iloc[k]['count_of_texts']\n",
      "        words_in_text_ = words_in_text_ + df.iloc[k]['words_in_text']\n",
      "        texts_length_ = texts_length_ + df.iloc[k]['texts_length']\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'text']=text_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'count_of_texts']=count_of_texts_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'words_in_text']=words_in_text_\n",
      "    user_all_texts_emails.loc[(user_all_texts_emails['user']==user_name) \n",
      "                              & (user_all_texts_emails['emailAddress']==user_email), 'texts_length']=texts_length_\n",
      "  31:\n",
      "print('before these changes: ', user_text_combined[user_text_combined['words_in_text']>=600].shape[0], '\\n',\n",
      "      'after these changes: ', user_all_texts_emails[user_all_texts_emails['words_in_text']>=600].shape[0])\n",
      "  32: user_all_texts_emails[user_all_texts_emails['words_in_text']>=600].head()\n",
      "  33: valid_user_texts = user_all_texts_emails[user_all_texts_emails['words_in_text']>=600]\n",
      "  34:\n",
      "print(valid_user_texts.shape[0])\n",
      "valid_user_texts.head(3)\n",
      "  35:\n",
      "valid_user_texts.loc[pd.isnull(valid_user_texts.emailAddress)==True,'emailAddress'] = '-'\n",
      "#valid_user_texts[['user','emailAddress']].drop_duplicates().to_csv('valid_users_unique.csv')\n",
      "print('Total number of unique users in valid user texts dataset: ', valid_user_texts[['user','emailAddress']].drop_duplicates().shape[0])\n",
      "valid_user_texts[['user','emailAddress']].drop_duplicates().head(3)\n",
      "  36:\n",
      "#valid_user_texts.groupby(['project']).count().to_csv('project_users.csv')\n",
      "valid_user_texts[['project', 'user']].groupby(['project']).count().sort_values('user', ascending=False)\n",
      "  37:\n",
      "valid_user_texts_unique = valid_user_texts[['user','emailAddress', 'text']].drop_duplicates()\n",
      "print(valid_user_texts_unique.shape[0])\n",
      "valid_user_texts_unique.head()\n",
      "  38: #valid_user_texts_unique.to_csv('valid_user_texts_unique.csv')\n",
      "  39: valid_user_texts_unique.head()\n",
      "  40:\n",
      "from __future__ import print_function\n",
      "import json\n",
      "from os.path import join, dirname\n",
      "from ibm_watson import PersonalityInsightsV3\n",
      "import csv\n",
      "import os\n",
      "from datetime import datetime\n",
      "\n",
      "service = PersonalityInsightsV3(\n",
      "    version='2017-10-13',\n",
      "    ## url is optional, and defaults to the URL below. Use the correct URL for your region.\n",
      "    url='https://gateway-lon.watsonplatform.net/personality-insights/api',\n",
      "    iam_apikey='oUImnq_VXfyDypESNM8rmtJOpzUzg6Zsp_TnZxmwHotR')\n",
      "\n",
      "'''\n",
      "another account credentials:\n",
      "BFsOsCFLvQgZHLicoAj_ywnJ_92h0ry0gTgKudmuDmjm\n",
      "https://gateway-lon.watsonplatform.net/personality-insights/api\n",
      "'''\n",
      "  41: valid_user_texts_unique.head()\n",
      "  42:\n",
      "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
      "user_personalities.to_csv('user_personalities.csv')\n",
      "user_personalities.head()\n",
      "  43:\n",
      "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
      "user_personalities.to_csv('user_personalities.csv')\n",
      "user_personalities.head()\n",
      "  44:\n",
      "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
      "user_personalities.to_csv('user_personalities.csv')\n",
      "user_personalities.head()\n",
      "  45: user_personalities=pd.read_csv('user_personalities.csv')\n",
      "  46:\n",
      "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
      "user_personalities.to_csv('user_personalities.csv')\n",
      "user_personalities.head()\n",
      "  47:\n",
      "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
      "user_personalities.to_csv('user_personalities.csv')\n",
      "user_personalities.head()\n",
      "  48:\n",
      "# Plot\n",
      "fig, axes = plt.subplots(2, 5, figsize=(14,6), sharex=False, sharey=False)\n",
      "colors = ['tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive',\n",
      "         'tab:blue', 'tab:purple', 'tab:red', 'tab:orange', 'tab:olive']\n",
      "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism',\n",
      "         'o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "\n",
      "for i, (axx, trait_) in enumerate(zip(axes.flatten(), traits)):\n",
      "    x = user_personalities[trait_]\n",
      "    axx.hist(x, alpha=0.8, bins=25, label=trait_, color=colors[i])\n",
      "    axx.set_title(trait_)\n",
      "\n",
      "plt.suptitle('Histogram of all users personality trait raw scores and percentiles', y=1.05, size=16)\n",
      "plt.tight_layout();\n",
      "  49:\n",
      "user_personalities.head()\n",
      "#valid_user_texts['project', 'user', 'emailAddress']\n",
      "p_cols = ['project']\n",
      "for col in user_personalities.columns:\n",
      "    p_cols.append(col)\n",
      "project_user_personalities = pd.merge(user_personalities, valid_user_texts, how = 'inner',\n",
      "                                   left_on = ['user', 'emailAddress'],\n",
      "                                   right_on = ['user', 'emailAddress'])[p_cols]\n",
      "\n",
      "print(valid_user_texts.shape[0], project_user_personalities.shape[0])\n",
      "  50:\n",
      "def boxpl(dt, x_cols, y_cols, title):\n",
      "    n = 1\n",
      "    x_cnt = len(x_cols)\n",
      "    y_cnt = len(y_cols)\n",
      "    figure = plt.figure(figsize=(18, 3.5 * x_cnt))\n",
      "    for x_ax in x_cols:\n",
      "        for i in y_cols:\n",
      "            ax = figure.add_subplot(x_cnt, y_cnt, n)\n",
      "            #ax.set_title(i)\n",
      "            g = sns.boxplot(x = dt[x_ax], y = dt[i])\n",
      "            g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
      "            plt.suptitle(title, size=16)\n",
      "            plt.subplots_adjust(bottom=0.15, wspace=0.4)\n",
      "            n = n + 1\n",
      "  51:\n",
      "y_cols = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
      "x_cols = ['project']\n",
      "title = 'Personality trait percentiles for projects'\n",
      "boxpl(project_user_personalities, x_cols, y_cols, title)\n",
      "\n",
      "y_cols = ['o_raw', 'c_raw', 'e_raw', 'a_raw', 'n_raw']\n",
      "x_cols = ['project']\n",
      "title = 'Perconality trait raw scores for projects'\n",
      "boxpl(project_user_personalities, x_cols, y_cols, title)\n",
      "  52:\n",
      "changelog_from_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['fromString']=='In Progress')\n",
      "                                    & (changelog['toString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress = changelog[(changelog['field']=='status') \n",
      "                                    & (changelog['toString']=='In Progress') \n",
      "                                    & (changelog['fromString']!='In Progress')].reset_index()\n",
      "changelog_to_inprogress.head()\n",
      "  53:\n",
      "changelog_from_inprogress['prev_status'] = np.nan\n",
      "changelog_from_inprogress['prev_status_created'] = np.nan\n",
      "changelog_from_inprogress['created'] = pd.to_datetime(changelog_from_inprogress['created'])\n",
      "changelog_to_inprogress['created'] = pd.to_datetime(changelog_to_inprogress['created'])\n",
      "\n",
      "for index, row in changelog_from_inprogress.iterrows():\n",
      "    _key = row['key']\n",
      "    _project = row['project']\n",
      "    _created = row['created']\n",
      "    _to_row = changelog_to_inprogress[(((changelog_to_inprogress['key'] == _key)\n",
      "                            & (changelog_to_inprogress['project'] == _project))\n",
      "                            & (pd.to_datetime(changelog_to_inprogress['created']) < pd.to_datetime(_created)))].sort_values('created', ascending=False).head(1)\n",
      "    for st in _to_row['fromString']:\n",
      "        _prev_st = st \n",
      "    for cr in _to_row['created']:\n",
      "        _prev_st_created = cr \n",
      "    \n",
      "    changelog_from_inprogress.loc[index,'prev_status'] = _prev_st\n",
      "    changelog_from_inprogress.loc[index,'prev_status_created'] = _prev_st_created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  54:\n",
      "changelog_from_inprogress['minutes_spent']=(pd.to_datetime(changelog_from_inprogress['created'])\n",
      "                            - pd.to_datetime(changelog_from_inprogress['prev_status_created'])) / np.timedelta64(1, 'm')\n",
      "changelog_from_inprogress.head(3)\n",
      "  55:\n",
      "user_key_timespent=changelog_from_inprogress.groupby(['key', 'project', 'author']).agg({'minutes_spent':'sum'})\n",
      "user_key_timespent.reset_index(level= [0,1,2], inplace=True)\n",
      "  56: user_key_timespent.head(3)\n",
      "  57:\n",
      "valid_users_times = pd.merge(user_personalities, user_key_timespent, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', 'minutes_spent']]\n",
      "  58: valid_users_times.head()\n",
      "  59: print(user_personalities.shape[0], valid_users_times.shape[0])\n",
      "  60:\n",
      "_percentile=0.3333\n",
      "_rownumber_within_percentile = round((int(valid_users_times.shape[0]) * _percentile))\n",
      "_top_percentile_rows_filter_value_minutes = valid_users_times.sort_values('minutes_spent', ascending=False).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "_bottom_percentile_rows_filter_value_minutes = valid_users_times.sort_values('minutes_spent', ascending=True).head(_rownumber_within_percentile).iloc[_rownumber_within_percentile-1].minutes_spent\n",
      "\n",
      "print(_rownumber_within_percentile, _top_percentile_rows_filter_value_minutes, _bottom_percentile_rows_filter_value_minutes)\n",
      "print('nr of rows from ', _percentile*100, '% to ',\n",
      "      str(100-100*_percentile), '% : ',\n",
      "      valid_users_times[(valid_users_times['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( valid_users_times['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)].shape[0])\n",
      "\n",
      "print('nr of rows above ', str(100-100*_percentile), '%: ',\n",
      "      valid_users_times[valid_users_times['minutes_spent']>=_top_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('nr of rows below, ', _percentile*100, '%: ',\n",
      "      valid_users_times[valid_users_times['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes].shape[0])\n",
      "\n",
      "print('Total number of rows in dataset: ', valid_users_times.shape[0])\n",
      "  61:\n",
      "valid_users_times['time_spending_category'] = np.nan\n",
      "\n",
      "valid_users_times.loc[(valid_users_times['minutes_spent']<_top_percentile_rows_filter_value_minutes) & \n",
      "                        ( valid_users_times['minutes_spent']>_bottom_percentile_rows_filter_value_minutes)\n",
      "                  , 'time_spending_category'] = 'medium'\n",
      "\n",
      "valid_users_times.loc[valid_users_times['minutes_spent']>=_top_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='high'\n",
      "\n",
      "valid_users_times.loc[valid_users_times['minutes_spent']<=_bottom_percentile_rows_filter_value_minutes\n",
      "                      , 'time_spending_category']='low'\n",
      "# check the actual values\n",
      "valid_users_times[['time_spending_category', 'user']].groupby('time_spending_category').count()\n",
      "  62: valid_users_times.head()\n",
      "  63:\n",
      "import pandasql as ps\n",
      "\n",
      "q1 = \"\"\"\n",
      "Select U.user, low_timespent_tasks, medium_timespent_tasks, high_timespent_tasks\n",
      "From \n",
      "    (\n",
      "    Select distinct user From valid_users_times) AS U\n",
      "    Left Join ( SELECT user, count(*) AS low_timespent_tasks\n",
      "    FROM valid_users_times AS F WHERE time_spending_category = 'low' Group By user\n",
      "    ) AS Low ON U.user = Low.user\n",
      "    Left Join (SELECT user, count(*) AS medium_timespent_tasks\n",
      "    FROM valid_users_times AS F WHERE time_spending_category = 'medium' Group By user\n",
      "    ) AS medium on U.user = medium.user\n",
      "    Left Join ( SELECT user, count(*) AS high_timespent_tasks\n",
      "    FROM valid_users_times AS F WHERE time_spending_category = 'high' Group By user\n",
      "    ) AS high ON U.user = high.user\n",
      "\"\"\"\n",
      "\n",
      "valid_user_time_agg = ps.sqldf(q1, locals())\n",
      "print(valid_user_time_agg.shape[0])\n",
      "valid_user_time_agg.head()\n",
      "  64:\n",
      "print(valid_user_time_agg[(pd.isnull(valid_user_time_agg['low_timespent_tasks'])==False)\n",
      "                         & (pd.isnull(valid_user_time_agg['medium_timespent_tasks'])==False)\n",
      "                         & (pd.isnull(valid_user_time_agg['high_timespent_tasks'])==False)].shape[0])\n",
      "\n",
      "print(valid_user_time_agg[(pd.isnull(valid_user_time_agg['low_timespent_tasks'])==False)\n",
      "                         | (pd.isnull(valid_user_time_agg['medium_timespent_tasks'])==False)\n",
      "                         | (pd.isnull(valid_user_time_agg['high_timespent_tasks'])==False)].shape[0])\n",
      "  65:\n",
      "def categorical_metric(field, multiple_fields, field_in_issues, field_exclusion_filter, values_filter, cat1_list, cat1_label, cat2_list, cat2_label, cat3_list, cat3_label):\n",
      "    #create table from the log with this specific field\n",
      "    if len(multiple_fields)>0:\n",
      "        log_dt = changelog[(changelog['field'].isin(multiple_fields))]\n",
      "    else:\n",
      "        log_dt = changelog[(changelog['field'].isin([field]))]\n",
      "    \n",
      "    #code when exclusion field values are passed\n",
      "    if len(field_exclusion_filter) > 0:\n",
      "        #merge to issues table and add the \n",
      "        log_dt = pd.merge(log_dt, issues[['key', field_in_issues]].drop_duplicates()\n",
      "                     , how='inner', left_on = 'key', right_on='key')[['author', 'created', 'field', 'fieldtype', 'from', 'fromString', 'key',\n",
      "                    'project', 'to', 'toString', field_in_issues]]\n",
      "        log_dt = log_dt[log_dt[field_in_issues]!=field_exclusion_filter]\n",
      "        log_dt = log_dt[log_dt['toString'].isin(values_filter)]\n",
      "        \n",
      "    #add new column to the previous log table for this field. Assign the values based on categories defined in list\n",
      "    log_dt[field] = np.nan\n",
      "    log_dt.loc[(log_dt.toString.isin(cat1_list)), field]=cat1_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat2_list)), field]=cat2_label\n",
      "    log_dt.loc[(log_dt.toString.isin(cat3_list)), field]=cat3_label\n",
      "    \n",
      "    #join table to user personalities table\n",
      "    valid_users_metrics = pd.merge(user_personalities, log_dt, how = 'inner',\n",
      "        left_on = 'user', right_on = 'author')[['user', 'emailAddress', 'project', 'key', field]]\n",
      "    valid_users_metrics = valid_users_metrics.drop_duplicates()\n",
      "    return valid_users_metrics\n",
      "    \n",
      "def categorical_metric_agg(data, field, cat1_label, cat2_label, cat3_label):\n",
      "    valid_users_metrics = data\n",
      "    #Aggregate the metrics per user\n",
      "    agg_query = \"\"\" SELECT U.user,\"\"\"+cat1_label+\"\"\", \"\"\"+cat2_label+\"\"\", \"\"\"+cat3_label+\"\"\", \n",
      "    CASE WHEN COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat1_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) THEN '\"\"\"+cat1_label+\"\"\"'\n",
      "         WHEN COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat3_label+\"\"\", 0) AND COALESCE(\"\"\"+cat2_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN '\"\"\"+cat2_label+\"\"\"'\n",
      "         WHEN COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat2_label+\"\"\", 0) AND COALESCE(\"\"\"+cat3_label+\"\"\", 0)>=COALESCE(\"\"\"+cat1_label+\"\"\", 0) THEN '\"\"\"+cat3_label+\"\"\"'\n",
      "    END AS metric\n",
      "    FROM (SELECT DISTINCT user FROM valid_users_metrics) AS U\n",
      "    LEFT JOIN ( SELECT user, COUNT(*) AS \"\"\"+cat1_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat1_label+\"\"\"' GROUP BY user) AS T1 ON U.user = T1.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat2_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat2_label+\"\"\"' GROUP BY user) AS T2 on U.user = T2.user\n",
      "    LEFT JOIN ( SELECT user, count(*) AS \"\"\"+cat3_label+\"\"\" FROM valid_users_metrics AS F WHERE \"\"\"+field+\"\"\" = '\"\"\"+cat3_label+\"\"\"' GROUP BY user) AS T3 ON U.user = T3.user\n",
      "    \"\"\"\n",
      "    valid_users_metrics_agg = ps.sqldf(agg_query, locals())\n",
      "    return valid_users_metrics_agg\n",
      "  66:\n",
      "statuses = changelog[changelog['field']=='status'][['toString', 'author']].groupby(\n",
      "    'toString').count().sort_values('author', ascending=False)\n",
      "statuses.reset_index(level= [0], inplace=True)\n",
      "statuses.head()\n",
      "  67:\n",
      "_todo = ['To Do', 'Open', 'Reopened', 'Reviewable', 'To Be Merged', 'Scoped', 'Refine', 'New', 'Raw'\n",
      "        , 'Waiting for Response', 'To Be Tested', 'Pending 3rd-Party']\n",
      "_inprogress = ['Pull Request Submitted', 'Planned Development', 'In Progress', 'In PR', 'In Review', 'In Review'\n",
      "               , 'Writing', 'Waiting for Review', 'Testing In Progress']\n",
      "_done = ['Closed', 'Resolved', 'Done', 'Triaged', 'Accepted', 'Inactive - Pending Closure', 'Defered']\n",
      "states_df = categorical_metric('status','','','','',_todo,'todo',_inprogress,'inprogress',_done,'done')\n",
      "states_df_agg = categorical_metric_agg(states_df, 'status','todo', 'inprogress', 'done')\n",
      "  68:\n",
      "print(states_df.shape[0], \n",
      "      states_df.drop_duplicates().shape[0],\n",
      "      states_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "states_df.head()\n",
      "  69:\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
      "                         & (pd.isnull(states_df_agg['todo'])==False)\n",
      "                         & (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
      "\n",
      "print(states_df_agg[(pd.isnull(states_df_agg['done'])==False)\n",
      "                         | (pd.isnull(states_df_agg['todo'])==False)\n",
      "                         | (pd.isnull(states_df_agg['inprogress'])==False)].shape[0])\n",
      "states_df_agg.head()\n",
      "  70:\n",
      "changelog[changelog['field']=='priority'][['toString', 'author']].groupby(\n",
      "    'toString').count().sort_values('author', ascending=False)\n",
      "  71:\n",
      "_high = ['High', 'Critical', 'Blocker']\n",
      "_medium = ['Medium', 'Major']\n",
      "_low = ['Low', 'Minor', 'None', 'Trivial', 'To be reviewed']\n",
      "\n",
      "priorities_df = categorical_metric('priority','','','','',_high,'high',_medium,'medium',_low,'low')\n",
      "priorities_df_agg = categorical_metric_agg(priorities_df, 'priority','high', 'medium', 'low')\n",
      "  72:\n",
      "print(priorities_df.shape[0], \n",
      "      priorities_df.drop_duplicates().shape[0],\n",
      "      priorities_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "priorities_df.head()\n",
      "  73:\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         & (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "\n",
      "print(priorities_df_agg[(pd.isnull(priorities_df_agg['high'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['medium'])==False)\n",
      "                         | (pd.isnull(priorities_df_agg['low'])==False)].shape[0])\n",
      "priorities_df_agg.head()\n",
      "  74:\n",
      "unique_keys = issues[['key', 'project', 'storypoints', 'fields.issuetype.name']].drop_duplicates()\n",
      "proj_story_tab = pd.crosstab(unique_keys['project'], unique_keys['storypoints'], values = unique_keys['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Greens', fmt='g')\n",
      "  75:\n",
      "unique_keys = issues[['key', 'project', 'storypoints', 'fields.issuetype.name']].drop_duplicates()\n",
      "proj_story_tab = pd.crosstab(unique_keys['project'], unique_keys['storypoints'], values = unique_keys['key'], aggfunc='count')\n",
      "proj_story_tab\n",
      "  76:\n",
      "unique_keys = issues[['key', 'project', 'storypoints', 'fields.issuetype.name']].drop_duplicates()\n",
      "proj_story_tab = pd.crosstab(unique_keys['project'], unique_keys['storypoints'], values = unique_keys['key'], aggfunc='count')\n",
      "hm = sns.set(rc={'figure.figsize':(20,4)})\n",
      "g = sns.heatmap(proj_story_tab, annot=True, cmap='Greens', fmt='g')\n",
      "g.set_yticklabels(g.get_yticklabels(), rotation=45)\n",
      "  77:\n",
      "#field_in_issues, field_exclusion_filter, values_filter\n",
      "\n",
      "story_points = ['0.5', '1', '2', '3', '5', '8', '13', '20', '40', '100']\n",
      "story_fields = ['Story Points', 'Actual Story Points']\n",
      "field = 'StoryPoints'\n",
      "e_low = ['0.5', '1', '2']\n",
      "e_medium = ['3', '5', '8', '13']\n",
      "e_high = ['20', '40', '100']\n",
      "\n",
      "\n",
      "estimates_df = categorical_metric(field,story_fields,'fields.issuetype.name','Epic',story_points,e_high,'high',e_medium,'medium',e_low,'low')\n",
      "estimates_df_agg = categorical_metric_agg(estimates_df, '`'+field+'`','high', 'medium', 'low')\n",
      "  78: estimates_df.head()\n",
      "  79: print(estimates_df.shape[0],estimates_df[['user', 'key']].drop_duplicates().shape[0])\n",
      "  80:\n",
      "print(estimates_df_agg.shape[0], \n",
      "      changelog[(changelog['field'].isin(story_fields)) & (changelog['author']=='grussell')]['key'].drop_duplicates().shape[0])\n",
      "estimates_df_agg.head(100)\n",
      "  81:\n",
      "print(estimates_df_agg[(pd.isnull(estimates_df_agg['high'])==False)\n",
      "                         & (pd.isnull(estimates_df_agg['medium'])==False)\n",
      "                         & (pd.isnull(estimates_df_agg['low'])==False)].shape[0])\n",
      "\n",
      "print(estimates_df_agg[(pd.isnull(estimates_df_agg['high'])==False)\n",
      "                         | (pd.isnull(estimates_df_agg['medium'])==False)\n",
      "                         | (pd.isnull(estimates_df_agg['low'])==False)].shape[0])\n",
      "  82: user_personalities.head()\n",
      "  83:\n",
      "user_personalities['Is_Open']           =user_personalities['openness'].apply(          lambda x: 'Yes_openness' if x>np.median(user_personalities['openness'])          else 'No_openness')\n",
      "user_personalities['Is_conscientious'] =user_personalities['conscientiousness'].apply( lambda x: 'Yes_conscientiousness' if x>np.median(user_personalities['conscientiousness']) else 'No_conscientiousness')\n",
      "user_personalities['Is_extravert']      =user_personalities['extraversion'].apply(      lambda x: 'Yes_extraversion' if x>np.median(user_personalities['extraversion'])      else 'No_extraversion')\n",
      "user_personalities['Is_agreeable']     =user_personalities['agreeableness'].apply(     lambda x: 'Yes_agreeableness' if x>np.median(user_personalities['agreeableness'])     else 'No_agreeableness')\n",
      "user_personalities['Is_neurotic']       =user_personalities['neuroticism'].apply(       lambda x: 'Yes_neuroticism' if x>np.median(user_personalities['neuroticism'])       else 'No_neuroticism')\n",
      "  84:\n",
      "estimates_assoc_df = pd.merge(user_personalities, estimates_df, how='inner', left_on = 'user', right_on = 'user')[[\n",
      "    'user','StoryPoints','Is_Open','Is_conscientious', 'Is_extravert', 'Is_agreeable', 'Is_neurotic']] \n",
      "estimates_assoc_df.head()\n",
      "  85:\n",
      "estimates_assoc_openness = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_Open']].values\n",
      "estimates_assoc_conscientiousness = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_conscientious']].values\n",
      "estimates_assoc_extraversion = estimates_assoc_df[[\n",
      "    'StoryPoints', 'Is_extravert']].values\n",
      "estimates_assoc_agreeableness = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_agreeable']].values\n",
      "estimates_assoc_neuroticism = estimates_assoc_df[[\n",
      "    'StoryPoints','Is_neurotic']].values\n",
      "  86: estimates_assoc_openness\n",
      "  87: from apyori import apriori\n",
      "  88:\n",
      "rules = apriori(estimates_assoc_openness\n",
      "                #, min_support = 0.00001\n",
      "                #, min_confidence = 0.6\n",
      "               )\n",
      "# TODO\n",
      "rules_result = list(rules)\n",
      "len(rules_result)\n",
      "  89: rules_result\n",
      "  90:\n",
      "import pandasql as ps\n",
      "print(changelog_to_inprogress.shape[0], changelog_from_inprogress.shape[0])\n",
      "  91:\n",
      "q1 = \"\"\"\n",
      "SELECT count(*)\n",
      "FROM changelog_from_inprogress AS F\n",
      "OUTTER APPLY (\n",
      "    SELECT * \n",
      "    FROM changelog_to_inprogress AS T\n",
      "    WHERE (1 = 1)\n",
      "        AND F.key = T.key \n",
      "        AND F.project = T.project \n",
      "        AND F.created > T.created\n",
      "    LIMIT 1) AS T\n",
      "\"\"\"\n",
      "\n",
      "print(ps.sqldf(q1, locals()))\n",
      "  92: estimates_assoc_openness\n",
      "  93: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandasql as ps\n",
    "print(changelog_to_inprogress.shape[0], changelog_from_inprogress.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"\"\"\n",
    "SELECT count(*)\n",
    "FROM changelog_from_inprogress AS F\n",
    "OUTTER APPLY (\n",
    "    SELECT * \n",
    "    FROM changelog_to_inprogress AS T\n",
    "    WHERE (1 = 1)\n",
    "        AND F.key = T.key \n",
    "        AND F.project = T.project \n",
    "        AND F.created > T.created\n",
    "    LIMIT 1) AS T\n",
    "\"\"\"\n",
    "\n",
    "print(ps.sqldf(q1, locals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
