{"contentItems": [{"content": "When a scheduler registers, the master will create a link from master to scheduler. If this link breaks, the master will consider the scheduler and mark it as . This causes a couple problems: 1) Master does not send offers to schedulers. But these schedulers might consider themselves \"registered\" in a one-way network partition scenario. 2) Any calls from the scheduler is still accepted, which leaves the scheduler in a starved, but state. See the related issue for more context: MESOS-5180 There should be an additional guard for registered, but schedulers here: The HTTP API already does this: Since the scheduler driver cannot return a 403, it may be necessary to return a and force the scheduler to abort.. The existing implementation of the scheduler driver does not re-register with the master under some network partition cases. When a scheduler registers with the master: 1) master links to the framework 2) framework links to the master It is possible for either of these links to break the master changing. (Currently, the scheduler driver will only re-register if the master changes). If both links break or if just link (1) breaks, the master views the framework as and . This means the framework will not receive any more events (such as offers) from the master until it re-registers. There is currently no way for the scheduler to detect a one-way link breakage. if link (2) breaks, it makes (almost) no difference to the scheduler. The scheduler usually uses the link to send messages to the master, but libprocess will create another socket if the persistent one is not available. To fix link breakages for (1+2) and (2), the scheduler driver should implement a event handler for the master's and trigger a master (re-)detection upon a disconnection. This in turn should make the driver (re)-register with the master. The scheduler library already does this: See the related issue MESOS-5181 for link (1) breakage.. Update example long running to use v1 API.. We need to modify the long running test framework similar to to use the v1 API. This would allow us to vet the v1 API and the scheduler library in test clusters.. Executor driver does not respect utor shutdown grace period.. Executor shutdown grace period, configured on the agent, is propagated to utors via the environment variable. The utor driver must use this timeout to delay the hard shutdown of the related utor.. There are two ways in which a shutdown of utor can be triggered: 1. If it receives an explicit `Shutdown` message from the agent. 2. If the recovery timeout period has elapsed, and the utor still hasnt been able to (re-)connect with the agent. Currently, the utor library relies on the field having a default value of 5 seconds to handle the second scenario. utor The driver used to trigger the grace period via a constant defined in .cpp L92 The agent may want to force a shorter shutdown grace period eviction may have shorter deadline) in the future. For now, we can just read the value via an environment variable.. Remove field from Shutdown event v1 protobuf.. We need to migrate the existing tests in to use the new callback interface introduced in . The changes to would be done when MESOS-4831 is resolved. For an example see which already uses this new interface.. My understanding is that the recommended path for the v1 scheduler API is , but the HTTP endpoint for this endpoint list the path as ; the filename of the doc page is also in the subdirectory. Similarly, we document the master state endpoint as , whereas the preferred name is now just , and so on for most of the other endpoints. Unlike we the V1 API, we might want to consider backward compatibility and document both forms sure. But certainly it seems like we should encourage people to use the shorter paths, not the longer ones.. Previously, all status update messages from the utor were forwarded by the agent to the master in the order that they had been received. However, that seems to be no longer valid due to a recently introduced change in the agent: This can sometimes lead to status updates being sent out of order depending on the order the is fulfilled from the call to .. Investigate test suite crashes after ZK socket disconnections.. Showed up on ASF CI: The test crashed with the following logs: There was another test which failed right after completion with a similar stack trace:. We need to add tests for the utor library . One possible approach would be to use the existing tests in and make them use the new utor library.. Currently, the slave process generates a process ID every time it is initialized via function call. This is a problem for testing HTTP utors as it can't retry if there is a disconnection after an agent restart since the prefix is incremented. There are a couple of ways to fix this: - Add a constructor to exclusively for testing that passes on a fixed instead of relying on . - Currently we delegate to (1) when nothing is specified as the URL in libprocess would delegate to . Instead of defaulting to (1), we can default to the last known active ID.. Introduce delete remove endpoint for quota. This endpoint is for removing quotas via the DELETE method.. Current rules around this are pretty confusing and undocumented, as evidenced by some recent bugs in this area. Some example snippets in the mesos source code that were a result of this confusion and are indeed bugs: 1.. C++ HTTP Scheduler Library does not work with SSL enabled. The C++ HTTP scheduler library does not work against Mesos when SSL is enabled (without downgrade). The fix should be simple: If SSL is enabled, connections should be made with HTTPS instead of HTTP.. If is enabled on a master, frameworks attempting to use the HTTP Scheduler API can't register. is already supported for HTTP based frameworks.. Implement AuthN handling in Master for the Scheduler endpoint. If you pass a nonsense string for \"master\" into a framework using the C++ HTTP scheduler library, the framework segfaults. For example, using the example frameworks: Results in: Results in. AFAICT, HTTP Pipelining is defined only for requests to the same . It does not guarantee that requests to two different URL's on the same server would return responses in order. In fact, this would be quite obvious by this example: Let's say your server hosts two and , and the request is first sent to , then to . It would always be possible that the computation done by the URL, would be more then and might in turn make return first then . Can you point me out to any bits in the RFC that discuss the behavior you outlined in the example ?. If the number of framework created exceeds the lib process threads then during master failover the zookeeper updates can cause deadlock. On a machine with 24 cpus, if the framework count exceeds 24 then when the master fails over all the libprocess threads block updating the cache ( GroupProcess) leading to deadlock. Below is the stack trace of one the libprocess thread : Solution: Create master detector per url instead of per framework. Will send the review request.. Currently, the HTTP Scheduler API has no concept of Sessions aka or a . This is useful in some failure scenarios. As of now, if a framework fails over and then subscribes again with the same with the option set. The Mesos master would subscribe it. If the previous instance of the tries to send a Call , with the same previous set, it would be still accepted by the master leading to erroneously killing a task. This is possible because we do not have a way currently of distinguishing connections. It used to work in the previous driver implementation due to the master also performing a check to verify if they matched and only then allowing the call.. Introduce sessions in HTTP Scheduler API Subscribed Responses. V1 API java protos are not generated. The java protos for the V1 api should be generated according to the Makefile; however, they do not show up in the generated build directory.. Add an abstraction to manage the life cycle of file descriptors. The Mesos UI is broken, it seems to fail to represent JSON from state. This may have been introduced with. Refactor Status Update method on Slave to handle HTTP based Executors. Currently, our testing infrastructure does not have a mechanism of HTTP events of a particular type from the Scheduler API response stream. We need a abstraction that can help us to filter a particular event type. This helper code is duplicated in at least two places currently, Scheduler Primitives tests. - The solution can be as trivial as moving this helper function to a common test-header. - Implement a similar to what we do for other protobufs via .. Currently , in libprocess, does not support HTTP pipelining. Each call as of know sends in the header, thereby, signaling to the server to close the TCP socket after the response. We either need to create a new interface for supporting HTTP pipelining , or modify the existing to do so. This is needed for the library to make sure \"Calls\" are sent in order to the master. Currently, in order to do so, we send in the next request only after we have received a response for an earlier call that results in degraded performance.. Agent : Create Basic Functionality to handle call endpoint. This is the first basic step in ensuring the basic call functionality: - Set up the route on the agent for utor\" endpoint. - The endpoint should perform basic validation and return for now. - Introduce initial tests in that just verify the status code.. call endpoint on the slave will return a 202 accepted code but has to do some basic validations before. In case of invalidation it will return a back to the client. - We need to create the required infrastructure to validate the request and then process it similar to in the check if the protobuf is properly initialized, has the required attributes set pertaining to the call message etc.. This was needed for testing the call HTTP endpoint on master. Submitted a diff up for review : ( is still a work in progress ). Implement the Events stream on slave for Call endpoint. ", "contenttype": "application/json", "created": 737603, "id": 51, "language": "en", "user_name": "anandmazumdar", "email": "anand@mesosphere.io"}]}