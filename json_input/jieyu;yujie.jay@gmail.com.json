{"contentItems": [{"content": "Currently, if a container uses container image, we'll do a bind mount of its sandbox ( - in the host mount namespace. However, doing the mounts in the host mount table is not ideal. That complicates both the cleanup path and the recovery path. Instead, we can do the sandbox bind mount in the container's mount namespace so that cleanup and recovery will be greatly simplified. We can setup mount propagation properly so that persistent volumes mounted at xxx can be propagated into the container. Here is a simple proof of concept: Console 1: Console 2: Console 1: Console 2:. does not handle empty string correctly.. Executor env variables should not be leaked to the command task.. Currently, command task inherits the env variables of the command utor. This is less ideal because the command utor environment variables include some Mesos internal env variables like MESOS XXX and Also, this behavior does not match what Docker containerizer does. We should construct the env variables from scratch for the command task, rather than relying on inheriting the env variables from the command utor.. The rationale behind this change is that many of the image specifications Docker Appc) are not just for filesystems. They also specify runtime configurations environment variables, volumes, etc) for the container. Provisioner should return those runtime configurations to the Mesos containerizer and Mesos containerizer will delegate the isolation of those runtime configurations to the relevant isolator. Here is what it will be look like eventually. We could do those changes in phases: 1) Provisioner will return a ProvisionInfo which includes a 'rootfs' and image specific runtime configurations (could be the Docker Appc manifest). 2) Then, the Mesos containerizer will generate a (a protobuf which includes rootfs, sandbox, docker appc manifest, similar to OCI's host independent and pass that to each isolator in 'prepare'. Imaging in the future, a takes the docker manifest from and prepare the container. 3) The isolator's prepare function will return a (contains environment variables, namespaces, which will be used by Mesos containerize to launch containers. Imaging that information will be passed to the launcher in the future. We can do the renaming - later.. So that when a user task is forked, it does not hold extra references to the sandbox mount and provisioner bind backend mounts. If we don't do that, we could get the following error message when cleaning up bind backend mount points and sandbox mount points.. When running the tests as root, we found fails consistently on some platforms. Turns out that the 'rmdir' after the 'umount' fails with EBUSY because there's still some references to the mount. FYI. Removing mount point fails with EBUSY in. This is related to MESOS-3095 and MESOS-3227. The idea is that we should allow command utor to run under host filesystem and provision the filesystem for the user. The command line utor will then chroot into user's root filesystem. This solves the issue that the command utor is not launchable in the user specified root filesystem. The design doc is here:. Appc spec specifies two image discovery mechanisms: simple and meta discovery. We need to have an abstraction for image discovery in AppcStore. For MVP, we can implement the simple discovery first. Update: simple discovery is removed from the spec. Meta discovery is the only discovery mechanism right now in the spec. Simple discovery is already shipped (we support an arbitrary operator specified ). So this ticket should focus on implementing Meta discovery.. Implement a utility for computing hash. Syscall 'pivot root' requires that the old and the new root are not in the same filesystem. Otherwise, the user will receive a \"Device or resource busy\" error. Currently, we rely on the provisioner to prepare the rootfs and do proper bind mount if needed so that pivot root can succeed. The drawback of this approach is that it potentially pollutes the host mount table which requires cleanup logics. For instance, in the test, we create a test rootfs by copying the host files. We need to do a self bind mount so that we can pivot root on it. That pollute the host mount table and it might leak mounts if test crashes before we do the lazy umount: What I propose is that we always perform a recursive self bind mount of rootfs itself in (after enter the new mount namespace). Seems that this is also done in libcontainer:. Marco, can you re-run the failing tests using and paste the results here? Thanks. Implement OverlayFS based provisioner backend. Implement copy based provisioner backend. Support pre-fetching images. Default container images can be specified with the flag to the slave. This may be a large image that will take a long time to initially when the first container is provisioned. Add optional support to start fetching the image when the slave starts and consider not registering until the fetch is complete. To extend that, we should support an operator endpoint so that operators can specify images to pre-fetch.. Resource estimator obviously need this information to calculate, say the usage slack. Now the question is how. There are two approaches: 1) Pass in the allocated resources for each utor through the interface. 2) Let containerizer return total resources allocated for each container when 'usages()' are invoked. I would suggest to take route (1) for several reasons: 1) Eventually, we'll need to pass in slave's total resources to the resource estimator (so that RE can calculate allocation slack). There is no way that we can get that from containerizer. The slave's total resources keep changing due to dynamic reservation. So we cannot pass in the slave total resources during 2) The current implementation of usages() might skip some containers if it fails to get statistics for that container (not an error). This will cause in-complete information to the RE. 3) We may want to calculate 'unallocated total - allocated' so that we can send allocation slack as well. Getting 'total' and 'allocated' from two different components might result in inconsistent value. Remember that 'total' keeps changing due to dynamic reservation.. Implement AppC image provisioner.. Introduce filesystem provisioner abstraction. Right now, the resource monitor returns a Usage which contains ContainerId, ExecutorInfo and In order for resource estimator qos controller to calculate usage slack, or tell if a container is using revokable resources or not, we need to expose the Resources that are currently assigned to the container. This requires us the change the containerizer interface to get the Resources as well while calling 'usage()'.. commit Author: Jie Yu Date: Wed May 6 2015 -0700 Added resources estimator abstraction for Review:. Right now, we use a sleep command to control the duration of perf sampling: This causes an additional process the sleep process) to be forked and causes troubles for us to terminate the perf sampler once the slave exits (See MESOS-2462). Seems that the additional sleep process is not necessary. The slave can just monitor the duration and send a SIGINT to the perf process when duration elapsed. This will cause the perf process to output the stats and terminate.. This is tricky in the case when a persistence id is re-used. When a persistent volume is destroyed explicitly by the framework, master deletes all information about this volume. That mean the master no longer has the ability to check if the persistence id is re-used (and reject the later attempt). On the slave side, we'll use some GC policy to remove directories associated with deleted persistent volumes (similar to how we GC sandboxes). That means the persistent volume directory won't be deleted immediately when the volume is destroyed by the framework explicitly. When the same persistence id is reused, we'll see the persistent volume still exists and we need to cancel the GC of that directory (similar to what we cancel the GC for meta directories during runTask).. Maintain persistent disk resources in master memory.. Maintain an in-memory data structure to track persistent disk resources on each slave. Update this data structure when slaves etc.. Looks like the TX RX network stats reported is the reverse of the actual network stats. The reason is because we simply get TX RX data from veth on the host. Since veth pair is a tunnel, the ingress of veth on host is the egress of eth0 in container (and vice versa). Therefore, we need to flip the data we got from veth.. As we introduce DiskInfo and reservation for Resource. We need to change the C++ Resources abstraction to properly deal with merge split of resources with those additional fields. Also, the existing C++ 'Resources' interfaces are poorly designed. Some of them are confusing and unintuitive. Some of them are overloaded with too many For instance, This interface in non-intuitive because A < B doesn't imply !(B < A). This one is also non-intuitive because if 'left' is not compatible with 'right', the result is 'left' (why not right???). Similar for operator '-'. This one assume Resources is flattened, but it might not be. As we start to introduce persistent disk resources (MESOS-1554), things will get more complicated. For example, one may want to get two types of 'disk()' functions: one returns the ephemeral disk bytes (with no disk info), one returns the total disk bytes (including ones that have disk info). We may wanna introduce a concept about Resource that indicates that a resource cannot be merged or split atomic?). Since we need to change this class anyway. I wanna take this chance to refactor it.. Refactor the C++ Resources abstraction for DiskInfo. Expose RTT in container stats. I would like to volunteer to be the release manager for which will be releasing the following major features: - Docker support in Mesos (MESOS-1524) - Container level network monitoring for mesos containerizer (MESOS-1228) - Authorization (MESOS-1342) - Framework rate limiting (MESOS-1306) - Enable building against installed third-party dependencies (MESOS-1071) I would like to track blockers for the release on this ticket.. If a network partition occurs between a Master and Slave, the Master will remove the Slave (as it fails health check) and mark the tasks being run there as LOST. However, the Slave is not aware that it has been removed so the tasks will continue to run. (To clarify a little bit: neither the master nor the slave receives 'exited' event, indicating that the connection between the master and slave is not closed). There are at least two possible approaches to solving this issue: 1. Introduce a health check from Slave to Master so they have a consistent view of a network partition. We may still see this issue should a one-way connection error occur. 2. Be less aggressive about marking tasks and Slaves as lost. Wait until the Slave reappears and reconcile then. We'd still need to mark Slaves and tasks as potentially lost (zombie state) but maybe the Scheduler can make a more intelligent decision.. ", "contenttype": "application/json", "created": 737603, "id": 48, "language": "en", "user_name": "jieyu", "email": "yujie.jay@gmail.com"}]}