{"contentItems": [{"content": "Upgrade to SHDP. As a developer, I'd like to upgrade to GA release, so I can leverage the latest improvements without breaking backwards compatibility. SHDP uses Boot and HDP and CDH versions that drop older Hive support. To avoid breaking changes we should instead use SHDP that has backported any improvements that we need as well as move Spring and Hadoop versions to more recent ones.. We are seeing JDBC connection pool errors when running 'jdbchdfs' jobs and 'jdbc' streams. The exception is: A workaround is to specify when creating the job. This has also been reported on SO (. As an XD module developer I would like to use HDFS for my custom module location even when my namenode is configured for HA. We had an issue filed in the project: \"It seems like custom module doesn't pickup namenode HA? and still use see:. Upgrade SCSM hdfs sink to SHDP. create BOMs for the major distributions we want to support * Integration tests to simulate successful load. There is a vulnerability in Groovy that is fixed in Remote ution of untrusted code See:. As a user I would like to be able to configure the logging directory to be outside of what is defined as The logging directory is currently hard coded as . This would be useful for RPM installations where the logs really should be going to instead of the current location.. As a user I would like to connect the Sqoop batch job to Teradata for import jobs. I have tried the Teradata JDBC driver directly using: but that results in an NPE. The only way so far is to use the Hortonworks Connector for Teradata - That one allows me to use the following:. Update with settings for HDP. Definitions: job create pollHdfs \"filepollhdfs true stream create csvStream \"file Here is the exception:. Running XD on YARN on PHD Ambari install. Uploading and submitting a custom job fails with the following: Same example jar works fine when submitted from XD cluster.. Update YARN deployment classpath settings for HDP and PHD. Starting teh shell without having admin running on results in the following message: Running gives a nasty stacktrace though, so these instructions are misleading and should be changed. Sqoop list-tables doesn't work oob. Add support for using Sqoop metastore. The hdfs sink doesn't recover after error writing to hdfs. Steps to reproduce - create a stream using hdfs sink with a small rollover: stop the datanode(s) and wait for an exception like: start the datanode(s) again, the sink never recovers and has to be undeployed and redeployed.. Update to SHDP for fixing hdfs store writer to recover after error writing to hdfs. As a build manager, I'd like to have Spring XD RPMs published in repository so that users can directly download the bits without having to go through appsuite repo or the EULA.. I'm running a jdbchdfs job with 8 partitions and 2 containers. Some steps complete ok while some (3-4 on average) fail with a connection pool error (see below). This happens with a decent size table rows). I tried two different databases - Oracle 11g on a separate server and MySQL running locally where the XD containers where running. Same pattern with both databases.. Frequent connection pool errors with jdbchdfs jobs. Create a Batch example to demonstrate JDBC- HDFS. Having problems testing against the Sandbox We need to set the following properties:. Make Sqoop job and MapReduce samples work with Hortonworks HDP single-node cluster. XD should use same hadoop security keys as Spring for Apache Hadoop. Add an \"xd-yarn info\" command to list admin servers and ports. As a user deploying XD on YARN I need a convenient way to get info like the admin port for my current deployment. Best way, for now, would be to add an info command to the xd-yarn script. With the latest changes the admin server runs on a random port when we deploy to YARN. In order for the user to connect they would have to query Zookeeper. This is inconvenient.. The Sqoop tasklet introduced an implementation in XD-2430. We are now adding similar support to Batch in BATCH-2329, BATCH-2330 and BATCH-2331. We should refactor Sqoop and Spark tasklets to use the as base.. Refactor Spark and Sqoop tasklets to use. Upgrade to Spring Boot. As to prepare for release, we would like to upgrade to Spring Boot (RC) (depends on Spring so that we can leverage the new features, enhancement and bug fixes.. The jdbc sink is currently limited to handling the entire payload as a string and converting a single json object to row data. We should improve that and support the following input types: - (single row) - List (multiple rows as a batch insert) - JSON string (single row) - JSON array (multiple rows as a batch insert) - none of the above use The above matches what the new jdbc source puts out (depending on outputType used). Looks like the isn't used when compressing files with bzip2, some use cases requirer bz2 instead of bzip2 as the extension. Also, should be the default extension. At the same time we should change the default gzip extension to. Problem using twittersearch when the system where the XD container is running has two network interfaces. With the following config: eth0 local network, resolves `hostname` eth1 internet network I get an error deploying the stream: If I flip the network interfaces to be: eth0 internet network, resolves `hostname` eth1 local network then it seems to work.. Modules that depend on HttpClient fail when running on YARN on Hadoop and later. Custom location for not working. tried local after setting have my twitter stuff in in that directory but not picked up by the twitterstream module Also not working for me deploying on YARN, this used to work at some point, not sure how long ago I actually tested this part - M6 M7? The setting used for YARN deployment:. Connection pool settings need to be in their own section in. The define this in the beginning - uncommenting this will any changes made earlier in the section since it defines again should either be removed or in separate section. Update configuration options. Need a re-write of the configuration files for YARN deployments. Trying to deploy the hashtagcount batch sample to Hadoop or Hortonworks HDP fails with an exception. Looks like a Guava versioning issue - Swapping out for in the xd lib directory solves it. Mark P suggested we try which is what Curator uses and that seems to work as well. Looking into changing the build to not force which is the IO platform version. I get the following exception:. This only happens when creating jobs via the CLI and deploying using the UI On the job page: I click for a Job and get a screen asking for Container Match Criteria and Job Module Count - clicking on the button on that screen does nothing - I see this error reported: Deploying Job Definition undefined TypeError: Cannot read property of undefined at ( at ( at ( at ( at ( at (. Undeploying twitterstream logs warning -. To reproduce - Download recent snapshot - Start XD and shell - create tweets \"twitterstream file\" undeploy tweets (Note: the has been fixed for RC1, still need to fix the There is an error logged in the logs:. Module info for jdbc sink and jobs are unreadable. The filejdbc job is broken in distributed mode (redis and rabbit) To reproduce: export start xd-admin start xd-container start shell and create this job: results in JOB starting but never completing: Steps: When using Redis, I also get this stacktrace in container:. Clean up duplicated dependencies from XD on YARN installation. Going forward it seems that providing Hadoop v1 will be of lesser importance and we might as well drop it now. SHDP will also drop any v1 support. Remove support for: - hadoop12 - Apache Hadoop - cdh4 - Cloudera CDH - hdp13 - Hortonworks Data Platform Keep: - hadoop22 - Apache Hadoop (default) - phd1 - Pivotal HD - phd20 - Pivotal HD - cdh5 - Cloudera CDH - hdp21 - Hortonworks Data Platform This should make configuration and documentation easier too. Not to mention testing. This affects startup scripts and the shell plus the build script.. New job that utes SQL script using JDBC. Each Hadoop distro uses different settings for and we should provide some starting points for the distros we support running XD on YARN for. We should add a commented out stub entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros.. We should add an easy way to configure the memory. Currently we only have the number of YARN containers configurable without diving into Spring YARN Boot specific config options. Proposing we do:. Not sure why the Hadoop classes are on the admin servers classpath. There is no way to select the distro, and the Hadoop classes shouldn't be needed except for module info for hdfs sink (see XD-1701). Add configuration for the partition strategy to HDFS sink to support writing files into subdirectories based on a partition key provided in the header or field in the message of the stream data. The writing using HDFS Store DataWriter should pass in the partition key value to be used for the write operation. Partition configuration could be made available to the sink using a parameter: that could then be used in XML config like: Similar to the time source.. When starting and stopping xd containers there are entries left in the directory that will cause 'runtime modules' command to fail. modules Command failed KeeperErrorCode NoNode for here the container is no longer running, but there is some data left over.. I get this: so far I have seen this with hdp13 and hadoop12 same command works fine using shell from M5 release. Get exception when accessing cdh4 from shell - This is supposed to be overridden by subclasses. at most likely due to being on the main classpath now Full stack trace:. Update dependency and add new Hadoop distros. Update to Spring for Apache Hadoop RC3 Add support for new hadoop distros: - Pivotal HD (phd20) - Hortonworks HDP (hdp21) - Cloudera CDH5 (cdh5). Sometimes getting NPE when master step runs for ftphdfs job. Tried deploying some batch jobs and they all seem to fail when running admin and one container using redis as transport create mongojob \"hdfsmongodb fails with this: WARN - Error handling failed (Error creating bean with name Initialization of bean failed; nested exception is has not been refreshed yet) ERROR - error occurred in message handler at Caused by: Error creating bean with name Cannot create inner bean '(inner bean)' of type while setting bean property 'listeners' with key ; nested exception is Error creating bean with name '(inner bean) 4': FactoryBean threw exception on object creation; nested exception is interface is not visible from class loader 18 more Caused by: Error creating bean with name '(inner bean) 4': FactoryBean threw exception on object creation; nested exception is interface is not visible from class loader at 41 more Caused by: interface is not visible from class loader at 43 more I'll post more errors as I collect them. Switch driver for Redis. The Spring Data team recommends using the Jedis driver since the Lettuce driver hasn't had any update activity for several months. Jedis is actively maintained. We might also want to investigate Redisson which is a fork of Lettuce -. Add way to provide module config options for XD on YARN. There seems to be some intersection with the work for this issue and the of how module properties are handled. There will be changes to management support such that each module (source, sink, etc) will be able to also be overridden in (or wherever points to. The HDFS sink module for example, will have default values based on it's and will be of the form .. That means in the configuration for sink, there would be a config section such as With default values defined by a class. The module file would not contain any references to a properties file. A file specified by could override the values in a config section such as sink: hdfs: : : etc.. Add YARN specific code based on Janne's prototyping Add YARN Client and AppMaster and startup config files This includes shell scripts to deploy XD to YARN Test working on Apache distribution We can modify config files, everything should be possible to override by providing command-line args or env variables.. Add XD deployment for YARN. Rename avro sink to hdfs-dataset and add support for parquet format. Create EC2 AMI for single-node install of Cloudera CDH. hdfsjdbc throws an exception: The hdfsjdbc job uses 'columns' instead of 'names' as the parameter for the column-names. Should we make this usage consistent between jobs? There is a comment in the docs - \"there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop and will be fixed in the Think this is this solved in Spring Hadoop now? should default to false now to be consistent with jdbc sink Rename to since these aren't just for import. filejdbc throws an exception: This can be solved by using a prefix Maybe just update the docs?. In the shell: job launch doesn't do completion of this is different behavior compared to job destroy typing 'job destroy' and hitting tab completes with ' typing 'job launch' and hitting tab does nothing. The batch jobs use different defaults compared to some of the sink source modules. filehdfs puts data in a data directory with files named after the stream using a .log file extension. The hdfs sink puts files in an xd directory using .txt as the default file extension. filehdfs needs a more descriptive naming. Align filehdfs batch job defaults with those of corresponding hdfs sink. Update to when it is released and remove the temporary in We should also review the supported hadoop distros - think we should support anything that is - hadoop12 - hadoop22 - phd1 (PHD - hdp13 - hdp20 - cdh4. The HDFS Sink should support writing POJOs to HDFS using Parquet. Support writing lines of text separated by a delimiter Support writing a CSV variables), TSV (tab-separated variables), No compression. We used to have a shared dependency in the lib dir. That's no longer there so hadoop distros that require this now fail (at least any hadoop based ones) We should also upgrade to current Hadoop versions (Hadoop stable). The HDFS Sink should support compressing files as they are copied. The HDFS Sink should support copying File payloads. We should support payloads in order to support non-textual file and large text file payloads being uploaded to HDFS. Currently text file payloads are converted to a text stream in memory and, non-String payloads are converted to JSON first, using an Ultimately we need to support streams such as \"file hdfs\" where the actually payload being copied to HDFS is not necessarily JSON or textual. Need to be able to support headers in the message that will indicate which HDFS file the data should be stored in.. We should adjust our options to the ones supported in the new - hadoop12 (default), cdh4, hdp13, phd1, hadoop20 This includes updating the wiki pages. Fix classpath error caused by multiple conflicting servlet-api jars. Create script to extract table data from JSON based on a given HAWQ table structure. Provide n strategy interface to obtain the key used when writing SequenceFiles. Writing POJOs using CDK Data (Avro) We should support both partitioned and un-partitioned Write POJO using - un-partitioned - partitioned date Document limitations in terms of which Java types are supported and not supported by the Avro serialization. The HDFS Sink should support writing POJOs to HDFS using Avro Serialization. The HDFS Sink should support a number of rollover options. The HDFS Sink should support a file naming strategy to distinguish between file currently being written and completed files. This is the basic setup of the commands file - no specific command implementations. Provide a client library for XD REST API. ", "contenttype": "application/json", "created": 737603, "id": 5, "language": "en", "user_name": "thomas.risberg"}]}