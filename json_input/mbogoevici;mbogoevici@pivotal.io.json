{"contentItems": [{"content": "Add support for custom headers with the Kafka bus. The Kafka sink should not make use of the message headers sent by the Kafka receivers in the Kafka bus. Similarly, the headers received from the Kafka source should not be propagated when sending to the Kakfa bus.. Do not include optional dependencies automatically via 'includes'. As a developer, I'd like the 'includes' feature of the module launcher not to include optional dependencies, so that I can have better control over what gets added to the class path.. Bootify ModuleLauncher. As a developer, I'd like to bootify , so I can use Spring Boot's support for property, setting, as well as adding options and new functionality in the future, such as CP augmentation.. As a developer, I'd like to create an annotation () driven programming model for modules, so instead of explicitly defining I O channels as beans on the module, for classes annotated with , the application would be responsible for creating the actual channel beans and channel adapters vs. the developer creating concrete channel instance types. The and annotations will be used to indicate the input and output channels of the module.. is ignored by the consumer, so downstream modules end up listening to fewer partitions. Message Bus optimizations (Kafka + Redis). Spike: Research mechanism for partitioned job management. The current implementation of partitioned job management is entirely based on message exchange over the message bus, in a request reply scenario. This creates challenges when it comes to using certain types of transports, as well as acknowledging crashes. To that effect, the option of using a different partitioned job coordination strategy, that relies on a distributed computing coordination mechanism such as ZooKeeper should be investigated.. How to reproduce: 1. Run xd-singlenode (for which setting the Spark master URL to 'local' is a requirement). Use more than 1 worker thread. 2. Deploy the word-count example 3. Create a stream 4. Send data the result INFO Executor task launch worker-3 - (e,1) INFO Executor task launch worker-1 - (d,1) INFO Executor task launch worker-2 - (b,1) INFO Executor task launch worker-1 - (g,1) INFO Executor task launch worker-1 - (a,1) INFO Executor task launch worker-2 - (b,1) INFO Executor task launch worker-3 - (c,1) (the last three results are coming from the second invocation)) Note: there seems to be a correlation between the number of values emitted and the number of workers, as, in all the attempts, there aren't more values emitted than the number of workers.. Add more comprehensive tests for the simple consumer-based Kafka Message Bus. RabbitMQ Sink is throwing: ERROR - Application startup failed Line 19 in XML document from class path resource is invalid; nested exception is lineNumber: 19; columnNumber: 53; Attribute is not allowed to appear in element at Caused by: lineNumber: 19; columnNumber: 53; Attribute is not allowed to appear in element at 30 more ERROR - Exception deploying module Line 19 in XML document from class path resource is invalid; nested exception is lineNumber: 19; columnNumber: 53; Attribute is not allowed to appear in element at Caused by: lineNumber: 19; columnNumber: 53; Attribute is not allowed to appear in element at. Rabbit Message Bus is throwing: 1 2 3 4 5 6 7 8 9 46 INFO - Deployment status for stream 'foo': interface is not visible from class loader at Caused by: interface is not visible from class loader 28 more. Rabbit Message Bus is throwing: INFO - Deployment status for stream 'foo':. Add partition allocation support for Kafka source. As a user, I want to be able to control the partition allocation for the Kafka source modules when a stream is deployed, so that I can colocate with other data sources.. Add starting offset support for Kafka source. A Kafka message source with support for starting offset and partition allocation for multiple modules exists. As a user, I want to be able to control the starting offset of the Kafka source when a stream is deployed, so that I can replay a topic if necessary. Note: - starting offset is only considered when the stream is deployed - progress made by modules must survive their crash for a running stream - undeploying and redeploying a stream with a specific start offset will cause the stream to read again from the start TBD: what happens when streams are - where do they resume from?. The field exists and it is referred to in but it does not have a setter and the bus will always use the configured default, which is 1.. Implementing a configuration change UI. A working proof of concept exists, addressing the following requirements: - configuration is centrally managed, so that admin and container nodes can be started with just a reference to the configuration server(s); - running components can be notified of configuration changes and can adapt accordingly; - the solution allows for modifying the configuration properties through a future administration UI; - the solution takes fault tolerance into account - either by design or by providing a clear roadmap for incorporating that; - access to central configuration requires authentication;. The HTTP Source creates the inefficiently. The used by the HTTP source should cache expensive objects used by the between requests, because creating them every time is inefficient (and in the case of HTTPS it can become even more expensive).. HTTPS Source Configuration issues. 1. The sample file that is included in the distribution contains the line: The correct key value is . This issue causes HTTPS sources to deploy, but not bind to the port. 2. The password is always defaulting to \"secret\". Two roles such as Admin or Viewer will be included Viewer to include just the GET access * Roles will be mapped to user groups. Batch jobs using Spring Batch MongoDB support require adding MongoDB dependencies to the shared libs folder. ", "contenttype": "application/json", "created": 737603, "id": 7, "language": "en", "user_name": "mbogoevici", "email": "mbogoevici@pivotal.io"}]}