{"contentItems": [{"content": "I am not sure a single end point serve any purpose here. The endpoints need to be canonical since the operator would want to rebootstrap each of these entities (isolators) individually (there is no dependency between them. This implies that if we have a single end point something will have route the requests to right isolator (probably the which will automatically happen if at the `libprocess` level if we separate end points for each isolator.. Currently there are no default `network` isolators for With the development of the `network cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator.. Setup proper etc hostname, etc hosts and for containers in network cni isolator.. Most docker and appc images wish to expose ports that micro-services are listening on, to the outside world. When containers are running on bridged (or ptp) networking this can be achieved by installing port forwarding rules on the agent (using iptables). This can be done in the `network cni` isolator. The reason we would like this functionality to be implemented in the `network cni` isolator, and not a CNI plugin, is that the specifications currently do not support specifying port forwarding rules. Further, to install these rules the isolator needs two pieces of information, the exposed ports and the IP address associated with the container. Bother are available to the isolator.. But how would the CNI isolator know if the underlying plugin has the capability or not? Or for that matter the parameters that it needs to pass to the plugin?. Introduce a port field in in order to set exposed ports for a container.. Networking isolators such as `network cni` need to learn about ports that a container wishes to be exposed to the outside world. This can be achieved by adding a field to the protobuf and allowing the to set these fields to inform the isolator of the ports that the container wishes to be exposed.. Get container status information in slave.. As part of MESOS-4487 an interface will be introduce into the to allow agents to retrieve container state information. The agent needs to use this interface to retrieve container state information during status updates from the utor. The container state information can be then use by the agent to expose various isolator specific configuration (for IP address allocated by network isolators, net cls handles allocated by isolator), that has been applied to the container, in the endpoint.. The isolator is responsible for allocating network handles to containers launched within a net cls cgroup. The isolator needs to expose these handles to the containerizer as part of the when the containerizer queries the status() method of the isolator. The information itself will go as part of a `CgroupInfo` protobuf that will be defined as part of MESOS-4488 .. The isolator needs to expose handles in the ContainerStatus. Introduce status() interface in `Containerizer`. In the Containerizer, during container isolation, the isolators end up modifying the state of the containers. Examples would be IP address allocation to a container by the 'network isolator, or net cls handle allocation by the cgroup net cls isolator. Often times the state of the container, needs to be exposed to operators through the end-point. For operators or frameworks might want to know the IP-address configured on a particular container, or the net cls handle associated with a container to configure the right TC rules. However, at present, there is no clean interface for the slave to retrieve the state of a container from the Containerizer for any of the launched containers. Thus, we need to introduce a `status` interface in the base class, in order for the slave to expose container state information in its. As part of implementing the net cls cgroup isolator we need a mechanism to manage the minor handles that will be allocated to containers when they are associated with a net cls cgroup. The network-handle manager needs to provide the following functionality: a) During normal operation keep track of the free and allocated network handles. There can be a total of 64K such network handles. b) On startup, learn the allocated network handle by walking the net cls cgroup tree for mesos and build a map of free network handles available to the agent.. The net cls cgroup associates a 16-bit major and 16-bit minor network handle to packets originating from tasks associated with a specific net cls cgroup. In mesos we need to give the operator the ability to fix the 16-bit major handle used in an agent (the minor handle will be allocated by the agent. See MESOS-4345). Fixing the parent handle on the agent allows operators to install default firewall rules using the parent handle to enforce a default policy (say DENY ALL) for all container traffic till the container is allocated a minor handle. A simple way to achieve this requirement is to pass the major handle as a flag to the agent at startup.. ", "contenttype": "application/json", "created": 737603, "id": 47, "language": "en", "user_name": "avinash@mesosphere.io"}]}