{"contentItems": [{"content": "A module developer can specify a general description with hyperlinks for a module. A module user can see the description via . A REST API consumer should be able to access the general module description as well.. It should be possible to configure a (short) description for a module that is display above the module options via . The description could contain a few lines describing the core functionality and potentially hyperlinks to additional information for a module. This information should be exposed via the REST interface as well. Currently only the module options are printed.. Often one has to perform some basic conversion parsings in Stream definitions. It would be helpful if one could provide some helper functions to simplify SpEL expressions. instead of: it would be nice to be able to write: I'm thinking of support for: parseInt parseLong parseDouble parseTuple (I don't think we'd need support for This issue is about: 1) providing the centralised infrastructure for defining the SpEL expressions 2) Add support for the above listed predefined SpEL expressions Those functions should be able to work with String based as well as as input.. The current representation of REST resources of time-series data aggregate counter) can lead to problems in consuming applications. Despite the time series data provided by the \"counts\" data structure is logically ordered by key (timestamps) it doesn't guarantee an ordering, since many consuming applications interpret JSON data as an unordered map like data structure. Because of this consuming applications have to apply special ordering transformation logic to get the data in an ordered fashion. It would be helpful if one could configure the rendering of the time series data, as a list of json object like: Where denotes the timestamp and denotes the value. It would also be helpful if one could adjust the date format either with a pattern or a well known date format like, . I attached a python example for this that demonstrates the problem. Steps to reproduce: Create stream Create tap on stream with Post some http data Display aggregate counter Install python Requests library (REST support) Start a python console (or and run the following program: The above program should result in a similar output, but as one can see, due to pythons interpretation of the JSON object as a dict, the order of the keys in the output got mixed up. Instead of showing the counts and then 1 as in the example above. This is just one example of how the current representation of the rest resource could lead to problems in consuming applications.. Often it is useful to have access to the median value for fields of a data stream since they are more robust with respect to outliers. The median is defined as the value of a dataset such that, when sorted, 50% of the data is smaller than the value and 50% of the data is larger then the value. Ordinarily this is difficult to calculate on a stream because it requires the collection and sorting of all data. The median of a data stream can be approximated with a technique called stochastic averaging. To approximate the median value of a data stream one could use the following approach: Given the current estimate of the median . If the next observed value in the stream is larger than , increase the current estimate by ( the learning rate). If it is smaller, decrease the estimate by . When is close to the median, it increases as often as it decreases, and therefore it stabilizes. The following example shows a primitive implementation of the above mentioned algorithm in groovy (to be placed under Stream definition: Post some JSON Output: After the median value should stabilize. This approach was taken from the book \"Real-time Analytics - Techniques to Analyze and Visualize Streaming Data\" P. 296 Byron Ellis Wiley Open points: - Support for resetting the median value - Better state management (Redis?) - Support median estimation for multiple fields - Make learning rate configurable from outside - Maybe add this as aggregation strategy?. In some scenarios like when performing exploratory data-analysis on streaming data one often create a stream, keep it running for some time (or until some condition is met) and then stop the stream and start to investigate the collected data. It would be cool to be able to specify some undeploy condition, like a timeout after x minutes, no. of events collected, a specific counter past a given threshold, file-size greater then x etc.. Create a simple sample application for the jpmml module. Create documentation for the core analtyical model abstractions and use of jpmml processor. Create a JPMML module that will evaluate a model.. Create subproject. ", "contenttype": "application/json", "created": 737603, "id": 14, "language": "en", "user_name": "thomasd"}]}