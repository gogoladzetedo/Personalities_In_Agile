{"contentItems": [{"content": "The existing implementation of the scheduler driver does not re-register with the master under some network partition cases. When a scheduler registers with the master: 1) master links to the framework 2) framework links to the master It is possible for either of these links to break the master changing. (Currently, the scheduler driver will only re-register if the master changes). If both links break or if just link (1) breaks, the master views the framework as and . This means the framework will not receive any more events (such as offers) from the master until it re-registers. There is currently no way for the scheduler to detect a one-way link breakage. if link (2) breaks, it makes (almost) no difference to the scheduler. The scheduler usually uses the link to send messages to the master, but libprocess will create another socket if the persistent one is not available. To fix link breakages for (1+2) and (2), the scheduler driver should implement a event handler for the master's and re-register in this case. See the related issue MESOS-5181 for link (1) breakage.. The s have a custom helper for setting up ACLs in the : This is no longer necessary with implicit roles.. fails on CentOS 6. sometimes fails on CentOS 7 with this kind of output: often has this output: Whether SSL is configured makes no difference. This test may also fail on other platforms, but more rarely.. Tests that use the test helper are generally fragile when the test fails an assert expect in the middle of the test. This is because the helper takes raw pointer arguments, which may be In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup. The test cleanup may dereference some of these destroyed objects, leading to a test crash like: The helper should take arguments instead. This also means that we can remove the helper from most of these tests.. Mostly copied from A subprocess inheriting the environment variables may run into some accidental fatalities: Subprocess uses libprocess Subprocess is something else Subprocess sets inherits the same by accident Bind failure - exit Nothing happens (?) Subprocess sets a different on purpose Bind success (?) Nothing happens (?) (?) means this is usually the case, but not 100%. A complete fix would look something like: The parts of dealing with libprocess & libmesos should be refactored into libprocess as a helper. We would use this helper for the Containerizer, Fetcher, and module. * If the call is given , we can LOG(WARN) and unset the env var locally.. Similar segfault appeared again, this time during (which appeared to be near successful completion). Separated out the zookeeper segfault:. Support docker runtime configuration env var from image.. Whenever a scheduler accepts an inverse offer, Mesos will print a line like this in the master logs: Inverse offers should not trigger this warning.. This should include: Master agent: specific options. The module.. Change how we pipe output from Docker, which also removes the necessity of loading a into the :. Adding a hook inside the Docker containerizer is slightly more involved than the Mesos containerizer. Docker utors tasks perform plain-file logging in different places depending on whether the agent is in a Docker container itself Agent Code Not in container In container in a process This means a will need to be loaded or hooked into the . Or we will need to change how piping in done in .. Ensure `Content-Type` field is set for some responses. Modularize existing plain-file logging for utor task logs launched with the Mesos Containerizer. Per comments in MESOS-3916, the fix for that issue decreased the degree of flakiness, but it seems that some intermittent test failures do occur be investigated. This is a race between . The status updates for each task are not necessarily received in the same order as launching the tasks. See for the explanation. The related logs are above the comment.. It might also be worthwhile to check if the tests fail without .. The documentation currently assumes that oversubscribed resources () are the only type of revocable resources. Optimistic offers will add a second type of revocable resource () that should not be acted upon by components. For example, the says the following: NOTE: If any resource used by a task or utor is revocable, the whole container is treated as a revocable container and can therefore be killed or throttled by the QoS Controller. which we may amend to something like: NOTE: If any resource used by a task or utor is revocable usage slack, the whole container is treated as an oversubscribed container and can therefore be killed or throttled by the QoS Controller.. This issue is for investigating what needs to be added changed in such that will start on a clean slate. Additional issues will be created once done. Also see . should cover the following components: should be sufficient. This closes the socket and thereby prevents any further interaction from it. Related prior work: Cleans up any other 'd process. The clock should be and before the clean up of . A new method would then clear timers, clocks, and s; and then the clock. Needs to be cleared after has been cleaned up. Processes use this to communicate events. If cleared prematurely, will not be sent correctly, leading to infinite waits. The idea here is to close all sockets and deallocate any existing or objects. It should be possible to statically initialize it. Once cleanup is done, these should be reset. : Implement . (Optional) Clean up . * Wrap everything up in .. Two patches from some of my initial investigation into the approach: Update to code:. Test-only libprocess. Libprocess initialization includes the spawning of a variety of global processes and the creation of the server socket which listens for incoming requests. Some properties of the server socket are configured via environment variables, such as the IP and port or the SSL configuration. In the case of tests, libprocess is initialized once per test binary. This means that testing different configurations (SSL in particular) is cumbersome as a separate process would be needed for every test case. Add some optional code between some tests like: See for more on .. In order to write tests that exercise SSL with other components of Mesos, such as the HTTP scheduler library, we need to use the setup teardown logic found in the fixture. Currently, the test fixtures have separate inheritance structures like this: where is a gtest class. The plan is the following: Change to inherit from . This will require moving the setup (generation of keys and certs) from to . At the same time, of the cleanup logic in the SSLTest will not be needed. Move the logic of generating keys certs into helpers, so that individual tests can call them when needed, much like . Write a child class of which has the same functionality as the existing , for use by the existing tests that rely on or the . Have inherit from (which might be renamed during the refactor). If Mesos is not compiled with , then could be 'd into any empty class. The resulting structure should be like:. The messages we pass between Mesos components are largely undocumented. See this .. Currently, the HTTP Scheduler library does not support SSL-enabled Mesos. (You can manually test this by spinning up an SSL-enabled master and attempt to run the event-call framework example against We need to add tests that check the HTTP Scheduler library against SSL-enabled Mesos: with required certifications, with without verification of certificates * with a custom certificate authority (CA) These options should be controlled by the same environment variables found on the . Note: This issue will be broken down into smaller sub-issues as bugs problems are discovered.. not set on tasks with. HTTP scheduler library does not gracefully parse invalid resource identifiers. If you pass a nonsense string for \"master\" into a framework using the HTTP scheduler library, the framework segfaults. For example, using the example frameworks: Results in: Results in. I am install Mesos on 4 servers which have very similar hardware and software After performing , , and some servers have completed successfully and other failed on test . Is there something I should check in this test?. With committed, the and endpoints should also take an input as an array. It is important to change this before maintenance primitives are released: Also, a minor change to the error message from these endpoints:. The previous changes (MESOS-3345) to support integer precision when converting JSON Protobuf did not support precision for unsigned integers between and . (There's some loss, but the conversion is still as good bad as it was with This problem is due to a limitation in the JSON parsing library we use (PicoJSON), which parses integers as . Some possible solutions or things to investigate: We can investigate using another parsing library. * If we want extra precision beyond 64 or 80 bits per double, one possibility is the . We'd still need to change the parsing library though.. We currently have some helper functionality for V1 API tests. This is copied in a few test files. Factor this out into a common place once the API is stabilized. We can also update the helpers in to support the V1 API. This would let us get ride of lines like: In favor of:. For the background, see the parent story . For the see the linked design document (below).. Add either log rotation or capped-size logging (for tasks). Tasks currently log their output stdout stderr) to files (the \"sandbox\") on an agent's disk. In some cases, the accumulation of these logs can completely fill up the agent's disk and thereby kill the task or machine. To prevent this, we should either implement a log rotation mechanism or capped-size logging. This would be used by utors to control the amount of logs they keep. Master agent logs will not be affected. We will first scope out several possible approaches for log in a design document (see ). Once an approach is chosen, this story will be broken down into some corresponding issues.. For , we added some protobufs to represent time with integer precision. However, this precision is not maintained through protobuf JSON conversion, because of how our JSON convert numbers to floating point. To maintain precision, we can try one of the following: Add logic to numbers without loss when possible. Update PicoJson and add a compiler flag, In all cases, we'll need to make sure that: The JSON decoder parses the integer without loss. * We have some unit tests for big (close to ) and small integers.. Documentation images do not load. Any images which are referenced from the generated docs () do not show up on the website. For example: *. Windows Server will have an API for containers. To make use of this, a separate containerizer, similar to the will be implemented. TODO(): Flesh out this description when more details are available, regarding: The nuances of Windows vs Linux. * Etc.. In the process of adding the Cmake build system, noted and stubbed out all OS-specific code. That sweep (mostly of libprocess and stout) is here: Instead of having inline , the OS-specific code will be separated into directories. The Windows code will be stubbed out.. Persisted info is fetched from the registry when a master is elected or after failover. Currently, this process involves 3 steps: Start an operation to add the new master to the fetched registry. ** Change the \"Recovery\" test to include checks for the new object.. We need to implement the HTTP endpoints for Quota as outlined in the Design Doc: ( This also includes validating quota requests in terms of syntax correctness, updating Master bookkeeping structures, persisting quota requests in the .. In order to modify the maintenance schedule in the replicated registry, we will need Operations The operations will likely correspond to the HTTP API: Given a set of machines, verify then transition machines from Draining to Deactivated. * Given a set of machines, verify then transition machines from Deactivated to Normal. Remove affected machines from the schedule(s).. This is primarily a refactoring. The prototype for modifying the registry is currently: In order to support Maintenance schedules (possibly Quotas as well), there should be an alternate prototype for Maintenance. Something like: The existing should be refactored to allow for more than one key. If necessary, refactor existing operations defined in (AdminSlave, ReadminSlave, RemoveSlave).. In order to persist maintenance schedules across failovers of the master, the schedule information must be kept in the replicated registry. This means adding an additional message in the Registry protobuf in The status of each individual slave's maintenance will also be persisted in this way. Note: There can be multiple SlaveID's attached to a single hostname.. Replicated registry needs a representation of maintenance schedules. Maintenance information is not populated in case of failover. When a master starts up, or after a master has failed, it must re-populate maintenance information from the registry to the local state). Particularly, in should be changed to process maintenance information.. Slaves are not deactivated upon reaching a maintenance window. Master does not handle InverseOffers in the Accept call (Event Call API). Offers are currently sent from to framework via InverseOffers, which are roughly equivalent to negative Offers, can be sent in the same package. In Sent InverseOffers can be tracked in the master's local state: In One actor (master or allocator) should populate the new InverseOffers field. is where the and Offer object is constructed. In the allocator InverseOffers negative resources) allocation could be calculated in this method. Accept resource offer, start task. Check that are sent to the framework. Check that more are sent. Check that more are sent.. does not send InverseOffers to resources to be maintained. In the Event Call API, the Decline call is currently used by frameworks to reject resource offers. In the case of InverseOffers, the framework could give additional information to the operators and or allocator, as to why the InverseOffer is declined. Suppose a cluster running some consensus algorithm is given an InverseOffer on one of its nodes. It may decline saying \"Too few nodes\" (or, more verbosely, \"Specified InverseOffer would lower the number of active nodes below quorum\"). This change requires the following changes: Change to either store the reason, or log it. Extend the interface in ** Add change the declineOffer method in. Decline call does not include an optional \"reason\", in the Event Call API. Resource offers do not contain given a maintenance schedule. The master will need to do a number of things to implement the maintenance primitives: For machines that have a maintenance window: For unused resources, offers must be augmented with an Filter them before sending them again. * For declined inverse offers, do something with the reason (store or log). Recover the maintenance information upon failover. Note: Some amount of this logic will need to be placed in the allocator.. To achieve for the maintenance primitives, we will need to add the maintenance information to the registry. The registry currently stores all of the slave information, which is quite large ( for 50,000 slaves from my testing), which results in a protobuf object that is extremely expensive to copy. As far as I can tell, reads writes to maintenance information is independent of reads writes to the existing 'registry' information. So there are two approach here: h4. Add maintenance information to 'maintenance' key: The advantage of this approach is that we don't further grow the large Registry object. This approach assumes that writes to 'maintenance' are independent of writes to the 'registry'. -If these writes are not independent, this approach requires that we add transactional support to the State -This approach requires adding compaction to This approach likely requires some refactoring to the Registrar. h4. Add maintenance information to 'registry' key: (This is the chosen The advantage of this approach is that it's the easiest to implement. This will further grow the single 'registry' object, but doesn't preclude it being split apart in the future. This approach may require using the diff support in LogStorage and or adding compression support to LogStorage snapshots to deal with the increased size of the registry.. Based on MESOS-1474, we'd like to provide an HTTP API on the master for the maintenance primitives in mesos. For the MVP, we'll want something like this for manipulating the schedule: (Note: The slashes in URLs might not be supported A schedule might look like: There should be firewall settings such that only those with access to master can use these endpoints.. In order to inform frameworks about upcoming maintenance on offered resources, per MESOS-1474, we'd like to add an optional information to offers:. InverseOffer was defined as part of the maintenance work in MESOS-1474, design doc here: This ticket is to capture the addition of the InverseOffer protobuf to the necessary API changes for Event Call and the language bindings will be tracked separately.. As a first step toward Optimistic Offers, take the description from the epic and build an implementation design doc that can be shared for comments. Note: the links to the working group notes and design doc are located in the .. ", "contenttype": "application/json", "created": 737603, "id": 52, "language": "en", "user_name": "kaysoky"}]}